use crate::model_config::ModelConfig;
use serde_json::{Map, Value};

pub(crate) fn model_defaults(repo_id: &str) -> ModelConfig {
    let key = repo_id.to_ascii_lowercase();
    match key.as_str() {
        // BGE Models
        "baai/bge-small-zh-v1.5" => preset(
            512,
            4,
            8,
            2048,
            512,
            21128,
            "bert",
            &["BertModel"],
            1e-12,
            2,
            0,
            0.1,
            0.1,
        ),
        "baai/bge-small-en-v1.5" => preset(
            384,
            12,
            12,
            1536,
            512,
            30522,
            "bert",
            &["BertModel"],
            1e-12,
            2,
            0,
            0.1,
            0.1,
        ),
        "baai/bge-base-en-v1.5" => preset(
            768,
            12,
            12,
            3072,
            512,
            30522,
            "bert",
            &["BertModel"],
            1e-12,
            2,
            0,
            0.1,
            0.1,
        ),
        "baai/bge-large-en-v1.5" => preset(
            1024,
            24,
            16,
            4096,
            512,
            30522,
            "bert",
            &["BertModel"],
            1e-12,
            2,
            0,
            0.1,
            0.1,
        ),

        // Sentence Transformers
        "sentence-transformers/all-minilm-l6-v2" => preset(
            384,
            6,
            12,
            1536,
            512,
            30522,
            "bert",
            &["BertModel"],
            1e-12,
            2,
            0,
            0.1,
            0.1,
        ),
        "sentence-transformers/all-minilm-l12-v2" => preset(
            384,
            12,
            12,
            1536,
            512,
            30522,
            "bert",
            &["BertModel"],
            1e-12,
            2,
            0,
            0.1,
            0.1,
        ),
        "sentence-transformers/all-mpnet-base-v2" => preset(
            768,
            12,
            12,
            3072,
            514,
            30527,
            "mpnet",
            &["MPNetForMaskedLM"],
            1e-5,
            1,
            1,
            0.1,
            0.1,
        ),
        "sentence-transformers/paraphrase-minilm-l6-v2" => preset(
            384,
            6,
            12,
            1536,
            512,
            30522,
            "bert",
            &["BertModel"],
            1e-12,
            2,
            0,
            0.1,
            0.1,
        ),
        "sentence-transformers/multi-qa-mpnet-base-dot-v1" => preset(
            768,
            12,
            12,
            3072,
            514,
            30527,
            "mpnet",
            &["MPNetForMaskedLM"],
            1e-5,
            1,
            1,
            0.1,
            0.1,
        ),
        "sentence-transformers/all-distilroberta-v1" => preset(
            768,
            6,
            12,
            3072,
            514,
            50265,
            "roberta",
            &["RobertaForMaskedLM"],
            1e-5,
            1,
            1,
            0.1,
            0.1,
        ),
        "sentence-transformers/paraphrase-multilingual-minilm-l12-v2" => preset(
            384,
            12,
            12,
            1536,
            512,
            250037,
            "bert",
            &["BertModel"],
            1e-12,
            2,
            0,
            0.1,
            0.1,
        ),
        "sentence-transformers/distiluse-base-multilingual-cased-v1" => preset(
            768,
            6,
            12,
            3072,
            512,
            119547,
            "distilbert",
            &["DistilBertModel"],
            1e-12,
            2,
            0,
            0.1,
            0.1,
        ),

        // Chinese Models
        "moka-ai/m3e-base" => preset(
            768,
            12,
            12,
            3072,
            512,
            21128,
            "bert",
            &["BertModel"],
            1e-12,
            2,
            0,
            0.1,
            0.1,
        ),

        // E5 Models
        repo if repo.starts_with("intfloat/e5-small") => preset(
            384,
            12,
            12,
            1536,
            512,
            30522,
            "bert",
            &["BertModel"],
            1e-12,
            2,
            0,
            0.1,
            0.1,
        ),
        repo if repo.starts_with("intfloat/e5-base") => preset(
            768,
            12,
            12,
            3072,
            512,
            30522,
            "bert",
            &["BertModel"],
            1e-12,
            2,
            0,
            0.1,
            0.1,
        ),
        repo if repo.starts_with("intfloat/e5-large") => preset(
            1024,
            24,
            16,
            4096,
            512,
            30522,
            "bert",
            &["BertModel"],
            1e-12,
            2,
            0,
            0.1,
            0.1,
        ),

        // JINA Models
        repo if repo.starts_with("jinaai/jina-embeddings-v2-small") => preset(
            512,
            4,
            8,
            2048,
            8192,
            30528,
            "bert",
            &["JinaBertForMaskedLM"],
            1e-12,
            2,
            0,
            0.0,
            0.1,
        ),
        repo if repo.starts_with("jinaai/jina-embeddings-v2-base") => preset(
            768,
            12,
            12,
            3072,
            8192,
            30528,
            "bert",
            &["JinaBertForMaskedLM"],
            1e-12,
            2,
            0,
            0.0,
            0.1,
        ),

        // BGE Rerankers
        "baai/bge-reranker-v2-m3" => with_rerank(preset(
            1024,
            24,
            16,
            4096,
            8194,
            250002,
            "xlm-roberta",
            &["XLMRobertaForSequenceClassification"],
            1e-5,
            1,
            1,
            0.1,
            0.1,
        )),
        "baai/bge-reranker-large" => with_rerank(preset(
            1024,
            24,
            16,
            4096,
            514,
            250002,
            "xlm-roberta",
            &["XLMRobertaForSequenceClassification"],
            1e-5,
            1,
            1,
            0.1,
            0.1,
        )),
        "baai/bge-reranker-base" => with_rerank(preset(
            768,
            12,
            12,
            3072,
            514,
            250002,
            "xlm-roberta",
            &["XLMRobertaForSequenceClassification"],
            1e-5,
            1,
            1,
            0.1,
            0.1,
        )),

        // MS MARCO Rerankers
        repo if repo.starts_with("cross-encoder/ms-marco-minilm-l-6-v2") => with_rerank(preset(
            384,
            6,
            12,
            1536,
            512,
            30522,
            "bert",
            &["BertForSequenceClassification"],
            1e-12,
            2,
            0,
            0.1,
            0.1,
        )),
        repo if repo.starts_with("cross-encoder/ms-marco-minilm-l-12-v2") => with_rerank(preset(
            384,
            12,
            12,
            1536,
            512,
            30522,
            "bert",
            &["BertForSequenceClassification"],
            1e-12,
            2,
            0,
            0.1,
            0.1,
        )),
        repo if repo.starts_with("cross-encoder/ms-marco-tinybert-l-2-v2") => with_rerank(preset(
            128,
            2,
            2,
            512,
            512,
            30522,
            "bert",
            &["BertForSequenceClassification"],
            1e-12,
            2,
            0,
            0.1,
            0.1,
        )),
        repo if repo.starts_with("cross-encoder/ms-marco-electra-base") => with_rerank(preset(
            768,
            12,
            12,
            3072,
            512,
            30522,
            "electra",
            &["ElectraForSequenceClassification"],
            1e-12,
            2,
            0,
            0.1,
            0.1,
        )),
        "cross-encoder/quora-distilroberta-base" => with_rerank(preset(
            768,
            6,
            12,
            3072,
            514,
            50265,
            "roberta",
            &["RobertaForSequenceClassification"],
            1e-5,
            1,
            1,
            0.1,
            0.1,
        )),

        // Default
        _ => preset(
            768,
            12,
            12,
            3072,
            512,
            30522,
            "bert",
            &["BertModel"],
            1e-12,
            2,
            0,
            0.1,
            0.1,
        ),
    }
}

fn preset(
    hidden_size: usize,
    layers: usize,
    heads: usize,
    intermediate: usize,
    max_pos: usize,
    vocab: usize,
    model_type: &str,
    architectures: &[&str],
    layer_norm_eps: f64,
    type_vocab_size: usize,
    pad_token_id: i64,
    attention_dropout: f32,
    hidden_dropout: f32,
) -> ModelConfig {
    ModelConfig {
        architectures: Some(architectures.iter().map(|s| s.to_string()).collect()),
        model_type: Some(model_type.to_string()),
        hidden_size,
        num_hidden_layers: layers,
        num_attention_heads: heads,
        vocab_size: vocab,
        max_position_embeddings: max_pos,
        attention_probs_dropout_prob: Some(attention_dropout),
        hidden_dropout_prob: Some(hidden_dropout),
        intermediate_size: Some(intermediate),
        max_batch_size: None,
        memory_limit_mb: None,
        gpu_memory_fraction: None,
        hidden_act: Some("gelu".to_string()),
        initializer_range: Some(0.02),
        layer_norm_eps: Some(layer_norm_eps),
        use_cache: Some(true),
        position_embedding_type: Some("absolute".to_string()),
        pooler_hidden_act: None,
        pooler_dropout: None,
        pooling_type: None,
        num_labels: None,
        classifier_dropout: None,
        tie_word_embeddings: Some(true),
        is_decoder: Some(false),
        cross_attention_hidden_size: None,
        pad_token_id: Some(pad_token_id),
        bos_token_id: None,
        eos_token_id: None,
        type_vocab_size: Some(type_vocab_size),
        extra: Value::Object(Map::new()),
    }
}

fn with_rerank(mut config: ModelConfig) -> ModelConfig {
    config.num_labels = Some(1);
    config.classifier_dropout = Some(config.classifier_dropout.unwrap_or(0.1));
    config
}

#[cfg(test)]
mod tests {
    use super::model_defaults;

    #[test]
    fn presets_cover_required_models() {
        let repos = [
            "baai/bge-small-zh-v1.5",
            "baai/bge-small-en-v1.5",
            "baai/bge-base-en-v1.5",
            "baai/bge-large-en-v1.5",
            "sentence-transformers/all-minilm-l6-v2",
            "sentence-transformers/all-mpnet-base-v2",
            "sentence-transformers/paraphrase-minilm-l6-v2",
            "sentence-transformers/multi-qa-mpnet-base-dot-v1",
            "sentence-transformers/all-minilm-l12-v2",
            "sentence-transformers/all-distilroberta-v1",
            "sentence-transformers/paraphrase-multilingual-minilm-l12-v2",
            "sentence-transformers/distiluse-base-multilingual-cased-v1",
            "intfloat/e5-large",
            "intfloat/e5-base",
            "intfloat/e5-small",
            "jinaai/jina-embeddings-v2-base-en",
            "jinaai/jina-embeddings-v2-small-en",
            "moka-ai/m3e-base",
            "baai/bge-reranker-v2-m3",
            "baai/bge-reranker-large",
            "baai/bge-reranker-base",
            "cross-encoder/ms-marco-minilm-l-6-v2",
            "cross-encoder/ms-marco-minilm-l-12-v2",
            "cross-encoder/ms-marco-tinybert-l-2-v2",
            "cross-encoder/ms-marco-electra-base",
            "cross-encoder/quora-distilroberta-base",
        ];

        for repo in repos {
            let cfg = model_defaults(repo);
            assert!(
                cfg.hidden_size > 0 && cfg.num_hidden_layers > 0 && cfg.num_attention_heads > 0,
                "preset should not contain zero values for {}",
                repo
            );
            assert!(
                cfg.type_vocab_size.unwrap_or(0) > 0,
                "type vocab missing for {repo}"
            );
            assert!(
                cfg.max_position_embeddings > 0,
                "max position missing for {repo}"
            );
        }
    }

    #[test]
    fn mpnet_defaults_match_roberta_variants() {
        let cfg = model_defaults("sentence-transformers/all-mpnet-base-v2");
        assert_eq!(cfg.model_type.as_deref(), Some("mpnet"));
        assert_eq!(cfg.layer_norm_eps.unwrap(), 1e-5);
        assert_eq!(cfg.pad_token_id, Some(1));
        assert_eq!(cfg.type_vocab_size, Some(1));
    }
}
