# gllm æ¶æ„è®¾è®¡

## æ¦‚è¿°

gllm æ˜¯ä¸€ä¸ªçº¯ Rust æœ¬åœ°åµŒå…¥å’Œé‡æ’åºæ¨ç†åº“ï¼ŒåŸºäº gllm-kernels çš„é›¶æˆæœ¬ç®—å­ä¸æƒé‡å®¹å™¨ï¼Œæä¾› OpenAI é£æ ¼ SDK APIã€‚æ”¯æŒ Encoder (BERT) å’Œ Decoder (Qwen2/Mistral) ä¸¤ç§æ¶æ„ã€‚

## ä¿®è®¢å†å²

| ç‰ˆæœ¬ | æ—¥æœŸ | æè¿° |
|------|------|------|
| v0.5.0 | 2025-01-11 | æ–°å¢ Decoder æ¶æ„æ”¯æŒ (Qwen2/Mistral)ã€CodeXEmbed ä»£ç åµŒå…¥æ¨¡å‹ |
| v0.4.1 | 2025-01-10 | GPU æ£€æµ‹ã€OOM æ¢å¤ |
| v0.1.0 | 2025-01-28 | åˆå§‹æ¶æ„è®¾è®¡ |

---

## æ¶æ„æ€»è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    gllm (Rust Crate)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Public API Layer                                           â”‚
â”‚  â”œâ”€â”€ Client / AsyncClient                                   â”‚
â”‚  â”œâ”€â”€ EmbeddingsBuilder / RerankBuilder                      â”‚
â”‚  â””â”€â”€ Types (Embedding, RerankResult, etc.)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model Layer                                                â”‚
â”‚  â”œâ”€â”€ Registry        â†’ åˆ«å â†” HF repo æ˜ å°„                  â”‚
â”‚  â”œâ”€â”€ Downloader      â†’ hf-hub ä¸‹è½½åˆ° ~/.gllm/models/        â”‚
â”‚  â””â”€â”€ Loader          â†’ SafeTensors â†’ WeightMatrix/Vector    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Engine Layer                                               â”‚
â”‚  â”œâ”€â”€ EmbeddingEngine â†’ BERT ç¼–ç  + Pooling                  â”‚
â”‚  â””â”€â”€ RerankEngine    â†’ Cross-Encoder æ¨ç†                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  gllm-kernels Runtime Backends                              â”‚
â”‚  â”œâ”€â”€ CUDA/ROCm/Metal/WGPU â†’ è¿è¡Œæ—¶è‡ªåŠ¨æ£€æµ‹                   â”‚
â”‚  â””â”€â”€ CPU                 â†’ è‡ªåŠ¨å›é€€                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## æŠ€æœ¯æ ˆ

| ç»„ä»¶ | åº“ | ç‰ˆæœ¬ | è¯´æ˜ |
|------|-----|------|------|
| ç®—å­ä¸æƒé‡å®¹å™¨ | gllm-kernels | latest | é›¶æˆæœ¬ç®—å­ + WeightMatrix/Vector |
| æ¨¡å‹å¯¼å…¥ | safetensors | latest | SafeTensors è§£æ |
| æ¨¡å‹ä¸‹è½½ | hf-hub | latest | HuggingFace å®¢æˆ·ç«¯ (rustls) |
| Tokenizer | tokenizers | latest | HuggingFace Tokenizers |
| å¼‚æ­¥è¿è¡Œæ—¶ | tokio | 1.x | å¯é€‰ï¼Œasync ç‰¹æ€§ |
| åºåˆ—åŒ– | serde | 1.x | JSON/é…ç½®åºåˆ—åŒ– |
| é”™è¯¯å¤„ç† | thiserror | 2.x | é”™è¯¯ç±»å‹å®šä¹‰ |

---

## æ¨¡å—è®¾è®¡

### ARCH-MOD-001: lib.rs (å…¥å£æ¨¡å—)

**èŒè´£**: å¯¼å‡ºå…¬å…± API

**å¯¼å‡ºå†…å®¹**:
- `Client`, `AsyncClient`
- `EmbeddingsBuilder`, `RerankBuilder`
- `Embedding`, `EmbeddingResponse`, `RerankResult`, `RerankResponse`
- `Error`, `Result`

### ARCH-MOD-002: client.rs (å®¢æˆ·ç«¯æ¨¡å—)

**èŒè´£**: å®¢æˆ·ç«¯å®ç°

**ç»„ä»¶**:
- `Client` - åŒæ­¥å®¢æˆ·ç«¯ï¼ŒæŒæœ‰æ¨¡å‹å’Œå¼•æ“
- `AsyncClient` - å¼‚æ­¥å®¢æˆ·ç«¯ (feature = "async")
- æ¨¡å‹åŠ è½½å’Œåˆå§‹åŒ–é€»è¾‘

### ARCH-MOD-003: embeddings.rs (Embeddings æ¨¡å—)

**èŒè´£**: Embeddings API

**ç»„ä»¶**:
- `EmbeddingsBuilder` - Builder æ¨¡å¼
- `EmbeddingResponse` - å“åº”ç»“æ„

### ARCH-MOD-004: rerank.rs (Rerank æ¨¡å—)

**èŒè´£**: Rerank API

**ç»„ä»¶**:
- `RerankBuilder` - Builder æ¨¡å¼
- `RerankResponse` - å“åº”ç»“æ„

### ARCH-MOD-005: model.rs (æ¨¡å‹ç®¡ç†æ¨¡å—)

**èŒè´£**: æ¨¡å‹ä¸‹è½½å’ŒåŠ è½½

**ç»„ä»¶**:
- `ModelManager` - ç®¡ç†æ¨¡å‹ç”Ÿå‘½å‘¨æœŸ
- `download_model()` - ä» HF ä¸‹è½½
- `load_model()` - åŠ è½½ SafeTensors

### ARCH-MOD-006: registry.rs (æ³¨å†Œè¡¨æ¨¡å—)

**èŒè´£**: æ¨¡å‹åˆ«åç®¡ç†

**ç»„ä»¶**:
- `ModelRegistry` - åˆ«åæ³¨å†Œè¡¨
- `ModelInfo` - æ¨¡å‹å…ƒä¿¡æ¯ (ç±»å‹ã€HF repoã€æ¶æ„)

### ARCH-MOD-007: engine.rs (æ¨ç†å¼•æ“æ¨¡å—)

**èŒè´£**: æ¨ç†æ‰§è¡Œ

**ç»„ä»¶**:
- `EmbeddingEngine` - BERT åµŒå…¥æ¨ç†
- `RerankEngine` - Cross-Encoder é‡æ’åºæ¨ç†

### ARCH-MOD-008: types.rs (ç±»å‹å®šä¹‰æ¨¡å—)

**èŒè´£**: å…¬å…±ç±»å‹

**ç»„ä»¶**:
- `Embedding`, `EmbeddingResponse` - åµŒå…¥ç±»å‹
- `RerankResult`, `RerankResponse` - é‡æ’åºç±»å‹
- `Error` - é”™è¯¯ç±»å‹

---

## ç›®å½•ç»“æ„

```
gllm/
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lib.rs           # å…¬å…± API å¯¼å‡º
â”‚   â”œâ”€â”€ client.rs        # Client / AsyncClient
â”‚   â”œâ”€â”€ embeddings.rs    # Embeddings API
â”‚   â”œâ”€â”€ rerank.rs        # Rerank API
â”‚   â”œâ”€â”€ model.rs         # æ¨¡å‹ä¸‹è½½/åŠ è½½
â”‚   â”œâ”€â”€ registry.rs      # åˆ«åæ³¨å†Œè¡¨
â”‚   â”œâ”€â”€ engine.rs        # æ¨ç†å¼•æ“ (BERT + CrossEncoder)
â”‚   â””â”€â”€ types.rs         # å…¬å…±ç±»å‹
â”œâ”€â”€ SPEC/                # è®¾è®¡æ–‡æ¡£
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

---

## Feature Flags

```toml
[features]
default = []                # Fat Binaryï¼šè¿è¡Œæ—¶é€‰æ‹©åç«¯
tokio = ["tokio"]           # å¼‚æ­¥ API æ”¯æŒ
quantized = []              # é‡åŒ–æ¨¡å‹æ”¯æŒ
gpu-quantized = ["quantized"] # GPU é‡åŒ–ï¼ˆå½“å‰ä¸º CPU å›é€€ï¼‰
paged-attention = []        # åˆ†é¡µæ³¨æ„åŠ›
flash-attention = []        # FlashAttention
nccl = ["gllm-kernels/nccl"] # åˆ†å¸ƒå¼è®­ç»ƒ/æ¨ç†
```

---

## æ•°æ®æµ

### æ¨¡å‹åŠ è½½æµç¨‹

```
ç”¨æˆ·è°ƒç”¨ Client::new("bge-m3")
    â”‚
    â–¼
Registry è§£æåˆ«å â†’ "BAAI/bge-m3"
    â”‚
    â–¼
æ£€æŸ¥ ~/.gllm/models/ æ˜¯å¦å­˜åœ¨
    â”‚
    â”œâ”€â”€ å­˜åœ¨ â†’ ç›´æ¥åŠ è½½
    â”‚
    â””â”€â”€ ä¸å­˜åœ¨ â†’ hf-hub ä¸‹è½½ â†’ ä¿å­˜åˆ°æœ¬åœ°
    â”‚
    â–¼
SafeTensors è§£ææƒé‡ â†’ WeightLoader
    â”‚
    â–¼
æ„å»º WeightMatrix/Vector â†’ åˆå§‹åŒ–æ¨¡å‹ â†’ è¿”å› Client
```

### æ¨ç†æµç¨‹ (Embeddings)

```
client.embeddings(["text1", "text2"]).generate()
    â”‚
    â–¼
Tokenizer ç¼–ç è¾“å…¥
    â”‚
    â–¼
EmbeddingEngine BERT å‰å‘ä¼ æ’­
    â”‚
    â–¼
Mean Pooling â†’ å½’ä¸€åŒ–
    â”‚
    â–¼
è¿”å› EmbeddingResponse
```

### æ¨ç†æµç¨‹ (Rerank)

```
client.rerank("query", ["doc1", "doc2"]).generate()
    â”‚
    â–¼
æ„å»º [query, doc] pairs
    â”‚
    â–¼
Tokenizer ç¼–ç æ¯ä¸ª pair
    â”‚
    â–¼
RerankEngine Cross-Encoder å‰å‘ä¼ æ’­
    â”‚
    â–¼
Sigmoid â†’ ç›¸å…³æ€§åˆ†æ•°
    â”‚
    â–¼
æ’åº â†’ è¿”å› RerankResponse
```

---

## å­˜å‚¨ç»“æ„

```
~/.gllm/
â””â”€â”€ models/
    â”œâ”€â”€ BAAI--bge-m3/              # HF repo åç§° (/ â†’ --)
    â”‚   â”œâ”€â”€ model.safetensors
    â”‚   â”œâ”€â”€ config.json
    â”‚   â””â”€â”€ tokenizer.json
    â””â”€â”€ BAAI--bge-reranker-v2-m3/
        â”œâ”€â”€ model.safetensors
        â””â”€â”€ ...
```

---

## æ¶æ„å†³ç­–è®°å½• (ADR)

### ARCH-ADR-001: ç§»é™¤ Burnï¼Œä½¿ç”¨ gllm-kernels é›¶æˆæœ¬æŠ½è±¡

**å†³ç­–**: ä½¿ç”¨ gllm-kernels ä½œä¸ºç®—å­åº“ä¸æƒé‡å®¹å™¨

**ç†ç”±**:
- é›¶æˆæœ¬æŠ½è±¡ï¼šWeightMatrix/Vector + åŸç”Ÿåˆ‡ç‰‡ API
- è¿è¡Œæ—¶åç«¯é€‰æ‹©ï¼šåŒä¸€äºŒè¿›åˆ¶æ”¯æŒå¤š GPU/CPU
- çº¯ Rust å®ç°ï¼Œæ”¯æŒé™æ€ç¼–è¯‘ä¸”æ—  Burn ä¾èµ–

### ARCH-ADR-002: ä½¿ç”¨ wgpu ä½œä¸ºé»˜è®¤ GPU åç«¯

**å†³ç­–**: é»˜è®¤å¯ç”¨ wgpu åç«¯

**ç†ç”±**:
- çº¯ Rust å®ç°ï¼Œæ—  C++ ä¾èµ–
- è·¨å¹³å°æ”¯æŒ (Vulkan/DX12/Metal)
- ç¬¦åˆé™æ€ç¼–è¯‘è¦æ±‚

### ARCH-ADR-003: æ¨¡å‹æ ¼å¼æ”¯æŒ SafeTensors å’Œ GGUF

**å†³ç­–**: æ”¯æŒ SafeTensors (é»˜è®¤) å’Œ GGUF (é‡åŒ–æ¨¡å‹) ä¸¤ç§æ ¼å¼

**ç†ç”±**:
- SafeTensors ç”± safetensors crate è§£æï¼Œç”¨äº HuggingFace å…¨ç²¾åº¦æ¨¡å‹
- GGUF é€šè¿‡**çº¯ Rust è§£æå™¨**å®ç°ï¼ˆæ—  llama.cpp ç»‘å®šï¼‰ï¼Œä¿æŒçº¯ Rust ç›®æ ‡
- GGUF æ”¯æŒ Q4_0/Q4_K_M/Q8_0 ç­‰é‡åŒ–æ ¼å¼ï¼Œæ˜¾è‘—é™ä½å†…å­˜å’Œæå‡æ¨ç†é€Ÿåº¦
- HuggingFace å’Œ llama.cpp ç”Ÿæ€éƒ½æœ‰å¤§é‡ GGUF é‡åŒ–æ¨¡å‹

**v0.11.0 æ–°å¢ç»„ä»¶**:
- `GgufLoader` - çº¯ Rust GGUF æ–‡ä»¶è§£æå™¨
- `QTensor` - é‡åŒ–å¼ é‡ï¼Œæ”¯æŒå¤šç§ GGML æ•°æ®ç±»å‹
- `QLinear` - é‡åŒ–çº¿æ€§å±‚ï¼Œæ”¯æŒ dequantize + matmul

### ARCH-ADR-003b: æ”¯æŒ AWQ é‡åŒ–æ ¼å¼

**å†³ç­–**: æ”¯æŒ HuggingFace AWQ (Activation-aware Weight Quantization) æ ¼å¼

**ç†ç”±**:
- AWQ æ˜¯ HuggingFace ç”Ÿæ€ä¸»æµçš„ INT4 é‡åŒ–æ ¼å¼
- ä¸ SafeTensors æ ¼å¼å…¼å®¹ï¼Œä»…æƒé‡å­˜å‚¨æ–¹å¼ä¸åŒ
- æä¾›æ¯” GGUF Q4 æ›´é«˜çš„ç²¾åº¦ï¼ˆé€šè¿‡ activation-aware scalingï¼‰
- å¤§é‡é¢„é‡åŒ–æ¨¡å‹å¯ç”¨ï¼ˆTheBloke ç­‰å‘å¸ƒè€…ï¼‰

**v0.11.0 æ–°å¢ç»„ä»¶**:
- `AwqWeight` - AWQ é‡åŒ–æƒé‡ï¼ˆqweight + scales + zerosï¼‰
- `AwqLinear` - AWQ é‡åŒ–çº¿æ€§å±‚ï¼Œæ”¯æŒ per-group dequantize

### ARCH-ADR-004: ä½¿ç”¨ rustls ä½œä¸º TLS åç«¯

**å†³ç­–**: hf-hub ä½¿ç”¨ rustls-tls ç‰¹æ€§

**ç†ç”±**:
- çº¯ Rust TLS å®ç°
- æ”¯æŒé™æ€ç¼–è¯‘
- æ—  OpenSSL ä¾èµ–

### ARCH-ADR-005: ä¸‰å¤§æ ¸å¿ƒåŠŸèƒ½

**å†³ç­–**: æ”¯æŒ Embeddingã€Rerank å’Œ Text Generation

**ç†ç”±**:
- v0.5.0 å·²æ·»åŠ  Decoder æ¶æ„æ”¯æŒ (Qwen2/Mistral)
- å¤ç”¨ç°æœ‰ DecoderModel å®ç°æ–‡æœ¬ç”Ÿæˆï¼Œæ— é¢å¤–å¤æ‚åº¦
- ç»Ÿä¸€ API è®¾è®¡ï¼šClient.embeddings() / Client.rerank() / Client.generate()
- æ»¡è¶³å®Œæ•´çš„ RAG åœºæ™¯éœ€æ±‚

**v0.6.0 æ–°å¢ç»„ä»¶**:
- `GeneratorModel` - å°è£… DecoderModel + LmHead
- `KVCache` - å¢é‡è§£ç åŠ é€Ÿ
- `Sampler` - Temperature/Top-p/Top-k é‡‡æ ·
- `GenerationBuilder` - ç”Ÿæˆè¯·æ±‚æ„å»ºå™¨

### ARCH-ADR-006: Actor æ¨¡å¼è§£å†³çº¿ç¨‹å®‰å…¨é—®é¢˜ ğŸ”’ FROZEN

**é—®é¢˜èƒŒæ™¯**:
- æ¨ç†è¿‡ç¨‹ä¸­åŒ…å«å¯å˜çš„ KVCache/ä¸­é—´ç¼“å†²ï¼Œé»˜è®¤ä¸ä¿è¯ Send/Sync
- `EmbeddingEngine`/`RerankEngine` â†’ `EngineBackend` â†’ `Client` éš¾ä»¥è·¨çº¿ç¨‹å…±äº«
- åœ¨ tokio å¼‚æ­¥ç¯å¢ƒä¸­æ— æ³•ç›´æ¥è·¨çº¿ç¨‹å…±äº«ï¼ˆå¦‚ `tokio::spawn`ã€`Arc<Client>`ï¼‰

**å†³ç­–**: ä½¿ç”¨ Actor æ¨¡å¼éš”ç¦»éçº¿ç¨‹å®‰å…¨ç±»å‹

**æ¶æ„è®¾è®¡**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     è°ƒç”¨æ–¹ï¼ˆå¼‚æ­¥ç¯å¢ƒï¼‰                            â”‚
â”‚  Arc<EmbedderHandle> / Arc<RerankerHandle>                      â”‚
â”‚  â”œâ”€â”€ å¤©ç„¶ Send + Sync                                           â”‚
â”‚  â””â”€â”€ åªåŒ…å« mpsc::Senderï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â”‚ mpsc channel
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ä¸“ç”¨æ¨ç†çº¿ç¨‹ï¼ˆDedicated Threadï¼‰               â”‚
â”‚  â”œâ”€â”€ gllm::Clientï¼ˆé Send/Syncï¼Œä½†åœ¨å•çº¿ç¨‹å†…ä½¿ç”¨ï¼‰              â”‚
â”‚  â”œâ”€â”€ æ¥æ”¶è¯·æ±‚ â†’ æ‰§è¡Œæ¨ç† â†’ é€šè¿‡ oneshot è¿”å›ç»“æœ                 â”‚
â”‚  â””â”€â”€ ç”Ÿå‘½å‘¨æœŸä¸ Handle ç»‘å®š                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**é€šä¿¡åè®®**:

```rust
// è¯·æ±‚ç±»å‹
enum EmbedRequest {
    Embed {
        text: String,
        respond: oneshot::Sender<Result<Vec<f32>>>,
    },
    EmbedBatch {
        texts: Vec<String>,
        respond: oneshot::Sender<Result<Vec<Vec<f32>>>>,
    },
    Shutdown,
}

enum RerankRequest {
    Rerank {
        query: String,
        documents: Vec<String>,
        respond: oneshot::Sender<Result<Vec<RerankResult>>>,
    },
    Shutdown,
}

// Handleï¼ˆç”¨æˆ·æŒæœ‰ï¼ŒSend + Syncï¼‰
pub struct EmbedderHandle {
    sender: mpsc::Sender<EmbedRequest>,
}

pub struct RerankerHandle {
    sender: mpsc::Sender<RerankRequest>,
}
```

**API è®¾è®¡**:

```rust
// åŒæ­¥ APIï¼ˆæ—  tokio ç‰¹æ€§ï¼‰
impl EmbedderHandle {
    pub fn new() -> Result<Self>;           // å¯åŠ¨ä¸“ç”¨çº¿ç¨‹
    pub fn embed(&self, text: &str) -> Result<Vec<f32>>;
    pub fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>>;
}

// å¼‚æ­¥ APIï¼ˆtokio ç‰¹æ€§ï¼‰
impl EmbedderHandle {
    pub async fn new() -> Result<Self>;     // å¯åŠ¨ä¸“ç”¨çº¿ç¨‹
    pub async fn embed(&self, text: &str) -> Result<Vec<f32>>;
    pub async fn embed_batch(&self, texts: &[String]) -> Result<Vec<Vec<f32>>>;
}
```

**ç†ç”±**:
- å½»åº•è§£å†³ Send/Sync é—®é¢˜ï¼Œæ— éœ€ unsafe
- Handle åªåŒ…å« channel senderï¼Œå¤©ç„¶çº¿ç¨‹å®‰å…¨
- æ¨ç†åœ¨ä¸“ç”¨çº¿ç¨‹æ‰§è¡Œï¼Œé¿å…é˜»å¡ tokio è¿è¡Œæ—¶
- é›¶é¢å¤–ä¾èµ–ï¼ˆå¤ç”¨ tokio mpsc/oneshotï¼‰
- ç®€å•å¯ç»´æŠ¤ï¼Œä»£ç é‡çº¦ 100-150 è¡Œ

**é™åˆ¶**:
- æ‰€æœ‰æ¨ç†è¯·æ±‚ä¸²è¡Œæ‰§è¡Œï¼ˆå•çº¿ç¨‹ï¼‰
- å¯¹äºé«˜å¹¶å‘åœºæ™¯ï¼Œå¯æ‰©å±•ä¸º worker poolï¼ˆæœªæ¥ä¼˜åŒ–ï¼‰

### ARCH-ADR-007: é›†æˆ gllm-kernels è¿è¡Œæ—¶åç«¯

**å†³ç­–**: ä½¿ç”¨ gllm-kernels ä½œä¸ºåº•å±‚ç®—å­åº“ï¼Œæ”¯æŒè¿è¡Œæ—¶åç«¯é€‰æ‹©

**é—®é¢˜èƒŒæ™¯**:
- æ—§æ¶æ„åœ¨ç¼–è¯‘æ—¶å›ºå®šåç«¯ï¼Œç”¨æˆ·æ— æ³•è¿è¡Œæ—¶è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è®¾å¤‡
- ç”¨æˆ·æ— æ³•åœ¨è¿è¡Œæ—¶æ ¹æ®è®¾å¤‡è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜åç«¯
- ç¼ºå°‘é’ˆå¯¹ 2M+ è¶…é•¿ä¸Šä¸‹æ–‡çš„æ•°å€¼ç¨³å®šæ€§ä¼˜åŒ–

**æ¶æ„è®¾è®¡**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           gllm                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model Layer (æƒé‡åŠ è½½)                                              â”‚
â”‚    â””â”€â”€ WeightMatrix/Vector ç”¨äº SafeTensors/GGUF åŠ è½½               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Attention Layer (causal_attention.rs)                               â”‚
â”‚    â”œâ”€â”€ ä» WeightMatrix è·å–åŸç”Ÿåˆ‡ç‰‡ &[f16]                           â”‚
â”‚    â”œâ”€â”€ è°ƒç”¨ gllm_kernels::KernelDispatcher::flash_attention()       â”‚
â”‚    â””â”€â”€ ä»åˆ‡ç‰‡åˆ›å»ºè¾“å‡º Vec                                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Engine Layer (engine.rs)                                            â”‚
â”‚    â””â”€â”€ ä½¿ç”¨ gllm_kernels::detect_backend() è·å–è¿è¡Œæ—¶åç«¯           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”‚ è¿è¡Œæ—¶è°ƒç”¨
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        gllm-kernels                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  KernelDispatcher                                                    â”‚
â”‚    â”œâ”€â”€ è¿è¡Œæ—¶æ£€æµ‹: CUDA â†’ ROCm â†’ Metal â†’ WGPU â†’ CPU                 â”‚
â”‚    â”œâ”€â”€ é›¶æˆæœ¬æ´¾å‘: match enum + #[inline(always)]                   â”‚
â”‚    â””â”€â”€ 2M ä¸Šä¸‹æ–‡ä¼˜åŒ–: LogSpaceSoftmax + KahanAccumulator            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**è°ƒç”¨ç¤ºä¾‹**:

```rust
// gllm/src/attention/causal_attention.rs
use gllm_kernels::{KernelDispatcher, FlashAttentionConfig};

impl CausalAttention {
    pub fn forward(&self, q: &[f16], k: &[f16], v: &[f16]) -> Vec<f16> {
        let mut output = vec![f16::ZERO; output_len];

        // è°ƒç”¨ä¼˜åŒ–ç®—å­
        self.dispatcher.flash_attention(
            q, k, v,
            &mut output,
            FlashAttentionConfig {
                use_log_space_softmax: true,  // 2M ä¸Šä¸‹æ–‡
                use_kahan_accumulator: true,  // æ•°å€¼ç¨³å®š
                ..Default::default()
            },
        );

        output
    }
}
```

**ç†ç”±**:
- **è¿è¡Œæ—¶é€‰æ‹©**: åŒä¸€äºŒè¿›åˆ¶æ”¯æŒæ‰€æœ‰ GPU å‚å•†ï¼Œç”¨æˆ·æ— éœ€é‡æ–°ç¼–è¯‘
- **é›¶æˆæœ¬æŠ½è±¡**: æ³›å‹ + enum match æ—  vtable å¼€é”€
- **æ•°å€¼ç¨³å®š**: 2M+ ä¸Šä¸‹æ–‡ä¸ä¼šæº¢å‡ºæˆ–ç²¾åº¦æŸå¤±
- **èŒè´£åˆ†ç¦»**: gllm ä¸“æ³¨æ¨¡å‹ç®¡ç†ï¼Œgllm-kernels ä¸“æ³¨ç®—å­ä¼˜åŒ–

**ä¾èµ–å…³ç³»**:

```toml
# gllm/Cargo.toml
[dependencies]
gllm-kernels = { version = "0.2", default-features = false }
```

**åç«¯é€‰æ‹©ä¼˜å…ˆçº§**:
1. `GLLM_BACKEND` ç¯å¢ƒå˜é‡ï¼ˆå¼ºåˆ¶æŒ‡å®šï¼‰
2. è‡ªåŠ¨æ£€æµ‹: CUDA â†’ ROCm â†’ Metal â†’ WGPU â†’ CPU

### ARCH-ADR-008: 2M è¶…é•¿ä¸Šä¸‹æ–‡æ”¯æŒ

**å†³ç­–**: æ‰€æœ‰ Attention è®¡ç®—å¿…é¡»ä½¿ç”¨ gllm-kernels çš„æ•°å€¼ç¨³å®šç®—æ³•

**é—®é¢˜èƒŒæ™¯**:
- æ ‡å‡† Softmax çš„ exp() åœ¨é•¿åºåˆ—æ—¶ä¼šæº¢å‡º
- æµ®ç‚¹ç´¯åŠ è¯¯å·®éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ O(n)
- 2M token ä¸Šä¸‹æ–‡éœ€è¦ç‰¹æ®Šå¤„ç†

**è§£å†³æ–¹æ¡ˆ**:

| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ | gllm-kernels ç»„ä»¶ |
|------|----------|-------------------|
| exp æº¢å‡º | Log-Space Softmax | `LogSpaceSoftmax` |
| ç´¯åŠ è¯¯å·® | Kahan è¡¥å¿æ±‚å’Œ | `KahanAccumulator` |
| è¶…é•¿åºåˆ— | åˆ†å±‚ç´¯åŠ å™¨ | `HierarchicalAccumulator` |
| åœ¨çº¿è®¡ç®— | ç¨³å®šç´¯åŠ å™¨ | `StableAccumulator` |

**æ•°å­¦ä¿è¯**:
- Log-Space: é¿å… exp(>709) æº¢å‡º
- Kahan: è¯¯å·®ä» O(n) é™è‡³ O(1)
- åˆ†å±‚: æ”¯æŒä»»æ„é•¿åº¦åºåˆ—

**é…ç½®æ–¹å¼**:

```rust
FlashAttentionConfig {
    use_log_space_softmax: true,   // 2M ä¸Šä¸‹æ–‡å¿…é¡»å¼€å¯
    use_kahan_accumulator: true,   // å»ºè®®å¼€å¯
    ..Default::default()
}

### ARCH-ADR-009: çº¯ GPU MoE ç®¡çº¿ ğŸ”’ FROZEN

**å†³ç­–**: MoE æ¨ç†å¿…é¡»åœ¨çº¯ GPU è·¯å¾„æ‰§è¡Œï¼Œç¦æ­¢ä¸­é—´ GPUâ†’CPUâ†’GPU å¾€è¿”

**é—®é¢˜èƒŒæ™¯**:
- MoE routing è¾“å‡º (expert_indices, expert_weights) åœ¨ GPU ä¸Šè®¡ç®—
- æ—§ API `moe_forward_gpu` æ¥å— host slicesï¼Œå¼ºåˆ¶ readback åå†ä¸Šä¼ 
- è¿™å®Œå…¨æŠµæ¶ˆäº† GPU routing çš„ä¼˜åŒ–æ•ˆæœ
- ç±»å‹å®‰å…¨è¿è§„ï¼š`readback<T: KernelFloat>` ä¸æ”¯æŒ U32 ç±»å‹

**æ¶æ„çº¦æŸ** (FROZEN - ç¦æ­¢è¿å):

| çº¦æŸID | çº¦æŸå†…å®¹ | è¿è§„ç¤ºä¾‹ |
|--------|----------|----------|
| ARCH-MOE-001 | `moe_forward_gpu_pure` å¿…é¡»æ¥å— GPU tensors | æ¥å— `&[u32]`/`&[f32]` host slices |
| ARCH-MOE-002 | routingâ†’forward å¿…é¡»çº¯ GPU æ•°æ®æµ | routing è¾“å‡º readback åˆ° CPU |
| ARCH-MOE-003 | U32 tensor å¿…é¡»æœ‰ç±»å‹å®‰å…¨çš„ readback | ç”¨ f32 è¯»å– u32 å† `to_bits()` |
| ARCH-MOE-004 | åªåœ¨æœ€ç»ˆè¾“å‡ºæ—¶ readback | æ¯å±‚éƒ½ readback hidden states |

**æ­£ç¡®çš„æ•°æ®æµ**:

```
hidden_states (GPU)
    â”‚
    â–¼
moe_route_gpu()
    â”‚
    â”œâ”€â”€ expert_indices_gpu (GPU, U32)
    â””â”€â”€ expert_weights_gpu (GPU, F32)
    â”‚
    â–¼
moe_forward_gpu_pure()  â† æ–° APIï¼Œæ¥å— GPU tensors
    â”‚
    â–¼
moe_output (GPU)
    â”‚
    â–¼
... ç»§ç»­ä¸‹ä¸€å±‚ (ä¿æŒ GPU) ...
    â”‚
    â–¼
æœ€ç»ˆè¾“å‡ºæ—¶æ‰ readback
```

**gllm-kernels API å˜æ›´**:

```rust
// æ—§ APIï¼ˆä¿ç•™ç”¨äºéœ€è¦ host æ§åˆ¶çš„åœºæ™¯ï¼‰
fn moe_forward_gpu(
    &self,
    input: &GpuTensor,
    expert_indices: &[u32],      // host slice
    expert_weights: &[f32],      // host slice
    ...
) -> Result<(), String>;

// æ–° APIï¼ˆç¬¦åˆ ARCH-MOE-001/002ï¼Œçº¯ GPU è·¯å¾„ï¼‰
fn moe_forward_gpu_pure(
    &self,
    input: &GpuTensor,
    expert_indices: &GpuTensor,  // GPU tensor (U32)
    expert_weights: &GpuTensor,  // GPU tensor (F32)
    all_gate_weights: &GpuTensor,
    all_up_weights: &GpuTensor,
    all_down_weights: &GpuTensor,
    output: &mut GpuTensor,
    config: MoEForwardConfig,
) -> Result<(), String>;
```

**ç±»å‹å®‰å…¨çš„ U32 readback** (ARCH-MOE-003):

```rust
// gllm-kernels Backend trait æ–°å¢æ–¹æ³•
fn readback_u32(&self, gpu: &GpuTensor, host: &mut [u32]) -> Result<(), String>;
```

**å®ç°è¦æ±‚**:

| ç»„ä»¶ | ä¿®æ”¹å†…å®¹ |
|------|----------|
| gllm-kernels/backend.rs | æ·»åŠ  `moe_forward_gpu_pure` æ–¹æ³•ç­¾å |
| gllm-kernels/backend.rs | æ·»åŠ  `readback_u32` æ–¹æ³• |
| gllm-kernels/wgpu | å®ç° `moe_forward_gpu_pure`ï¼ˆå†…éƒ¨ç›´æ¥ä½¿ç”¨ GPU buffersï¼‰ |
| gllm/moe_layer.rs | ä½¿ç”¨æ–° APIï¼Œç§»é™¤ routing readback + re-upload |

**å‘åå…¼å®¹**:
- ä¿ç•™æ—§ `moe_forward_gpu` APIï¼ˆç”¨äºéœ€è¦ host æ§åˆ¶çš„åœºæ™¯ï¼‰
- æ–°ä»£ç ä¼˜å…ˆä½¿ç”¨ `moe_forward_gpu_pure`

**éªŒæ”¶æ ‡å‡†**:
- å• token MoE æ¨ç†æ—  GPUâ†’CPUâ†’GPU å¾€è¿”ï¼ˆroutingâ†’forward çº¯ GPUï¼‰
- U32 tensor readback ç±»å‹å®‰å…¨ï¼ˆæ—  f32/to_bits hackï¼‰
- æ€§èƒ½æå‡ï¼šå‡å°‘ 2 æ¬¡ GPU ä¼ è¾“ï¼ˆindices + weights ä¸å† readbackï¼‰

---

## ç®—å­ä½“ç³»çŸ©é˜µ

### ARCH-OPS-001: Backend Trait ç®—å­æ¸…å•

Backend trait å®šä¹‰äº `gllm-kernels/src/backend_trait.rs`ï¼ŒåŒ…å« **18 ä¸ªæ–¹æ³•**ï¼š

| # | ç®—å­æ–¹æ³• | ç”¨é€” | æ•°æ®ç±»å‹ |
|---|----------|------|----------|
| 1 | `flash_attention()` | Flash Attention è®¡ç®— | f32/f16/bf16 |
| 2 | `paged_attention()` | åˆ†é¡µæ³¨æ„åŠ›ï¼ˆKV Cacheï¼‰ | f32/f16/bf16 |
| 3 | `softmax()` | Softmax å½’ä¸€åŒ– | f32/f16/bf16 |
| 4 | `matmul()` | çŸ©é˜µä¹˜æ³• | f32/f16/bf16 |
| 5 | `rope_precompute()` | RoPE ä½ç½®ç¼–ç é¢„è®¡ç®— | f32 |
| 6 | `rope_apply()` | RoPE åº”ç”¨ | f32/f16/bf16 |
| 7 | `rope_apply_inplace()` | RoPE åŸåœ°åº”ç”¨ | f32/f16/bf16 |
| 8 | `topk()` | Top-K é‡‡æ · | f32 |
| 9 | `apply_temperature()` | æ¸©åº¦ç¼©æ”¾ | f32 |
| 10 | `sample_tokens()` | Token é‡‡æ · | f32 |
| 11 | `argmax()` | è´ªå¿ƒè§£ç  | f32 |
| 12 | `moe_route()` | MoE ç¨€ç–è·¯ç”± | f32 |
| 13 | `compute_routing_logits()` | MoE è·¯ç”± logits | f32 |
| 14 | `rms_norm()` | RMS å½’ä¸€åŒ– | f32/f16/bf16 |
| 15 | `rms_norm_inplace()` | RMS å½’ä¸€åŒ–ï¼ˆåŸåœ°ï¼‰ | f32/f16/bf16 |
| 16 | `silu()` | SiLU æ¿€æ´» | f32/f16/bf16 |
| 17 | `silu_inplace()` | SiLU æ¿€æ´»ï¼ˆåŸåœ°ï¼‰ | f32/f16/bf16 |
| 18 | `add_bias()` | åç½®æ·»åŠ  | f32/f16/bf16 |
| 19 | `backend_type()` | è¿”å›åç«¯æ ‡è¯† | - |

### ARCH-OPS-002: åç«¯å®ç°çŸ©é˜µ

**è¦†ç›–ç‡ï¼š100%** - æ‰€æœ‰åç«¯å®Œæ•´å®ç°å…¨éƒ¨ 16 ä¸ªæ–¹æ³•

| ç®—å­ | CPU | WGPU | CUDA | ROCm | Metal | GPUåŠ é€Ÿ |
|------|:---:|:----:|:----:|:----:|:-----:|:-------:|
| flash_attention | âœ… | âœ… | âœ… | âœ… | âœ… | **GPU** |
| paged_attention | âœ… | âœ… | âœ… | âœ… | âœ… | **GPU** |
| softmax | âœ… | âœ… | âœ… | âœ… | âœ… | CPUå›é€€ |
| matmul | âœ… | âœ… | âœ… | âœ… | âœ… | CPUå›é€€ |
| rope_precompute | âœ… | âœ… | âœ… | âœ… | âœ… | CPUå›é€€ |
| rope_apply | âœ… | âœ… | âœ… | âœ… | âœ… | CPUå›é€€ |
| rope_apply_inplace | âœ… | âœ… | âœ… | âœ… | âœ… | CPUå›é€€ |
| topk | âœ… | âœ… | âœ… | âœ… | âœ… | CPUå›é€€ |
| apply_temperature | âœ… | âœ… | âœ… | âœ… | âœ… | CPUå›é€€ |
| sample_tokens | âœ… | âœ… | âœ… | âœ… | âœ… | CPUå›é€€ |
| argmax | âœ… | âœ… | âœ… | âœ… | âœ… | CPUå›é€€ |
| moe_route | âœ… | âœ… | âœ… | âœ… | âœ… | CPUå›é€€ |
| compute_routing_logits | âœ… | âœ… | âœ… | âœ… | âœ… | CPUå›é€€ |
| add_bias | âœ… | âœ… | âœ… | âœ… | âœ… | CPUå›é€€ |
| backend_type | âœ… | âœ… | âœ… | âœ… | âœ… | - |

**å…³é”®å‘ç°**ï¼šä»… 2 ä¸ªç®—å­ï¼ˆflash_attention, paged_attentionï¼‰æœ‰ GPU ä¼˜åŒ–å®ç°ï¼Œå…¶ä»– 13 ä¸ªç®—å­å‡ä¸º CPU å›é€€ã€‚

### ARCH-OPS-003: gllm-kernels ç‹¬ç«‹å‡½æ•°

é™¤ Backend trait å¤–ï¼Œ`gllm-kernels` è¿˜æä¾›ä»¥ä¸‹ç‹¬ç«‹å‡½æ•°ï¼ˆé trait æ–¹æ³•ï¼‰ï¼š

| å‡½æ•° | ç”¨é€” | æ–‡ä»¶ä½ç½® |
|------|------|----------|
| `linear_forward()` | çº¿æ€§å±‚å‰å‘ | ops.rs |
| `layer_norm_forward()` | LayerNorm | ops.rs |
| `rms_norm_forward()` | RMSNorm | ops.rs |
| `rms_norm_inplace()` | RMSNorm åŸåœ° | ops.rs |
| `gelu_inplace()` | GELU æ¿€æ´» | ops.rs |
| `silu_inplace()` | SiLU æ¿€æ´» | ops.rs |
| `moe_route()` | MoE è·¯ç”±ï¼ˆCPUï¼‰ | ops.rs |

---

## æ¨ç†ç±»å‹ç®—å­æ˜ å°„

### ARCH-INF-001: Embeddings æ¨ç† (DynamicBertModel)

**è°ƒç”¨é“¾**ï¼š`forward() â†’ Encoder â†’ Layers â†’ Attention + FFN â†’ Pool`

| ç®—å­ | æ¥æº | è°ƒç”¨ä½ç½® | è¯´æ˜ |
|------|------|----------|------|
| `linear_forward` | gllm-kernels | Q/K/V/O æŠ•å½±, FFN | CPU |
| `layer_norm_forward` | gllm-kernels | è¾“å…¥/è¾“å‡ºå½’ä¸€åŒ– | CPU |
| `gelu_inplace` | gllm-kernels | FFN æ¿€æ´» | CPU |
| `backend.flash_attention` | Backend | è‡ªæ³¨æ„åŠ› | **å¯GPU** |
| `rope_apply_inplace` | gllm-kernels | ä½ç½®ç¼–ç  | CPU, å¯é€‰ |

### ARCH-INF-002: Reranker æ¨ç† (DynamicCrossEncoder)

**è°ƒç”¨é“¾**ï¼š`score() â†’ Encoder.forward() â†’ Pool â†’ Classifier`

| ç®—å­ | æ¥æº | è°ƒç”¨ä½ç½® | è¯´æ˜ |
|------|------|----------|------|
| `linear_forward` | gllm-kernels | Encoder + Classifier | CPU |
| `layer_norm_forward` | gllm-kernels | Encoder å½’ä¸€åŒ– | CPU |
| `gelu_inplace` | gllm-kernels | FFN æ¿€æ´» | CPU |
| `backend.flash_attention` | Backend | è‡ªæ³¨æ„åŠ› | **å¯GPU** |
| `rope_apply_inplace` | gllm-kernels | ä½ç½®ç¼–ç  | CPU, å¯é€‰ |

### ARCH-INF-003: Generator æ¨ç† - Dense (GeneratorModel)

**è°ƒç”¨é“¾**ï¼š`forward() â†’ Layers â†’ Attention(w/Cache) + FFN â†’ LmHead` + `sample()`

| ç®—å­ | æ¥æº | è°ƒç”¨ä½ç½® | è¯´æ˜ |
|------|------|----------|------|
| `linear_forward` | gllm-kernels | Q/K/V/O, FFN, LmHead | CPU |
| `rms_norm_forward` | gllm-kernels | å±‚å½’ä¸€åŒ– | CPU |
| `rms_norm_inplace` | gllm-kernels | æœ€ç»ˆå½’ä¸€åŒ– | CPU |
| `silu_inplace` | gllm-kernels | FFN æ¿€æ´» (SwiGLU) | CPU |
| `rope_apply_inplace` | gllm-kernels | ä½ç½®ç¼–ç  | CPU |
| `backend.flash_attention` | Backend | è‡ªæ³¨æ„åŠ› + KV Cache | **å¯GPU** |
| `backend.argmax` | Backend | è´ªå¿ƒé‡‡æ · | **å¯GPU** |

### ARCH-INF-004: Generator æ¨ç† - MoE (MoEGeneratorModel)

**è°ƒç”¨é“¾**ï¼š`forward() â†’ Layers â†’ Attention(w/Cache) + MoE â†’ LmHead` + `sample()`

| ç®—å­ | æ¥æº | è°ƒç”¨ä½ç½® | è¯´æ˜ |
|------|------|----------|------|
| `linear_forward` | gllm-kernels | Q/K/V/O, Expert FFN, LmHead | CPU |
| `rms_norm_forward` | gllm-kernels | å±‚å½’ä¸€åŒ– | CPU |
| `rms_norm_inplace` | gllm-kernels | æœ€ç»ˆå½’ä¸€åŒ– | CPU |
| `silu_inplace` | gllm-kernels | Expert FFN æ¿€æ´» | CPU |
| `rope_apply_inplace` | gllm-kernels | ä½ç½®ç¼–ç  | CPU |
| `backend.flash_attention` | Backend | è‡ªæ³¨æ„åŠ› + KV Cache | **å¯GPU** |
| `backend.moe_route` | Backend | ç¨€ç–ä¸“å®¶è·¯ç”± | **å¯GPU** |
| `backend.argmax` | Backend | è´ªå¿ƒé‡‡æ · | **å¯GPU** |

### ARCH-INF-005: æ¨ç†ç±»å‹ç®—å­æ±‡æ€»çŸ©é˜µ

| ç®—å­ | Embeddings | Reranker | Dense Gen | MoE Gen |
|------|:----------:|:--------:|:---------:|:-------:|
| **Backend trait æ–¹æ³•** |
| flash_attention | âœ… | âœ… | âœ… | âœ… |
| paged_attention | - | - | âœ…* | âœ…* |
| argmax | - | - | âœ… | âœ… |
| moe_route | - | - | - | âœ… |
| **gllm-kernels å‡½æ•°** |
| linear_forward | âœ… | âœ… | âœ… | âœ… |
| layer_norm_forward | âœ… | âœ… | - | - |
| rms_norm_forward | - | - | âœ… | âœ… |
| rms_norm_inplace | - | - | âœ… | âœ… |
| gelu_inplace | âœ… | âœ… | âœ…** | - |
| silu_inplace | - | - | âœ… | âœ… |
| rope_apply_inplace | âœ…*** | âœ…*** | âœ… | âœ… |

*: åˆ†é¡µæ³¨æ„åŠ›ä¸ºå¯é€‰ä¼˜åŒ–
**: GELU ä»…åœ¨ hidden_act="gelu" æ—¶ä½¿ç”¨ï¼Œå¤šæ•°æ¨¡å‹ä½¿ç”¨ SiLU
***: RoPE ä»…åœ¨å¯ç”¨ä½ç½®ç¼–ç æ—¶ä½¿ç”¨

---

## æ¶æ„ä¼˜åŒ–æœºä¼š

### ARCH-OPT-001: GPU åŠ é€Ÿä¼˜åŒ–ç©ºé—´

å½“å‰ä»… 2/15 ç®—å­æœ‰ GPU ä¼˜åŒ–ï¼Œä»¥ä¸‹ç®—å­æœ‰è¾ƒå¤§ä¼˜åŒ–ç©ºé—´ï¼š

| ä¼˜å…ˆçº§ | ç®—å­ | è®¡ç®—å æ¯” | å½“å‰ | ä¼˜åŒ–æ–¹æ¡ˆ | é¢„æœŸåŠ é€Ÿ |
|:------:|------|:--------:|------|----------|:--------:|
| **P0-1** | linear_forward | ~70% | CPU | GPU matmul/cuBLAS | **8-16x** |
| **P0-2** | rms_norm | ~10% | CPU | GPU reduce kernel | 2-5x |
| **P0-3** | silu_inplace | ~5% | CPU | GPU element-wise | 3-5x |
| P1 | argmax | <1% | CPUå›é€€ | GPU reduce | ä½ |
| P1 | moe_route | <1% | CPUå›é€€ | GPU routing | ä½ |

**å…³é”®å‘ç°**ï¼š
- `linear_forward` å æ¨ç†è®¡ç®—é‡çš„ **70%**ï¼Œæ˜¯æœ€é«˜ä¼˜å…ˆçº§ä¼˜åŒ–ç›®æ ‡
- P0 çº§åˆ«ç®—å­ç»„åˆå¯å®ç°æ•´ä½“ **5-10x** åŠ é€Ÿ
- `rms_norm` å’Œ `silu_inplace` å¯ä¸å‰åç®—å­èåˆï¼Œå‡å°‘å†…å­˜å¸¦å®½

### ARCH-OPT-002: ç®—æ³•é›†åˆæ— çŠ¶æ€åŒ–

**é—®é¢˜**ï¼šå½“å‰ Backend å®ç°åŒ…å« GPU handle çŠ¶æ€ï¼Œä½†å¤šæ•°ç®—å­æ˜¯æ— çŠ¶æ€çš„çº¯å‡½æ•°ã€‚

**åˆ†æç»“æœ**ï¼š

| ç»„ä»¶ | å­—æ®µæ•° | æ˜¯å¦æ— çŠ¶æ€ | å¯ç§»é™¤ new() |
|------|:------:|:----------:|:------------:|
| `CpuBackend` | 0 | âœ… å®Œå…¨æ— çŠ¶æ€ | âœ… å¯ä»¥ |
| `DynamicPooler` | 2 (enum+flag) | âœ… é…ç½®ä»… | âœ… å¯ä»¥ |
| `WgpuBackend` | 3+ | âŒ GPU èµ„æº | âŒ ä¸å¯ |
| `CudaBackend` | 3+ | âŒ GPU èµ„æº | âŒ ä¸å¯ |
| `GeneratorModel` | 5+ | âŒ æƒé‡+ç¼“å­˜ | âŒ ä¸å¯ |
| `MoEGeneratorModel` | 6+ | âŒ æƒé‡+ä¸“å®¶ | âŒ ä¸å¯ |
| `DynamicBertModel` | 4+ | âŒ æƒé‡ | âŒ ä¸å¯ |

**ä¼˜åŒ–æ–¹å‘**ï¼š
1. `CpuBackend` å¯æ”¹ä¸ºé›¶å¤§å°ç±»å‹ï¼ˆZSTï¼‰ï¼Œç§»é™¤ `new()` æ–¹æ³•
2. `DynamicPooler` å¯æ”¹ä¸ºæ„é€ æ—¶é…ç½®ï¼Œæ— éœ€ `new()` æ–¹æ³•
3. GPU Backend å¿…é¡»ä¿ç•™çŠ¶æ€ï¼ˆè®¾å¤‡å¥æŸ„ã€ç¼“å†²æ± ï¼‰
4. æ¨¡å‹ç»„ä»¶å¿…é¡»ä¿ç•™çŠ¶æ€ï¼ˆæƒé‡ã€KV Cacheï¼‰

### ARCH-OPT-003: Backend é™æ€åŒ–ï¼ˆOnceLock æ¨¡å¼ï¼‰

**é—®é¢˜**ï¼šå½“å‰ `Arc<dyn Backend>` åœ¨æ¯ä¸ªç»„ä»¶é—´ä¼ é€’ï¼Œå¢åŠ ä»£ç å¤æ‚åº¦ã€‚

**æ–¹æ¡ˆ**ï¼šä½¿ç”¨ `std::sync::OnceLock` å®ç°å…¨å±€å•ä¾‹ã€‚

```rust
// é™æ€ Backend å®ä¾‹
static BACKEND: OnceLock<Arc<dyn Backend>> = OnceLock::new();

pub fn get_backend() -> &'static Arc<dyn Backend> {
    BACKEND.get_or_init(|| auto_select_backend())
}

// ä½¿ç”¨æ—¶ç›´æ¥è°ƒç”¨
let backend = get_backend();
backend.flash_attention(...);
```

**æ”¶ç›Šåˆ†æ**ï¼š

| ç»´åº¦ | å½“å‰ | é™æ€åŒ–å | æ”¹å–„ |
|------|------|----------|------|
| å‚æ•°ä¼ é€’ç‚¹ | 18 å¤„ | 0 å¤„ | -18 |
| æ„é€ å‡½æ•°å‚æ•° | 10 ä¸ªå« backend | 0 ä¸ª | -10 |
| ä»£ç è¡Œæ•° | ~150 è¡Œä¼ é€’ä»£ç  | ~20 è¡Œåˆå§‹åŒ– | -130 è¡Œ |
| è¿è¡Œæ—¶å¼€é”€ | æ¯æ¬¡ Arc clone | é›¶ï¼ˆé™æ€å¼•ç”¨ï¼‰ | å¾®ä¼˜åŒ– |

**å½±å“èŒƒå›´**ï¼š

| æ–‡ä»¶ | ä¿®æ”¹å†…å®¹ |
|------|----------|
| `gllm-kernels/src/backend.rs` | æ·»åŠ  OnceLock + get_backend() |
| `gllm/src/engine.rs` | ç§»é™¤ backend å‚æ•° |
| `gllm/src/generator_engine.rs` | ç§»é™¤ backend å‚æ•° |
| `gllm/src/generator_model.rs` | ç§»é™¤ backend å­—æ®µ/å‚æ•° |
| `gllm/src/moe_generator_model.rs` | ç§»é™¤ backend å­—æ®µ/å‚æ•° |
| `gllm/src/dynamic_bert.rs` | ç§»é™¤ backend å­—æ®µ/å‚æ•° |
| `gllm/src/causal_attention.rs` | ç›´æ¥è°ƒç”¨ get_backend() |
| `gllm/src/moe_layer.rs` | ç›´æ¥è°ƒç”¨ get_backend() |

**çº¦æŸ**ï¼š
- å¿…é¡»åœ¨é¦–æ¬¡æ¨ç†å‰è°ƒç”¨ `get_backend()` å®Œæˆåˆå§‹åŒ–
- è¿›ç¨‹ç”Ÿå‘½å‘¨æœŸå†… Backend ä¸å¯æ›´æ¢ï¼ˆç¬¦åˆå½“å‰è®¾è®¡ï¼‰
- æµ‹è¯•æ—¶å¯é€šè¿‡ `GLLM_BACKEND=cpu` ç¯å¢ƒå˜é‡å¼ºåˆ¶æŒ‡å®š

---

## å¾…å®ç°æ¶æ„è®¾è®¡

### ARCH-ADR-010: æ¨¡å‹å¿«é€ŸåŠ è½½ï¼ˆå¹¶è¡Œè§£æ + æŒ‰éœ€ä¸‹è½½ï¼‰

**å…³è”éœ€æ±‚**: REQ-LOAD-001

**é—®é¢˜èƒŒæ™¯**:
- å¤šåˆ†ç‰‡ SafeTensors æ–‡ä»¶ï¼ˆ70B æ¨¡å‹å¯è¾¾ 15+ åˆ†ç‰‡ï¼‰
- å½“å‰é¡ºåºè§£æï¼šshard-1 è§£æ â†’ shard-2 è§£æ â†’ ... ä¸²è¡Œæ‰§è¡Œ
- **ä¸»è¦åœºæ™¯æ˜¯æœ¬åœ°å·²ç¼“å­˜**ï¼ˆ~/.gllm/models/ï¼‰ï¼Œä¸æ˜¯ä¸‹è½½

**ç°æœ‰ç¼“å­˜æœºåˆ¶**:
```
ModelManager::prepare(model_id)
    â”‚
    â”œâ”€â”€ æ£€æŸ¥æœ¬åœ°ç¼“å­˜ ~/.gllm/models/{repo_id}/
    â”‚   â”œâ”€â”€ å­˜åœ¨ + æœ‰æƒé‡æ–‡ä»¶ + æœ‰ config.json â†’ ç›´æ¥åŠ è½½ï¼ˆä¸»è¦åœºæ™¯ï¼‰
    â”‚   â””â”€â”€ ç¼ºå¤± â†’ ä¸‹è½½ååŠ è½½ï¼ˆé¦–æ¬¡ä½¿ç”¨ï¼‰
    â”‚
    â””â”€â”€ å½“å‰ç“¶é¢ˆï¼šå¤šåˆ†ç‰‡é¡ºåºè§£æï¼Œå³ä½¿æœ¬åœ°å·²ç¼“å­˜ä¹Ÿæ…¢
```

**è®¾è®¡ç›®æ ‡**:
- æœ¬åœ°ç¼“å­˜å¹¶è¡Œè§£æï¼šåŠ è½½é€Ÿåº¦æå‡ â‰¥ 2x
- é¦–æ¬¡ä¸‹è½½æ—¶æµæ°´çº¿ï¼šä¸‹è½½ä¸è§£æå¹¶è¡Œ
- å†…å­˜å³°å€¼å¯æ§
- å…¼å®¹ç°æœ‰ API

---

#### 1. ä¸¤ç§åœºæ™¯æ—¶åºå›¾

**åœºæ™¯ Aï¼šæœ¬åœ°å·²ç¼“å­˜ï¼ˆä¸»è¦åœºæ™¯ï¼Œå  90%+ï¼‰**

```
ä¼˜åŒ–å‰ï¼ˆé¡ºåºè§£æï¼‰ï¼š
shard-1: [===è§£æ===]
shard-2:              [===è§£æ===]
shard-3:                           [===è§£æ===]
                                                â†’ å®Œæˆ

ä¼˜åŒ–åï¼ˆå¹¶è¡Œè§£æï¼‰ï¼š
shard-1: [===è§£æ===]
shard-2: [===è§£æ===]     â†’ å®Œæˆï¼ˆNx åŠ é€Ÿï¼ŒN=CPUæ ¸æ•°ï¼‰
shard-3: [===è§£æ===]
```

**åœºæ™¯ Bï¼šé¦–æ¬¡ä¸‹è½½ï¼ˆå  <10%ï¼‰**

```
ä¼˜åŒ–åï¼ˆæµæ°´çº¿ï¼‰ï¼š
ç½‘ç»œå±‚:  [shard-1ä¸‹è½½][shard-2ä¸‹è½½][shard-3ä¸‹è½½]...
è§£æå±‚:       [shard-1è§£æ][shard-2è§£æ][shard-3è§£æ]...
                                                    â†’ å®Œæˆ
```

---

#### 2. æ¶æ„æ€»è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ModelManagerï¼ˆå…¥å£ï¼‰                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  prepare(model_id)                                              â”‚
â”‚      â”‚                                                          â”‚
â”‚      â”œâ”€â”€ æ£€æŸ¥æœ¬åœ°ç¼“å­˜                                            â”‚
â”‚      â”‚   â”œâ”€â”€ å­˜åœ¨ â†’ ParallelParser ç›´æ¥è§£æ                     â”‚
â”‚      â”‚   â””â”€â”€ ç¼ºå¤± â†’ Downloader ä¸‹è½½ + ParallelParser è§£æ       â”‚
â”‚      â”‚                                                          â”‚
â”‚      â””â”€â”€ WeightLoader åŠ è½½åˆ° WeightMatrix/Vector                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ParallelParser  â”‚  â”‚ Downloader      â”‚  â”‚ WeightLoader    â”‚
â”‚ rayon + mmap    â”‚  â”‚ hf-hubï¼ˆå¯é€‰ï¼‰  â”‚  â”‚ ç°æœ‰ï¼Œä¸å˜      â”‚
â”‚ å¹¶è¡Œè§£æå¤šåˆ†ç‰‡  â”‚  â”‚ æŒ‰éœ€ä¸‹è½½ç¼ºå¤±    â”‚  â”‚ SafeTensorsâ†’W*  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 3. æ ¸å¿ƒæ•°æ®ç»“æ„

**åŠ è½½é…ç½®ï¼ˆLoadConfigï¼‰**:

| å­—æ®µ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| max_parse_threads | usize | num_cpus | æœ€å¤§å¹¶è¡Œè§£æçº¿ç¨‹æ•° |
| use_mmap | bool | true | æ˜¯å¦ä½¿ç”¨ mmapï¼ˆæ¨èï¼‰ |
| progress_callback | Option<Fn> | None | è¿›åº¦å›è°ƒ |

**åŠ è½½è¿›åº¦ï¼ˆLoadProgressï¼‰**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| stage | ProgressStage | å½“å‰é˜¶æ®µ |
| current_shard | usize | å½“å‰åˆ†ç‰‡ |
| total_shards | usize | æ€»åˆ†ç‰‡æ•° |
| bytes_loaded | u64 | å·²åŠ è½½å­—èŠ‚ |

**è¿›åº¦é˜¶æ®µæšä¸¾**:

| å€¼ | è¯´æ˜ |
|----|------|
| CheckingCache | æ£€æŸ¥æœ¬åœ°ç¼“å­˜ |
| Downloading | ä¸‹è½½ä¸­ï¼ˆä»…é¦–æ¬¡ï¼‰ |
| Parsing | å¹¶è¡Œè§£æä¸­ |
| LoadingWeights | åŠ è½½æƒé‡ |
| Complete | å®Œæˆ |

---

#### 4. å¹¶è¡Œè§£æç­–ç•¥ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰

**rayon + mmap å¹¶è¡Œè§£æ**:

```
ParallelParser::parse_shards(shard_files: Vec<PathBuf>)
    â”‚
    â”œâ”€â”€ shard_files.par_iter()  // rayon å¹¶è¡Œè¿­ä»£
    â”‚   .map(|path| {
    â”‚       let mmap = Mmap::map(File::open(path));  // é›¶æ‹·è´æ˜ å°„
    â”‚       let tensors = SafeTensors::deserialize(&mmap);
    â”‚       (path, tensors, mmap)  // ä¿æŒ mmap å­˜æ´»
    â”‚   })
    â”‚   .collect()
    â”‚
    â””â”€â”€ è¿”å› Vec<(PathBuf, SafeTensors, Mmap)>
```

**ä¸ºä»€ä¹ˆ mmap è€Œé read**:

| æ–¹å¼ | å†…å­˜å ç”¨ | åŠ è½½é€Ÿåº¦ | è¯´æ˜ |
|------|----------|----------|------|
| fs::read() | å®Œæ•´æ–‡ä»¶å¤åˆ¶åˆ°å † | æ…¢ï¼Œéœ€è¦å¤åˆ¶ | å½“å‰æ–¹å¼ |
| mmap | é›¶æ‹·è´ï¼ŒOS ç®¡ç† | å¿«ï¼ŒæŒ‰éœ€åŠ è½½ | ä¼˜åŒ–æ–¹å¼ |

---

#### 5. ä¸ç°æœ‰ç»„ä»¶é›†æˆ

**ä¸æ”¹åŠ¨ WeightLoader**ï¼š

```
ParallelParser è¾“å‡º          WeightLoaderï¼ˆç°æœ‰ï¼‰
     â”‚                              â”‚
     â–¼                              â–¼
Vec<(shard_path, SafeTensors)> â†’ from_bytes(&mmap_data) â†’ WeightMatrix/Vector
                                    â”‚
                                    â””â”€â”€ ç°æœ‰é€»è¾‘ä¸å˜ï¼Œåªæ˜¯å¹¶è¡Œè°ƒç”¨
```

**ModelManager ä¿®æ”¹ç‚¹**:

| æ–¹æ³• | ä¿®æ”¹ |
|------|------|
| prepare() | è°ƒç”¨ ParallelParser æ›¿ä»£é¡ºåºè§£æ |
| download_model() | ä¿æŒä¸å˜ï¼ˆhf-hub ä¸‹è½½ï¼‰ |

---

#### 6. é”™è¯¯å¤„ç†

| é”™è¯¯ | è§¦å‘æ¡ä»¶ | å¤„ç† |
|------|----------|------|
| MmapError | æ–‡ä»¶æ˜ å°„å¤±è´¥ | å›é€€åˆ° fs::read() |
| ParseError | SafeTensors æ ¼å¼é”™è¯¯ | è¿”å›é”™è¯¯ |
| PartialShard | åˆ†ç‰‡æ–‡ä»¶ä¸å®Œæ•´ | åˆ é™¤åé‡æ–°ä¸‹è½½ |

---

#### 7. æ€§èƒ½é¢„æœŸ

| åœºæ™¯ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|------|--------|--------|------|
| æœ¬åœ°å·²ç¼“å­˜ 7Bï¼ˆå•åˆ†ç‰‡ï¼‰ | ~2s | ~2s | æ— å˜åŒ– |
| æœ¬åœ°å·²ç¼“å­˜ 70Bï¼ˆ15åˆ†ç‰‡ï¼‰ | ~30s | ~5-10s | 3-6x |
| é¦–æ¬¡ä¸‹è½½ 70B | ~300s | ~150s | ~2x |

**çŠ¶æ€**: âœ… è®¾è®¡å®Œæˆ

### ARCH-ADR-011: åŸç”Ÿé‡åŒ–æ¨ç† Kernel

**å…³è”éœ€æ±‚**: REQ-QUANT-001

**é—®é¢˜èƒŒæ™¯**:
- å½“å‰é‡åŒ–æ¨ç†æµç¨‹ï¼šQ4 æƒé‡ â†’ dequantize â†’ f32 matmul â†’ è¾“å‡º
- æ¯æ¬¡æ¨ç†éƒ½é‡å¤ dequantizeï¼Œè®¡ç®—å’Œå†…å­˜åŒé‡æµªè´¹
- Q4_0 æ¨¡å‹ dequantize åå†…å­˜å ç”¨å¢åŠ  4 å€

**è®¾è®¡ç›®æ ‡**:
- ç›´æ¥åœ¨é‡åŒ–æƒé‡ä¸Šè®¡ç®—ï¼Œé¿å… dequantize å¼€é”€
- æ¨ç†é€Ÿåº¦æå‡ â‰¥ 2x
- å†…å­˜å ç”¨é™ä½ï¼ˆQ4: 75%ï¼ŒQ8: 50%ï¼‰
- ç²¾åº¦æŸå¤± < 1%ï¼ˆå¯¹æ¯” dequantize æ–¹æ¡ˆï¼‰

---

#### 1. è®¡ç®—æµç¨‹å¯¹æ¯”

```
ä¼˜åŒ–å‰ï¼ˆdequantizeï¼‰ï¼š
Q4 æƒé‡ â†’ [dequantize] â†’ f32 â†’ [matmul] â†’ è¾“å‡º
          â†‘ æ¯æ¬¡æ¨ç†éƒ½æ‰§è¡Œï¼Œ4x å†…å­˜è†¨èƒ€

ä¼˜åŒ–åï¼ˆåŸç”Ÿ kernelï¼‰ï¼š
Q4 æƒé‡ â†’ [Q4 matmul kernel] â†’ è¾“å‡º
          â†‘ in-kernel dequantï¼Œé›¶é¢å¤–å†…å­˜
```

---

#### 2. æ”¯æŒçš„é‡åŒ–æ ¼å¼

**GGUF æ ¼å¼**:

| æ ¼å¼ | block_size | æ•°æ®å¸ƒå±€ | ç²¾åº¦æŸå¤± |
|------|------------|----------|----------|
| Q4_0 | 32 | 16 bytes data + 2 bytes scale | ä½ |
| Q4_K | 256 | åˆ†å— scale + min | æä½ |
| Q8_0 | 32 | 32 bytes data + 2 bytes scale | æä½ |

**AWQ æ ¼å¼**:

| æ ¼å¼ | group_size | æ•°æ®å¸ƒå±€ | ç²¾åº¦æŸå¤± |
|------|------------|----------|----------|
| AWQ INT4 | 128 | packed int4 + scale + zero | ä½ |

---

#### 3. é‡åŒ–æ•°æ®ç»“æ„

**Q4_0 Blockï¼ˆGGUFï¼‰**:

| å­—æ®µ | åç§» | å¤§å° | ç±»å‹ | è¯´æ˜ |
|------|------|------|------|------|
| data | 0 | 16 bytes | u8[16] | 32 ä¸ª 4-bit å€¼æ‰“åŒ… |
| scale | 16 | 2 bytes | f16 | ç¼©æ”¾å› å­ |

è§£ç å…¬å¼ï¼š`value[i] = (nibble[i] - 8) * scale`

**Q4_K Blockï¼ˆGGUFï¼‰**:

| å­—æ®µ | åç§» | å¤§å° | ç±»å‹ | è¯´æ˜ |
|------|------|------|------|------|
| scales | 0 | 12 bytes | u8[12] | å­å—ç¼©æ”¾å› å­ï¼ˆ6-bit æ‰“åŒ…ï¼‰ |
| d | 12 | 2 bytes | f16 | è¶…çº§å—ç¼©æ”¾ |
| dmin | 14 | 2 bytes | f16 | è¶…çº§å—æœ€å°å€¼ç¼©æ”¾ |
| data | 16 | 128 bytes | u8[128] | 256 ä¸ª 4-bit å€¼æ‰“åŒ… |

**Q8_0 Blockï¼ˆGGUFï¼‰**:

| å­—æ®µ | åç§» | å¤§å° | ç±»å‹ | è¯´æ˜ |
|------|------|------|------|------|
| data | 0 | 32 bytes | i8[32] | 32 ä¸ª 8-bit æœ‰ç¬¦å·å€¼ |
| scale | 32 | 2 bytes | f16 | ç¼©æ”¾å› å­ |

è§£ç å…¬å¼ï¼š`value[i] = data[i] * scale`

**AWQ INT4 Layout**:

| å­—æ®µ | å¤§å° | è¯´æ˜ |
|------|------|------|
| qweight | N/8 * M bytes | æ‰“åŒ…çš„ 4-bit æƒé‡ |
| scales | N/group_size * M * 2 bytes | æ¯ç»„ç¼©æ”¾å› å­ï¼ˆf16ï¼‰ |
| qzeros | N/group_size * M/8 bytes | æ‰“åŒ…çš„ 4-bit é›¶ç‚¹ |

è§£ç å…¬å¼ï¼š`value[i] = (qweight[i] - qzeros[group]) * scales[group]`

---

#### 4. Backend trait æ‰©å±•

**æ–°å¢æ–¹æ³•ç­¾å**:

| æ–¹æ³• | è¾“å…¥å‚æ•° | è¾“å‡º | è¯´æ˜ |
|------|----------|------|------|
| q4_matmul | input: &[f16], weight: Q4Tensor, output: &mut [f16], M, N, K | Result<()> | Q4_0 çŸ©é˜µä¹˜ |
| q4k_matmul | input: &[f16], weight: Q4KTensor, output: &mut [f16], M, N, K | Result<()> | Q4_K çŸ©é˜µä¹˜ |
| q8_matmul | input: &[f16], weight: Q8Tensor, output: &mut [f16], M, N, K | Result<()> | Q8_0 çŸ©é˜µä¹˜ |
| awq_matmul | input: &[f16], weight: AwqTensor, output: &mut [f16], M, N, K | Result<()> | AWQ INT4 çŸ©é˜µä¹˜ |

**å‚æ•°å®šä¹‰**:

| å‚æ•° | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| input | &[f16] | è¾“å…¥æ¿€æ´» [M, K] è¡Œä¼˜å…ˆ |
| weight | Q*Tensor | é‡åŒ–æƒé‡ |
| output | &mut [f16] | è¾“å‡º [M, N] è¡Œä¼˜å…ˆ |
| M | usize | batch * seq_len |
| N | usize | è¾“å‡ºç»´åº¦ï¼ˆhidden_sizeï¼‰ |
| K | usize | è¾“å…¥ç»´åº¦ |

---

#### 5. åç«¯å®ç°ä¼˜å…ˆçº§

éµå¾ª gllm-kernels åç«¯å®ç°ä¼˜å…ˆçº§ï¼š

| ä¼˜å…ˆçº§ | åç«¯ | å®ç°ç­–ç•¥ |
|--------|------|----------|
| P0 | CPU | å‚è€ƒå®ç°ï¼Œé€ block è§£ç  + FMA |
| P1 | WGPU | WGSL compute shaderï¼Œè·¨å¹³å° |
| P2 | CUDA | cuBLAS æ— åŸç”Ÿæ”¯æŒï¼Œè‡ªå®šä¹‰ kernel |
| P3 | Metal | Metal Performance Shaders æ‰©å±• |
| P4 | ROCm | HIP kernelï¼Œå‚è€ƒ CUDA å®ç° |

---

#### 6. WGSL Kernel è®¾è®¡è¦ç‚¹

**Q4_0 MatMul Kernel æ¶æ„**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Workgroup (16x16 threads)                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Shared Memory:                                           â”‚
â”‚   - tile_A[16][64]: f16, ä» input åŠ è½½                   â”‚
â”‚   - scales[64]: f16, ä» Q4 blocks åŠ è½½ scale             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è®¡ç®—æµç¨‹ï¼ˆæ¯ thread è´Ÿè´£ 1 ä¸ªè¾“å‡ºå…ƒç´ ï¼‰ï¼š                  â”‚
â”‚   1. åä½œåŠ è½½ input tile åˆ° shared memory                â”‚
â”‚   2. åä½œåŠ è½½ Q4 scales åˆ° shared memory                 â”‚
â”‚   3. é€ block å¾ªç¯ï¼š                                     â”‚
â”‚      a. åŠ è½½ Q4 packed data (16 bytes = 32 values)       â”‚
â”‚      b. In-register dequant: nibble â†’ f16               â”‚
â”‚      c. FMA: acc += dequant_val * input_val             â”‚
â”‚   4. å†™å› output                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ä¼˜åŒ–ç‚¹ï¼š                                                 â”‚
â”‚   - å‘é‡åŒ–åŠ è½½ï¼ˆvec4<u32>ï¼‰                              â”‚
â”‚   - Scale ç¼“å­˜åˆ° shared memory                          â”‚
â”‚   - å±•å¼€å†…å±‚å¾ªç¯ï¼ˆ#pragma unrollï¼‰                       â”‚
â”‚   - ä½¿ç”¨ fma() æŒ‡ä»¤                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Workgroup é…ç½®**:

| é‡åŒ–æ ¼å¼ | workgroup_size | tile_M | tile_N | tile_K |
|----------|---------------|--------|--------|--------|
| Q4_0 | 256 (16x16) | 16 | 16 | 64 |
| Q4_K | 256 (16x16) | 16 | 16 | 256 |
| Q8_0 | 256 (16x16) | 16 | 16 | 32 |
| AWQ | 256 (16x16) | 16 | 16 | 128 |

---

#### 7. CUDA Kernel è®¾è®¡è¦ç‚¹

**Q4_0 MatMul CUDA Kernel**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Grid: (N/128, M/128)                                     â”‚
â”‚ Block: (128 threads)                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Shared Memory:                                           â”‚
â”‚   - smem_A[128][32+padding]: f16                        â”‚
â”‚   - smem_scales[32]: f16                                â”‚
â”‚   - smem_qdata[32][16]: u8 (Q4 packed)                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è®¡ç®—æµç¨‹ï¼š                                               â”‚
â”‚   1. åŒç¼“å†²é¢„å–ï¼š                                        â”‚
â”‚      - Buffer 0 è®¡ç®—æ—¶ï¼ŒBuffer 1 åŠ è½½ä¸‹ä¸€ tile           â”‚
â”‚   2. å‘é‡åŒ–åŠ è½½ï¼š                                        â”‚
â”‚      - LDG.128 åŠ è½½ Q4 packed data                       â”‚
â”‚      - __half2 å‘é‡åŒ– FMA                                â”‚
â”‚   3. Warp-level ä¼˜åŒ–ï¼š                                   â”‚
â”‚      - __shfl_sync äº¤æ¢ partial sum                      â”‚
â”‚      - Warp-level reduce                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ä¼˜åŒ–ç‚¹ï¼š                                                 â”‚
â”‚   - ä½¿ç”¨ __half2 è¿›è¡Œ 2-way SIMD                         â”‚
â”‚   - Shared memory bank conflict é¿å…ï¼ˆpaddingï¼‰          â”‚
â”‚   - å¯„å­˜å™¨åˆ†å—ï¼ˆregister tilingï¼‰                        â”‚
â”‚   - åŒç¼“å†²éšè—å†…å­˜å»¶è¿Ÿ                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### 8. gllm ä¾§æ¥å£è®¾è®¡

**NativeQLinear å±‚**:

| å±æ€§ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| weight | QTensor | é‡åŒ–æƒé‡ï¼ˆQ4/Q8/AWQï¼‰ |
| bias | Option<Vec<f16>> | å¯é€‰åç½® |
| in_features | usize | è¾“å…¥ç»´åº¦ |
| out_features | usize | è¾“å‡ºç»´åº¦ |
| quant_type | QuantType | é‡åŒ–ç±»å‹æšä¸¾ |

**QuantType æšä¸¾**:

| å€¼ | è¯´æ˜ |
|----|------|
| Q4_0 | GGUF Q4_0 æ ¼å¼ |
| Q4_K | GGUF Q4_K æ ¼å¼ |
| Q8_0 | GGUF Q8_0 æ ¼å¼ |
| AWQ_INT4 | AWQ INT4 æ ¼å¼ |

**QTensor ç»“æ„**:

| å­—æ®µ | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| data | Vec<u8> | é‡åŒ–æ•°æ®ï¼ˆåŸå§‹å­—èŠ‚ï¼‰ |
| shape | (usize, usize) | é€»è¾‘å½¢çŠ¶ (out_features, in_features) |
| quant_type | QuantType | é‡åŒ–ç±»å‹ |
| block_size | usize | å—å¤§å°ï¼ˆQ4_0=32, Q4_K=256, Q8_0=32ï¼‰ |

**forward æ–¹æ³•**:

| å‚æ•° | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| input | &[f16] | è¾“å…¥ [batch, seq_len, in_features] |
| è¿”å› | Result<Vec<f16>> | è¾“å‡º [batch, seq_len, out_features] |

---

#### 9. ä¸ç°æœ‰ç»„ä»¶é›†æˆ

**ä¸ QTensor/AwqWeight é›†æˆ**:

```
åŠ è½½æµç¨‹ï¼š
SafeTensors â†’ QTensorï¼ˆç°æœ‰ï¼‰â†’ NativeQLinearï¼ˆæ–°å¢ï¼‰
                  â”‚
                  â””â”€â†’ Backend::q*_matmulï¼ˆæ–°å¢ï¼‰

ç°æœ‰ä»£ç è·¯å¾„ï¼ˆä¿ç•™å…¼å®¹ï¼‰ï¼š
QTensor â†’ dequantize() â†’ f32 â†’ linear_forward

æ–°ä»£ç è·¯å¾„ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰ï¼š
QTensor â†’ NativeQLinear::forward â†’ Backend::q*_matmul
```

**æ£€æµ‹ä¸å›é€€æœºåˆ¶**:

| æ¡ä»¶ | è¡Œä¸º |
|------|------|
| Backend æ”¯æŒ q*_matmul | ä½¿ç”¨åŸç”Ÿ kernel |
| Backend ä¸æ”¯æŒ | å›é€€åˆ° dequantize + linear_forward |
| GPU kernel æ‰§è¡Œå¤±è´¥ | é™é»˜å›é€€ CPU åŸç”Ÿå®ç° |

---

#### 10. ç²¾åº¦éªŒè¯ç­–ç•¥

**éªŒè¯æ–¹æ³•**:

| æµ‹è¯•ç±»å‹ | éªŒè¯å†…å®¹ | é€šè¿‡æ ‡å‡† |
|----------|----------|----------|
| å•å…ƒæµ‹è¯• | å• block è§£ç æ­£ç¡®æ€§ | bit-exact |
| çŸ©é˜µä¹˜æµ‹è¯• | å°çŸ©é˜µ (128x128) ç»“æœ | ç›¸å¯¹è¯¯å·® < 1e-3 |
| æ¨¡å‹çº§æµ‹è¯• | å®Œæ•´æ¨ç†è¾“å‡º | perplexity å·®å¼‚ < 0.1% |
| å›å½’æµ‹è¯• | å¯¹æ¯” dequantize æ–¹æ¡ˆ | æœ€å¤§è¯¯å·® < 1% |

**æµ‹è¯•çŸ©é˜µ**:

| é‡åŒ–æ ¼å¼ | æµ‹è¯•è§„æ¨¡ | æ¯”è¾ƒåŸºå‡† |
|----------|----------|----------|
| Q4_0 | M=1,32,128, N=4096, K=4096 | dequantize + f32 matmul |
| Q4_K | M=1,32,128, N=4096, K=4096 | dequantize + f32 matmul |
| Q8_0 | M=1,32,128, N=4096, K=4096 | dequantize + f32 matmul |
| AWQ | M=1,32,128, N=4096, K=4096 | dequantize + f32 matmul |

---

#### 11. æ€§èƒ½é¢„æœŸ

| é‡åŒ–æ ¼å¼ | å†…å­˜èŠ‚çœ | å¸¦å®½èŠ‚çœ | è®¡ç®—åŠ é€Ÿ | ç²¾åº¦æŸå¤± |
|----------|----------|----------|----------|----------|
| Q4_0 | 75% | 75% | ~2x | < 1% |
| Q4_K | 75% | 75% | ~2x | < 0.5% |
| Q8_0 | 50% | 50% | ~1.5x | < 0.1% |
| AWQ INT4 | 75% | 75% | ~2x | < 1% |

**åŸºå‡†æµ‹è¯•åœºæ™¯**:
- æ¨¡å‹ï¼šQwen2.5-7B-Q4_0
- ç¡¬ä»¶ï¼šRTX 4090 / Apple M2 / AMD RX 7900
- æŒ‡æ ‡ï¼štokens/secï¼Œå†…å­˜å ç”¨ï¼Œé¦– token å»¶è¿Ÿ

**çŠ¶æ€**: âœ… è®¾è®¡å®Œæˆ

---

## æ•°æ®ç»“æ„å®¡è®¡ä¸é‡æ„è®¡åˆ’

### ARCH-AUDIT-001: æ•°æ®ç»“æ„ä¸ç±»å‹å®¡è®¡

**å®¡è®¡æ—¥æœŸ**: 2025-01-26

**å®¡è®¡èŒƒå›´**: gllm é¡¹ç›®æ ¸å¿ƒæ•°æ®æµä¸­çš„ç±»å‹å®šä¹‰ã€è½¬æ¢å¼€é”€ã€å†…å­˜åˆ†é…æ¨¡å¼

---

#### 1. ç±»å‹é‡å¤å®šä¹‰é—®é¢˜

| ç±»å‹ | ä½ç½® | å­˜å‚¨æ–¹å¼ | é—®é¢˜ |
|------|------|----------|------|
| `FFNWeights` | decoder_layer.rs:11-17 | `Vec<f32>` (gate/up/down) | ä¸ LinearWeights åŠŸèƒ½é‡å¤ |
| `LinearWeights` | weight_loader.rs:191-194 | `WeightMatrix` + `Option<WeightVector>` | æ›´è§„èŒƒçš„æŠ½è±¡ |
| `ExpertWeights` | moe_layer.rs:9-13 | `Vec<f32>` (gate/up/down) | ä¸ FFNWeights å®Œå…¨ç›¸åŒ |

**é—®é¢˜æè¿°**ï¼šåŒä¸€æ¦‚å¿µï¼ˆçº¿æ€§å±‚æƒé‡ï¼‰æœ‰å¤šç§è¡¨ç¤ºæ–¹å¼ï¼Œå¯¼è‡´ï¼š
- ä»£ç é‡å¤
- è½¬æ¢å¼€é”€ï¼ˆåœ¨ä¸åŒè¡¨ç¤ºé—´è½¬æ¢ï¼‰
- ç»´æŠ¤å›°éš¾

---

#### 2. æ•°æ®è½¬æ¢å¼€é”€é—®é¢˜

| è½¬æ¢ | ä½ç½® | é¢‘ç‡ | å¼€é”€ |
|------|------|------|------|
| F16/BF16 â†’ f32 | weight_loader.rs:141-185 `convert_to_f32()` | æ¯æ¬¡åŠ è½½ | **é«˜**ï¼šæ‰€æœ‰æƒé‡éƒ½è½¬æ¢ |
| Q4/Q8 â†’ f32 | quantized.rs `dequantize()` | æ¯æ¬¡æ¨ç† | **æé«˜**ï¼šæ¯æ¬¡ matmul éƒ½è§£é‡åŒ– |
| AWQ INT4 â†’ f32 | awq.rs:58 `dequantize()` | æ¯æ¬¡æ¨ç† | **æé«˜**ï¼šåŒä¸Š |
| LoadedTensor â†’ WeightMatrix | weight_loader.rs:57 `.clone()` | æ¯æ¬¡åŠ è½½ | **ä¸­**ï¼šä¸å¿…è¦çš„å¤åˆ¶ |
| LoadedTensor â†’ WeightVector | weight_loader.rs:69,77 `.clone()` | æ¯æ¬¡åŠ è½½ | **ä¸­**ï¼šåŒä¸Š |

**æ•°æ®æµå›¾ï¼ˆå½“å‰çŠ¶æ€ï¼‰**ï¼š

```
SafeTensors (F16/BF16)
    â”‚
    â”œâ”€â†’ convert_to_f32() â”€â”€â†’ Vec<f32> â”€â”€â†’ .clone() â”€â”€â†’ WeightMatrix
    â”‚       [CPUè½¬æ¢]         [åˆ†é…]        [å¤åˆ¶]        [åŒ…è£…]
    â”‚
    â””â”€â†’ ç†æƒ³è·¯å¾„ï¼šç›´æ¥ mmap ä¸º &[f16]ï¼ŒGPU ç«¯ä¿æŒ F16
```

**é‡åŒ–æ•°æ®æµï¼ˆå½“å‰çŠ¶æ€ï¼‰**ï¼š

```
GGUF/AWQ é‡åŒ–æƒé‡
    â”‚
    â””â”€â†’ QTensor (Vec<u8>)
            â”‚
            â””â”€â†’ dequantize() â”€â”€â†’ Vec<f32> â”€â”€â†’ linear_forward()
                  [æ¯æ¬¡æ¨ç†]      [4xå†…å­˜]     [è®¡ç®—]
```

---

#### 3. å†…å­˜åˆ†é…æ¨¡å¼é—®é¢˜

| ç»“æ„ | ä½ç½® | åˆ†é…æ¨¡å¼ | é—®é¢˜ |
|------|------|----------|------|
| `KVCache` | kv_cache.rs:9-13 | `Vec<Vec<f32>>` | æ¯å±‚ç‹¬ç«‹åˆ†é…ï¼Œå†…å­˜ç¢ç‰‡åŒ– |
| `forward()` è¿”å› | decoder_layer.rs:51 | è¿”å› `Vec<f32>` | æ¯æ¬¡è°ƒç”¨åˆ†é…æ–°å†…å­˜ |
| `ffn_forward()` | decoder_layer.rs:81-131 | åˆ†é… gate, up, output | ä¸‰ä¸ªä¸´æ—¶ Vec |
| `forward_with_cache()` | causal_attention.rs:154-217 | åˆ†é… q,k,v,attn_out,output | äº”ä¸ªä¸´æ—¶ Vec |
| `repeat_kv()` | causal_attention.rs:232-256 | å³ä½¿ repeat=1 ä¹Ÿåˆ†é… | ä¸å¿…è¦çš„å¤åˆ¶ |

**çƒ­è·¯å¾„å†…å­˜åˆ†é…ç»Ÿè®¡ï¼ˆå•æ¬¡ forwardï¼‰**ï¼š

| ä½ç½® | åˆ†é…æ¬¡æ•° | å¤§å° | å¯é¿å… |
|------|----------|------|--------|
| DecoderLayer.forward | 1 | seq_len * hidden_size | âœ… å¯å¤ç”¨ |
| ffn_forward | 3 | seq_len * intermediate_size Ã— 2 + seq_len * hidden_size | âœ… å¯å¤ç”¨ |
| forward_with_cache | 5 | q + k + v + attn_out + output | âœ… å¯å¤ç”¨ |
| repeat_kv | 2 | å½“ GQA repeat > 1 æ—¶å¿…è¦ | éƒ¨åˆ†å¯é¿å… |

---

#### 4. ä¸å¿…è¦çš„ clone/to_vec è°ƒç”¨

| ä½ç½® | ä»£ç  | é—®é¢˜ | ä¿®å¤å»ºè®® |
|------|------|------|----------|
| causal_attention.rs:235 | `return (k.to_vec(), v.to_vec())` | repeat=1 æ—¶ä¸å¿…è¦å¤åˆ¶ | è¿”å› Cow<[f32]> |
| decoder_layer.rs:61-65 | `.iter().zip().map().collect()` | åˆ›å»ºæ–° Vec | åŸåœ°æ“ä½œ |
| generator_model.rs:111 | `logits.to_vec()` | ç”¨äºæ¸©åº¦ç¼©æ”¾ | åŸåœ°æ“ä½œ |
| moe_generator_model.rs:160 | `logits.to_vec()` | åŒä¸Š | åŸåœ°æ“ä½œ |
| hooks.rs:299 | `hidden_states.to_vec()` | Hook æ•è· | å¯é€‰æ·±æ‹·è´ |

---

#### 5. ç±»å‹ä¸ä¸€è‡´é—®é¢˜

| æ•°æ® | å­˜å‚¨æ ¼å¼ | è®¡ç®—æ ¼å¼ | gllm-kernels æœŸæœ› |
|------|----------|----------|-------------------|
| æ¨¡å‹æƒé‡ | SafeTensors (F16/BF16) | Vec<f32> | &[f32] æˆ– WeightMatrix |
| KV Cache | Vec<f32> | &[f32] | TensorSlice::F32 |
| é‡åŒ–æƒé‡ | Vec<u8> (packed) | Vec<f32> (dequantized) | åŸç”Ÿ Q*Tensor |
| æ¿€æ´»å€¼ | Vec<f32> | &[f32] | TensorSlice |

**ç†æƒ³çŠ¶æ€**ï¼š
- æƒé‡ä¿æŒ F16 å­˜å‚¨ï¼ŒGPU ç«¯è®¡ç®—è½¬æ¢
- é‡åŒ–æƒé‡ä¿æŒ packed æ ¼å¼ï¼Œin-kernel dequantize
- æ¿€æ´»å€¼ä½¿ç”¨é¢„åˆ†é…ç¼“å†²åŒº

---

### ARCH-REFACTOR-001: é‡æ„å·¥ä½œè®¡åˆ’

**ç›®æ ‡**ï¼šæ¶ˆé™¤ä¸å¿…è¦çš„ç±»å‹è½¬æ¢å’Œå†…å­˜åˆ†é…ï¼Œæå‡æ¨ç†æ€§èƒ½

---

#### é˜¶æ®µ 1ï¼šç±»å‹ç»Ÿä¸€ï¼ˆä½é£é™©ï¼‰

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | å½±å“èŒƒå›´ | é¢„æœŸæ”¶ç›Š |
|------|--------|----------|----------|
| 1.1 ç»Ÿä¸€ FFNWeights å’Œ LinearWeights | P1 | decoder_layer, moe_layer | ä»£ç ç®€åŒ– |
| 1.2 ç§»é™¤ LoadedTensor.clone() | P0 | weight_loader.rs | å‡å°‘åŠ è½½æ—¶å†…å­˜å ç”¨ 50% |
| 1.3 ä½¿ç”¨ Cow<[f32]> æ›¿ä»£ to_vec() | P1 | causal_attention, decoder_layer | å‡å°‘ä¸å¿…è¦å¤åˆ¶ |

**1.1 è¯¦ç»†è®¾è®¡**ï¼š

```
ç°çŠ¶ï¼š
â”œâ”€ FFNWeights { gate_proj: Vec<f32>, ... }
â””â”€ LinearWeights { weight: WeightMatrix, ... }

ç›®æ ‡ï¼š
â””â”€ ç»Ÿä¸€ä½¿ç”¨ LinearWeightsï¼ŒFFNWeights æ”¹ä¸ºï¼š
   struct FFN {
       gate: LinearWeights,
       up: LinearWeights,
       down: LinearWeights,
   }
```

**1.2 è¯¦ç»†è®¾è®¡**ï¼š

```
ç°çŠ¶ï¼š
LoadedTensor::to_weight_matrix(&self) â†’ WeightMatrix::new(self.data.clone(), ...)

ç›®æ ‡ï¼š
LoadedTensor::into_weight_matrix(self) â†’ WeightMatrix::new(self.data, ...)
                                         ^^ æ¶ˆè´¹ selfï¼Œé¿å… clone
```

---

#### é˜¶æ®µ 2ï¼šå†…å­˜å¤ç”¨ï¼ˆä¸­é£é™©ï¼‰

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | å½±å“èŒƒå›´ | é¢„æœŸæ”¶ç›Š |
|------|--------|----------|----------|
| 2.1 å¼•å…¥ ScratchBuffer å·¥ä½œåŒº | P0 | decoder_layer, causal_attention | å‡å°‘ 80% ä¸´æ—¶åˆ†é… |
| 2.2 KVCache è¿ç»­å†…å­˜å¸ƒå±€ | P1 | kv_cache.rs | å‡å°‘ç¢ç‰‡ï¼Œæå‡ç¼“å­˜å‘½ä¸­ |
| 2.3 forward() æ¥å—è¾“å‡ºç¼“å†²åŒº | P1 | æ‰€æœ‰ forward æ–¹æ³• | å…è®¸å¤–éƒ¨ç®¡ç†å†…å­˜ |

**2.1 ScratchBuffer è®¾è®¡**ï¼š

```
struct ScratchBuffer {
    // é¢„åˆ†é…çš„å·¥ä½œåŒºï¼ŒæŒ‰æœ€å¤§éœ€æ±‚åˆ†é…
    ffn_gate: Vec<f32>,      // max_batch * max_seq * intermediate_size
    ffn_up: Vec<f32>,        // åŒä¸Š
    ffn_output: Vec<f32>,    // max_batch * max_seq * hidden_size
    attn_q: Vec<f32>,        // max_batch * max_seq * num_heads * head_dim
    attn_k: Vec<f32>,        // åŒä¸Šï¼ˆKV headsï¼‰
    attn_v: Vec<f32>,        // åŒä¸Š
    attn_out: Vec<f32>,      // åŒä¸Š
}

impl ScratchBuffer {
    fn new(config: &ModelConfig, max_batch: usize, max_seq: usize) -> Self;
    fn get_ffn_workspace(&mut self, batch: usize, seq: usize) -> FFNWorkspace;
    fn get_attn_workspace(&mut self, batch: usize, seq: usize) -> AttnWorkspace;
}
```

**2.2 KVCache è¿ç»­å¸ƒå±€è®¾è®¡**ï¼š

```
ç°çŠ¶ï¼š
KVCache {
    k_cache: Vec<Vec<f32>>,  // [num_layers][max_seq * kv_heads * head_dim]
    v_cache: Vec<Vec<f32>>,  // æ¯å±‚ç‹¬ç«‹åˆ†é…
}

ç›®æ ‡ï¼š
KVCache {
    // å•æ¬¡åˆ†é…ï¼Œè¿ç»­å†…å­˜
    data: Vec<f32>,  // [2 * num_layers * max_seq * kv_heads * head_dim]

    fn k_layer(&self, layer: usize) -> &[f32];  // è§†å›¾è®¿é—®
    fn v_layer(&self, layer: usize) -> &[f32];
}
```

---

#### é˜¶æ®µ 3ï¼šé›¶æ‹·è´åŠ è½½ï¼ˆé«˜æ”¶ç›Šï¼‰

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | å½±å“èŒƒå›´ | é¢„æœŸæ”¶ç›Š |
|------|--------|----------|----------|
| 3.1 F16 ç›´é€šåŠ è½½ | P0 | weight_loader | æ¶ˆé™¤ F16â†’f32 è½¬æ¢ |
| 3.2 mmap æƒé‡è®¿é—® | P1 | weight_loader, safetensors | é›¶å†…å­˜å¤åˆ¶åŠ è½½ |
| 3.3 GPU ç«¯ F16 è®¡ç®— | P2 | Backend trait | è¿›ä¸€æ­¥å‡å°‘è½¬æ¢ |

**3.1 F16 ç›´é€šè®¾è®¡**ï¼š

```
ç°çŠ¶ï¼š
SafeTensors (F16) â†’ convert_to_f32() â†’ Vec<f32> â†’ WeightMatrix

ç›®æ ‡ï¼ˆä¿æŒ API å…¼å®¹ï¼‰ï¼š
SafeTensors (F16) â†’ WeightMatrixF16 â†’ Backend::linear_f16()
                                      â†“
                                 ï¼ˆä»…åœ¨ CPU å›é€€æ—¶è½¬æ¢ä¸º f32ï¼‰
```

**3.2 mmap è®¾è®¡**ï¼š

```
struct MmapWeight<'a> {
    data: &'a [u8],      // mmap ç›´æ¥å¼•ç”¨
    shape: (usize, usize),
    dtype: Dtype,
}

impl MmapWeight {
    fn as_f16_slice(&self) -> &[f16];  // é›¶æ‹·è´è§†å›¾
    fn to_f32_vec(&self) -> Vec<f32>;  // æŒ‰éœ€è½¬æ¢ï¼ˆå›é€€è·¯å¾„ï¼‰
}
```

---

#### é˜¶æ®µ 4ï¼šé‡åŒ–ç›´é€šï¼ˆä¾èµ– ARCH-ADR-011ï¼‰

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | ä¾èµ– | é¢„æœŸæ”¶ç›Š |
|------|--------|------|----------|
| 4.1 æ¶ˆé™¤ dequantize å¼€é”€ | P0 | ARCH-ADR-011 å®ç° | 4x å†…å­˜èŠ‚çœ + 2x åŠ é€Ÿ |
| 4.2 åŸç”Ÿé‡åŒ– KV Cache | P2 | 4.1 å®Œæˆ | è¿›ä¸€æ­¥å‹ç¼© Cache |

---

#### å®æ–½ä¼˜å…ˆçº§çŸ©é˜µ

| ä¼˜å…ˆçº§ | ä»»åŠ¡ | é£é™© | æ”¶ç›Š | ä¾èµ– |
|--------|------|------|------|------|
| **P0** | 1.2 ç§»é™¤ clone() | ä½ | ä¸­ | æ—  |
| **P0** | 2.1 ScratchBuffer | ä¸­ | é«˜ | æ—  |
| **P0** | 3.1 F16 ç›´é€š | ä¸­ | é«˜ | æ—  |
| P1 | 1.1 ç±»å‹ç»Ÿä¸€ | ä½ | ä½ | æ—  |
| P1 | 1.3 Cow æ›¿ä»£ to_vec | ä½ | ä½ | æ—  |
| P1 | 2.2 KVCache è¿ç»­åŒ– | ä¸­ | ä¸­ | æ—  |
| P1 | 2.3 è¾“å‡ºç¼“å†²åŒºå‚æ•° | ä¸­ | ä¸­ | 2.1 |
| P1 | 3.2 mmap æƒé‡ | é«˜ | é«˜ | 3.1 |
| P2 | 3.3 GPU F16 è®¡ç®— | é«˜ | é«˜ | 3.1 + Backend æ”¯æŒ |
| P2 | 4.1 é‡åŒ–ç›´é€š | é«˜ | æé«˜ | ARCH-ADR-011 |

---

#### éªŒæ”¶æ ‡å‡†

| é˜¶æ®µ | æŒ‡æ ‡ | åŸºå‡† | ç›®æ ‡ |
|------|------|------|------|
| é˜¶æ®µ 1 | åŠ è½½æ—¶å†…å­˜å³°å€¼ | 100% | â‰¤ 50% |
| é˜¶æ®µ 2 | æ¨ç†æ—¶ä¸´æ—¶åˆ†é…æ¬¡æ•° | ~10 æ¬¡/forward | â‰¤ 2 æ¬¡/forward |
| é˜¶æ®µ 3 | æ¨¡å‹åŠ è½½æ—¶é—´ (7B) | åŸºå‡† | â‰¤ 50% |
| é˜¶æ®µ 4 | é‡åŒ–æ¨¡å‹æ¨ç†é€Ÿåº¦ | dequant æ–¹æ¡ˆ | â‰¥ 2x |

**çŠ¶æ€**: âœ… å®¡è®¡å®Œæˆï¼Œå¾…å®æ–½

---

### ARCH-AUDIT-002: ä»£ç çº§æ¶æ„è¿è§„å®¡è®¡

**å®¡è®¡æ—¥æœŸ**: 2025-01-26

**å®¡è®¡èŒƒå›´**: gllm å’Œ gllm-kernels é¡¹ç›®ä¸­è¿åæ¶æ„åŸåˆ™çš„ä»£ç æ¨¡å¼

---

#### 1. å†…å­˜-GPU æ•°æ®ç§»åŠ¨è¿è§„ï¼ˆ12å¤„ï¼‰

**åŸåˆ™**: CPU è®¡ç®—å…¨åœ¨å†…å­˜ï¼ŒGPU è®¡ç®—å…¨åœ¨æ˜¾å­˜ï¼Œæ•°æ®ä¼ è¾“ä»…åœ¨æ¨ç†å¼€å§‹/ç»“æŸæ—¶å‘ç”Ÿ

| ä½ç½® | æ–‡ä»¶:è¡Œå· | é—®é¢˜æè¿° | ä¼˜å…ˆçº§ |
|------|-----------|----------|--------|
| WGPU ç±»å‹è½¬æ¢ | wgpu_backend.rs:142-144 | çƒ­è·¯å¾„ Tâ†’f32 è½¬æ¢åˆ›å»ºä¸´æ—¶ Vec | P0 |
| Staging Buffer | wgpu_backend.rs:231-283 | readback_f32() æ¯æ¬¡åˆ›å»º staging buffer | P0 |
| RmsNorm | wgpu_backend.rs:314-315 | GPU buffer åˆ›å»º+readback æ¯æ¬¡è°ƒç”¨ | P0 |
| Softmax | wgpu_backend.rs:349 | åŒä¸Š | P1 |
| Linear | wgpu_backend.rs:388-389 | åŒä¸Š | P0 |
| Attention | wgpu_backend.rs:446 | åŒä¸Š | P0 |
| Apply Temp | wgpu_backend.rs:498 | åŒä¸Š | P1 |
| Paged Attn | paged_attn/dispatch.rs:130-164 | æ¯æ¬¡ attention åˆ›å»º 7 ä¸ª GPU buffers | **P0** |
| repeat_kv | causal_attention.rs:232-255 | repeat=1 æ—¶ä»æ‰§è¡Œå¤åˆ¶ | P1 |
| Layer å¾ªç¯ | generator_model.rs:70-81 | æ¯å±‚åˆ›å»ºæ–° hidden Vecï¼ˆ80å±‚=80æ¬¡åˆ†é…ï¼‰ | P0 |
| FFN åˆ†é… | decoder_layer.rs:81-131 | çƒ­è·¯å¾„ 3 æ¬¡ Vec åˆ†é… | P0 |
| Residual | decoder_layer.rs:61-65 | æ‰‹å†™ add åˆ›å»ºæ–° Vec | P1 |

**ä¿®å¤æ–¹æ¡ˆ**:
- **GPU Buffer Pool**: åœ¨ WgpuBackend å¼•å…¥ç¼“å†²æ± ï¼Œé¿å…æ¯æ¬¡è°ƒç”¨åˆ›å»º/é”€æ¯
- **Staging Buffer å¤ç”¨**: ç»´æŠ¤å¯å¤ç”¨çš„ staging buffer æ± 
- **ScratchBuffer**: ä¸ºçƒ­è·¯å¾„é¢„åˆ†é…å·¥ä½œåŒºï¼ˆè§ ARCH-REFACTOR-001 Â§2.1ï¼‰

---

#### 2. åç«¯é€‰æ‹©è¿è§„ï¼ˆ4å¤„ï¼‰

**åŸåˆ™**: åç«¯é€‰æ‹©ä»…åœ¨ç¨‹åºå¯åŠ¨æ—¶æ‰§è¡Œä¸€æ¬¡ï¼ˆOnceLock æ¨¡å¼ï¼‰

| ä½ç½® | æ–‡ä»¶:è¡Œå· | é—®é¢˜æè¿° | å½±å“ |
|------|-----------|----------|------|
| build_embedding_backend | engine.rs:505-510 | é‡å¤è°ƒç”¨ detect_backend() | å†—ä½™æ£€æµ‹ |
| build_rerank_backend | engine.rs:523-530 | åŒä¸Š | åŒä¸Š |
| build_generator_backend | engine.rs:590-599 | åŒä¸Š | åŒä¸Š |
| build_backend | engine.rs:626-635 | åŒä¸Š | åŒä¸Š |

**æ ¹å› åˆ†æ**:
- `Engine::new()` è°ƒç”¨ `auto_select_backend()`
- å„ `build_*_backend()` æ–¹æ³•å†æ¬¡è°ƒç”¨ `detect_backend()`
- åŒä¸€ Engine å®ä¾‹å†…é‡å¤æ£€æµ‹åç«¯

**ä¿®å¤æ–¹æ¡ˆ**:
- å®ç° ARCH-OPT-003 Backend é™æ€åŒ–ï¼ˆOnceLock æ¨¡å¼ï¼‰
- ç§»é™¤ `build_*_backend()` ä¸­çš„ `detect_backend()` è°ƒç”¨
- æ‰€æœ‰åç«¯è·å–é€šè¿‡ `get_backend()` é™æ€æ–¹æ³•

---

#### 3. ä¸å¿…è¦çš„æ•°æ®å¤åˆ¶ï¼ˆ12å¤„ï¼‰

**åŸåˆ™**: é›¶æ‹·è´/é›¶åˆ†é…ï¼Œé¿å…ä¸å¿…è¦çš„ Vec åˆ†é…ã€.clone()ã€.to_vec()

| ä½ç½® | æ–‡ä»¶:è¡Œå· | é—®é¢˜æè¿° | æ•°æ®é‡ | ä¼˜å…ˆçº§ |
|------|-----------|----------|--------|--------|
| repeat_kv | causal_attention.rs:235 | repeat=1 æ—¶ .to_vec() | ~MB | P1 |
| sample | generator_model.rs:111 | temperature=1.0 æ—¶ logits.to_vec() | ~KB | P2 |
| æƒé‡åŠ è½½ | weight_loader.rs:57 | .clone() åˆ›å»º WeightMatrix | **GBçº§** | **P0** |
| æƒé‡åŠ è½½ | weight_loader.rs:69 | .clone() åˆ›å»º bias | MBçº§ | P0 |
| æƒé‡åŠ è½½ | weight_loader.rs:77 | .clone() åˆ›å»º norm | MBçº§ | P0 |
| KV Cache | paged_attention.rs:80 | get_kv() å…‹éš†æ•´ä¸ª cache | **MBçº§/æ¯æ¬¡** | **P0** |
| MoE token | moe_layer.rs:89 | å¾ªç¯å†… Vec åˆ†é… | KBÃ—tokens | P1 |
| MoE gate | moe_layer.rs:114 | expert_forward å†…åˆ†é… | KBÃ—experts | P1 |
| MoE up | moe_layer.rs:126 | åŒä¸Š | åŒä¸Š | P1 |
| MoE down | moe_layer.rs:146 | åŒä¸Š | åŒä¸Š | P1 |
| Pooling | engine.rs:325 | pooled_rows æ¯è¡Œ .to_vec() | KBÃ—rows | P2 |
| MoE sample | moe_generator_model.rs:160 | åŒ generator_model.rs:111 | ~KB | P2 |

**ä¿®å¤æ–¹æ¡ˆ**:
- **æ‰€æœ‰æƒè½¬ç§»**: `into_weight_matrix(self)` æ›¿ä»£ `.clone()`
- **Cow<[f32]>**: æ¡ä»¶å¤åˆ¶ï¼Œä»…å¿…è¦æ—¶åˆ†é…
- **åŸåœ°æ“ä½œ**: ä½¿ç”¨ gllm-kernels çš„ `*_inplace` ç®—å­
- **é¢„åˆ†é…å·¥ä½œåŒº**: ScratchBuffer é¿å…å¾ªç¯å†…åˆ†é…

---

#### 4. ç®—å­ä½¿ç”¨ä¸æ­£ç¡®ï¼ˆ11å¤„ï¼‰

**åŸåˆ™**: ä½¿ç”¨ gllm-kernels å‘é‡åŒ–ç®—å­ï¼Œç¦æ­¢æ‰‹å†™æ ‡é‡å¾ªç¯

| ä½ç½® | æ–‡ä»¶:è¡Œå· | é—®é¢˜ä»£ç  | æ­£ç¡®ç®—å­ | ä¼˜å…ˆçº§ |
|------|-----------|----------|----------|--------|
| Residual add | decoder_layer.rs:61-65 | `.iter().zip().map(a+b).collect()` | `add_inplace()` | P0 |
| SwiGLU | decoder_layer.rs:113-116 | åˆ†å¼€çš„ silu + mul | `silu_mul_inplace()` | P0 |
| ReLU | dynamic_bert.rs:508-514 | `if x < 0 { 0 } else { x }` å¾ªç¯ | `relu_inplace()` | P1 |
| Tanh | dynamic_bert.rs:518-522 | `x.tanh()` å¾ªç¯ | `tanh_inplace()` | P1 |
| Sigmoid | dynamic_bert.rs:524-526 | `1.0 / (1.0 + (-x).exp())` å¾ªç¯ | `sigmoid_scalar()` | P1 |
| Pos Embed | dynamic_bert.rs:571-588 | 3 å±‚åµŒå¥—å¾ªç¯åŠ ä½ç½®åµŒå…¥ | `add_inplace()` | P0 |
| Pos Embed | dynamic_bert.rs:590-610 | åŒä¸Šï¼ˆå¦ä¸€ä¸ª caseï¼‰ | `add_inplace()` | P0 |

**ä¿®å¤æ–¹æ¡ˆ**:
- æ›¿æ¢æ‰‹å†™å¾ªç¯ä¸º gllm-kernels ç®—å­è°ƒç”¨
- è€ƒè™‘ç®—å­èåˆï¼ˆå¦‚ residual_add_rms_norm_fusedï¼‰
- ç¡®ä¿ SIMD å‘é‡åŒ–ï¼ˆå·²åœ¨ gllm-kernels CPU åç«¯å®ç°ï¼‰

---

#### 5. è¿è§„ç»Ÿè®¡ä¸ä¼˜å…ˆçº§æ±‡æ€»

| ç±»åˆ« | è¿è§„æ•° | P0 | P1 | P2 | æ€§èƒ½å½±å“ |
|------|--------|----|----|----|---------|
| å†…å­˜-GPU æ•°æ®ç§»åŠ¨ | 12 | 6 | 5 | 1 | **æé«˜** |
| åç«¯é€‰æ‹© | 4 | 4 | 0 | 0 | ä¸­ |
| ä¸å¿…è¦æ•°æ®å¤åˆ¶ | 12 | 4 | 5 | 3 | **é«˜** |
| ç®—å­ä½¿ç”¨ä¸æ­£ç¡® | 11 | 4 | 3 | 4 | é«˜ |
| **æ€»è®¡** | **39** | **18** | **13** | **8** | - |

**é¢„æœŸæ”¶ç›Š**:

| ä¿®å¤ç±»åˆ« | é¢„æœŸåŠ é€Ÿ | å†…å­˜èŠ‚çœ |
|----------|----------|----------|
| GPU Buffer Pool | 2-5xï¼ˆWGPU è·¯å¾„ï¼‰ | å‡å°‘ GPU å†…å­˜ç¢ç‰‡ |
| åç«¯å•ä¾‹åŒ– | å¾®ä¼˜åŒ– | ä»£ç ç®€åŒ– |
| é›¶æ‹·è´åŠ è½½ | 1.5-2x åŠ è½½é€Ÿåº¦ | **50% å³°å€¼å†…å­˜** |
| ç®—å­æ›¿æ¢ | 1.5-3xï¼ˆCPU è·¯å¾„ï¼‰ | - |

**çŠ¶æ€**: âœ… å®¡è®¡å®Œæˆ

---

### ARCH-REFACTOR-002: ä»£ç çº§é‡æ„è®¡åˆ’

**åŸºäº**: ARCH-AUDIT-002 å‘ç°

---

#### é˜¶æ®µ 0ï¼šGPU ç¼“å†²ç®¡ç†ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | å½±å“æ–‡ä»¶ | é¢„æœŸæ”¶ç›Š |
|------|--------|----------|----------|
| 0.1 WgpuBackend å¼•å…¥ BufferPool | **P0** | wgpu_backend.rs | 2-5x GPU è·¯å¾„åŠ é€Ÿ |
| 0.2 Staging Buffer å¤ç”¨ | **P0** | wgpu_backend.rs | å‡å°‘ GPU åŒæ­¥å¼€é”€ |
| 0.3 Paged Attention ç¼“å†²æ±  | **P0** | paged_attn/dispatch.rs | æ¯æ¬¡ attn å‡å°‘ 7 æ¬¡åˆ†é… |

**0.1 BufferPool è®¾è®¡**:

```
struct BufferPool {
    // æŒ‰å¤§å°åˆ†æ¡¶çš„å¯å¤ç”¨ buffer
    buckets: HashMap<usize, Vec<wgpu::Buffer>>,

    fn acquire(&mut self, size: usize) -> wgpu::Buffer;  // è·å–æˆ–åˆ›å»º
    fn release(&mut self, buffer: wgpu::Buffer);         // å½’è¿˜å¤ç”¨
}

ä½¿ç”¨æ¨¡å¼ï¼š
let buffer = pool.acquire(size);
// ä½¿ç”¨ buffer...
pool.release(buffer);  // ä¸é”€æ¯ï¼Œå½’è¿˜æ± ä¸­
```

**0.2 Staging Buffer å¤ç”¨è®¾è®¡**:

```
struct StagingBufferManager {
    read_staging: Option<wgpu::Buffer>,   // å¤ç”¨çš„è¯»å– staging
    write_staging: Option<wgpu::Buffer>,  // å¤ç”¨çš„å†™å…¥ staging
    max_size: usize,                      // å½“å‰æœ€å¤§å®¹é‡

    fn get_read_staging(&mut self, size: usize) -> &wgpu::Buffer;
    fn get_write_staging(&mut self, size: usize) -> &wgpu::Buffer;
}
```

---

#### é˜¶æ®µ 1ï¼šåç«¯å•ä¾‹åŒ–

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | å½±å“æ–‡ä»¶ | é¢„æœŸæ”¶ç›Š |
|------|--------|----------|----------|
| 1.1 å®ç° ARCH-OPT-003 OnceLock | **P0** | backend.rs, engine.rs | ä»£ç ç®€åŒ– |
| 1.2 ç§»é™¤å†—ä½™ detect_backend() | P0 | engine.rs | æ¶ˆé™¤å¯åŠ¨æ—¶å†—ä½™æ£€æµ‹ |

---

#### é˜¶æ®µ 2ï¼šé›¶æ‹·è´æƒé‡åŠ è½½

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | å½±å“æ–‡ä»¶ | é¢„æœŸæ”¶ç›Š |
|------|--------|----------|----------|
| 2.1 into_weight_matrix() | **P0** | weight_loader.rs | 50% åŠ è½½å†…å­˜èŠ‚çœ |
| 2.2 KV Cache å¼•ç”¨è¿”å› | **P0** | paged_attention.rs | æ¶ˆé™¤æ¯æ¬¡ get çš„ MB çº§å¤åˆ¶ |
| 2.3 Cow<[f32]> æ¡ä»¶å¤åˆ¶ | P1 | causal_attention.rs | å‡å°‘ä¸å¿…è¦å¤åˆ¶ |

**2.2 KV Cache å¼•ç”¨è®¾è®¡**:

```
ç°çŠ¶ï¼ˆè¿è§„ï¼‰ï¼š
fn get_kv(&self) -> (Vec<f32>, Vec<f32>) {
    (keys.clone(), values.clone())  // MB çº§å¤åˆ¶
}

ç›®æ ‡ï¼š
fn get_kv(&self) -> (&[f32], &[f32]) {  // é›¶æ‹·è´å¼•ç”¨
    (&self.keys, &self.values)
}

// éœ€è¦ä¿®æ”¹æ—¶æ‰å¤åˆ¶
fn get_kv_mut(&mut self) -> (&mut [f32], &mut [f32]);
```

---

#### é˜¶æ®µ 3ï¼šç®—å­æ›¿æ¢

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | å½±å“æ–‡ä»¶ | æ­£ç¡®ç®—å­ |
|------|--------|----------|----------|
| 3.1 Residual add æ›¿æ¢ | **P0** | decoder_layer.rs | `add_inplace()` |
| 3.2 SwiGLU èåˆ | **P0** | decoder_layer.rs | `silu_mul_inplace()` |
| 3.3 ä½ç½®åµŒå…¥å‘é‡åŒ– | **P0** | dynamic_bert.rs | `add_inplace()` |
| 3.4 æ¿€æ´»å‡½æ•°æ›¿æ¢ | P1 | dynamic_bert.rs | `relu/tanh/sigmoid_inplace()` |

---

#### é˜¶æ®µ 4ï¼šçƒ­è·¯å¾„å†…å­˜å¤ç”¨

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | å½±å“æ–‡ä»¶ | é¢„æœŸæ”¶ç›Š |
|------|--------|----------|----------|
| 4.1 ScratchBufferï¼ˆè§ ARCH-REFACTOR-001 Â§2.1ï¼‰ | **P0** | å…¨å±€ | å‡å°‘ 80% ä¸´æ—¶åˆ†é… |
| 4.2 MoE å¾ªç¯ä¼˜åŒ– | P1 | moe_layer.rs | å‡å°‘å¾ªç¯å†…åˆ†é… |
| 4.3 Generator hidden å¤ç”¨ | P0 | generator_model.rs | æ¶ˆé™¤æ¯å±‚åˆ†é… |

**4.3 Generator hidden å¤ç”¨è®¾è®¡**:

```
ç°çŠ¶ï¼ˆè¿è§„ï¼‰ï¼š
for layer in &self.layers {
    let new_hidden = layer.forward(&hidden, ...);  // æ¯å±‚åˆ†é…
    hidden = new_hidden;
}

ç›®æ ‡ï¼š
let mut hidden_a = vec![0.0; size];  // é¢„åˆ†é…åŒç¼“å†²
let mut hidden_b = vec![0.0; size];
for (i, layer) in self.layers.iter().enumerate() {
    let (input, output) = if i % 2 == 0 {
        (&hidden_a, &mut hidden_b)
    } else {
        (&hidden_b, &mut hidden_a)
    };
    layer.forward_into(input, output, ...);  // åŸåœ°å†™å…¥
}
```

---

#### å®æ–½ä¼˜å…ˆçº§çŸ©é˜µï¼ˆæ›´æ–°ï¼‰

| ä¼˜å…ˆçº§ | ä»»åŠ¡ | é£é™© | æ”¶ç›Š | ä¾èµ– |
|--------|------|------|------|------|
| **P0** | 0.1 WgpuBackend BufferPool | ä¸­ | **æé«˜** | æ—  |
| **P0** | 0.2 Staging Buffer å¤ç”¨ | ä½ | é«˜ | æ—  |
| **P0** | 0.3 Paged Attention ç¼“å†²æ±  | ä¸­ | **æé«˜** | 0.1 |
| **P0** | 1.1 OnceLock åç«¯å•ä¾‹ | ä½ | ä¸­ | æ—  |
| **P0** | 2.1 into_weight_matrix | ä½ | é«˜ | æ—  |
| **P0** | 2.2 KV Cache å¼•ç”¨è¿”å› | ä¸­ | **é«˜** | æ—  |
| **P0** | 3.1 Residual add æ›¿æ¢ | ä½ | ä¸­ | æ—  |
| **P0** | 3.2 SwiGLU èåˆ | ä½ | ä¸­ | æ—  |
| **P0** | 4.3 Generator hidden å¤ç”¨ | ä¸­ | é«˜ | æ—  |
| P1 | 1.2 ç§»é™¤å†—ä½™ detect | ä½ | ä½ | 1.1 |
| P1 | 2.3 Cow æ¡ä»¶å¤åˆ¶ | ä½ | ä½ | æ—  |
| P1 | 3.3 ä½ç½®åµŒå…¥å‘é‡åŒ– | ä½ | ä¸­ | æ—  |
| P1 | 3.4 æ¿€æ´»å‡½æ•°æ›¿æ¢ | ä½ | ä½ | æ—  |
| P1 | 4.2 MoE å¾ªç¯ä¼˜åŒ– | ä¸­ | ä¸­ | 4.1 |

---

#### éªŒæ”¶æ ‡å‡†ï¼ˆæ›´æ–°ï¼‰

| é˜¶æ®µ | æŒ‡æ ‡ | åŸºå‡† | ç›®æ ‡ |
|------|------|------|------|
| é˜¶æ®µ 0 | WGPU å•æ¬¡ forward GPU åˆ†é… | ~10 æ¬¡ | â‰¤ 2 æ¬¡ |
| é˜¶æ®µ 1 | åç«¯æ£€æµ‹è°ƒç”¨æ¬¡æ•° | ~4 æ¬¡/Engine | 1 æ¬¡/è¿›ç¨‹ |
| é˜¶æ®µ 2 | æ¨¡å‹åŠ è½½å³°å€¼å†…å­˜ | 100% | â‰¤ 50% |
| é˜¶æ®µ 3 | CPU ç®—å­è€—æ—¶ï¼ˆBERT forwardï¼‰ | åŸºå‡† | â‰¤ 50% |
| é˜¶æ®µ 4 | æ¨ç†æ—¶ä¸´æ—¶åˆ†é…æ¬¡æ•° | ~10 æ¬¡/forward | â‰¤ 2 æ¬¡/forward |

**çŠ¶æ€**: âœ… å®¡è®¡å®Œæˆï¼Œå¾…å®æ–½

---

### ARCH-ROADMAP-001: å®Œæ•´ä¼˜åŒ–è·¯çº¿å›¾ï¼ˆCPU/GPU åˆ†ç±»ï¼‰

> **ç›®çš„**ï¼šæ±‡æ€»æ‰€æœ‰å®¡è®¡å‘ç°å’Œå¾…å®æ–½ä¼˜åŒ–ä»»åŠ¡ï¼Œæ˜ç¡®æ ‡æ³¨æ¯ä¸ªä»»åŠ¡çš„åç«¯ç±»å‹

#### åç«¯ç±»å‹è¯´æ˜

| æ ‡è®° | å«ä¹‰ | è¯´æ˜ |
|------|------|------|
| **[CPU]** | CPU åç«¯ | çº¯ Rust å®ç°ï¼Œæ—  GPU ä¾èµ– |
| **[WGPU]** | WGPU åç«¯ | è·¨å¹³å° GPUï¼ˆVulkan/DX12/Metalï¼‰ |
| **[CUDA]** | CUDA åç«¯ | NVIDIA GPU ä¸“ç”¨ |
| **[Metal]** | Metal åç«¯ | Apple Silicon ä¸“ç”¨ |
| **[ROCm]** | ROCm åç«¯ | AMD GPU ä¸“ç”¨ |
| **[å…¨åç«¯]** | æ‰€æœ‰åç«¯ | æ¶æ„å±‚å˜æ›´ï¼Œå½±å“æ‰€æœ‰åç«¯ |

---

#### ä¸€ã€P0-GPU: GPU æ ¸å¿ƒç®—å­åŠ é€Ÿï¼ˆgllm-kernelsï¼‰

> **æ¥æº**ï¼šARCH-OPT-001 GPU åŠ é€Ÿä¼˜åŒ–ç©ºé—´
> **çŠ¶æ€**: âœ… å·²å®Œæˆ (2026-01-26) - æ‰€æœ‰æ ¸å¿ƒç®—å­å·²æœ‰ GPU å®ç°

| ID | ä»»åŠ¡ | åç«¯ | å½±å“ | é¢„æœŸæ”¶ç›Š | çŠ¶æ€ |
|----|------|------|------|----------|------|
| P0-GPU-1 | linear_forward GPU å®ç° | **[WGPU]** **[CUDA]** **[Metal]** **[ROCm]** | è®¡ç®—é‡ 70% | 8-16x åŠ é€Ÿ | âœ… |
| P0-GPU-2 | rms_norm GPU å®ç° | **[WGPU]** **[CUDA]** **[Metal]** **[ROCm]** | æ¯å±‚ 2 æ¬¡è°ƒç”¨ | 2-5x åŠ é€Ÿ | âœ… |
| P0-GPU-3 | silu_inplace GPU å®ç° | **[WGPU]** **[CUDA]** **[Metal]** **[ROCm]** | element-wise | 3-5x åŠ é€Ÿ | âœ… |

---

#### äºŒã€P0-MEM: å†…å­˜ä¸ Buffer ç®¡ç†ä¼˜åŒ–

> **æ¥æº**ï¼šARCH-AUDIT-002 å†…å­˜-GPU æ•°æ®ç§»åŠ¨è¿è§„ï¼ˆ12 å¤„ï¼‰
> **çŠ¶æ€**: âœ… å…¨éƒ¨å®Œæˆ (2026-01-26) [commit: 10cb10a, c34ff82]

| ID | ä»»åŠ¡ | åç«¯ | ä½ç½® | é—®é¢˜ | çŠ¶æ€ |
|----|------|------|------|------|------|
| P0-MEM-1 | BufferPool å•ä¾‹å®ç° | **[WGPU]** | wgpu_backend.rs | æ¯æ¬¡ forward é‡æ–°åˆ†é… | âœ… |
| P0-MEM-2 | StagingBuffer å¤ç”¨ | **[WGPU]** | wgpu_backend.rs | æ¯æ¬¡ map_read é‡æ–°åˆ›å»º | âœ… |
| P0-MEM-3 | PagedAttention dispatch ä¼˜åŒ– | **[WGPU]** | paged_attn/dispatch.rs | çƒ­è·¯å¾„ Vec åˆ†é… | âœ… |
| P0-MEM-4 | KV Cache buffer é¢„åˆ†é… | **[CPU]** | generator_model.rs:445-467 | é€ token æ‰©å±• | âœ… å·²å®ç° |
| P0-MEM-5 | decoder attention è¾“å‡ºå¤ç”¨ | **[CPU]** | decoder_layer.rs:178-195 | é‡å¤ .to_vec() | âœ… |
| P0-MEM-6 | WeightLoader é›¶æ‹·è´ | **[CPU]** | weight_loader.rs:89-156 | SafeTensor å¤šæ¬¡å…‹éš† | âœ… |
| P0-MEM-7 | PagedAttention ç´¢å¼•é¢„åˆ†é… | **[CPU]** | paged_attention.rs:234-278 | å¾ªç¯å†… Vec æ‰©å±• | âœ… |
| P0-MEM-8 | MoE routing æ‰¹é‡è®¡ç®— | **[CPU]** | moe_layer.rs:123-189 | é€ token .to_vec() | âœ… |

---

#### ä¸‰ã€P0-ARCH: æ¶æ„å±‚ä¼˜åŒ–

> **æ¥æº**ï¼šARCH-AUDIT-002 åç«¯é€‰æ‹©è¿è§„ï¼ˆ4 å¤„ï¼‰
> **çŠ¶æ€**: âœ… å·²å®Œæˆ (2026-01-26) [commit: 10cb10a]

| ID | ä»»åŠ¡ | åç«¯ | ä½ç½® | é—®é¢˜ | çŠ¶æ€ |
|----|------|------|------|------|------|
| P0-ARCH-1 | OnceLock å•æ¬¡åç«¯æ£€æµ‹ | **[å…¨åç«¯]** | engine.rs:505-635 | æ¯æ¬¡åˆå§‹åŒ–é‡å¤æ£€æµ‹ | âœ… |
| P0-ARCH-2 | BackendType é™æ€ç¡®å®š | **[å…¨åç«¯]** | engine.rs | è¿è¡Œæ—¶é‡å¤åˆ¤æ–­ | âœ… |

---

#### å››ã€P0-OPS: ç®—å­å®ç°æ›¿æ¢

> **æ¥æº**ï¼šARCH-AUDIT-002 æœªæ­£ç¡®ä½¿ç”¨ gllm-kernels ç®—å­ï¼ˆ11 å¤„ï¼‰
> **çŠ¶æ€**: âœ… å·²éªŒè¯ (2026-01-26) - ä»£ç å·²ä½¿ç”¨ gllm_kernels ç®—å­

| ID | ä»»åŠ¡ | åç«¯ | ä½ç½® | å½“å‰å®ç° | æ›¿æ¢ä¸º | çŠ¶æ€ |
|----|------|------|------|----------|--------|------|
| P0-OPS-1 | softmax æ›¿æ¢ | **[CPU]** | decoder_layer.rs:234-256 | æ‰‹å†™å¾ªç¯ | gllm_kernels::softmax | âœ… å·²ä½¿ç”¨ |
| P0-OPS-2 | layer_norm æ›¿æ¢ | **[CPU]** | dynamic_bert.rs:178-201 | æ‰‹å†™å¾ªç¯ | gllm_kernels::layer_norm | âœ… å·²ä½¿ç”¨ |
| P0-OPS-3 | gelu æ›¿æ¢ | **[CPU]** | dynamic_bert.rs:156-167 | æ‰‹å†™ tanh è¿‘ä¼¼ | gllm_kernels::gelu | âœ… å·²ä½¿ç”¨ |
| P0-OPS-4 | rope_embedding æ›¿æ¢ | **[CPU]** | decoder_layer.rs:289-334 | æ‰‹å†™ä¸‰è§’å‡½æ•° | gllm_kernels::rope | âœ… å·²ä½¿ç”¨ |
| P0-OPS-5 | attention_scores æ›¿æ¢ | **[CPU]** | decoder_layer.rs:178-195 | æ‰‹å†™ matmul+scale | gllm_kernels::attention | âœ… FlashAttn |
| P0-OPS-6 | cross_entropy æ›¿æ¢ | **[CPU]** | generator_model.rs:234-256 | æ‰‹å†™å¾ªç¯ | gllm_kernels::cross_entropy | N/A æ— æ­¤åœºæ™¯ |

---

#### äº”ã€P1-LOAD: æ¨¡å‹åŠ è½½ä¼˜åŒ–

> **æ¥æº**ï¼šARCH-ADR-010 å¼‚æ­¥å¹¶è¡Œæ¨¡å‹åŠ è½½
> **çŠ¶æ€**: âœ… å·²å®Œæˆ (2026-01-26) [commit: cd9338f] REQ-LOAD-001

| ID | ä»»åŠ¡ | åç«¯ | è¯´æ˜ | çŠ¶æ€ |
|----|------|------|------|------|
| P1-LOAD-1 | AsyncShardLoader å®ç° | **[CPU]** | å¤šåˆ†ç‰‡å¹¶è¡Œä¸‹è½½ | âœ… parallel_parser.rs |
| P1-LOAD-2 | MmapWeightLoader å®ç° | **[CPU]** | å†…å­˜æ˜ å°„æƒé‡åŠ è½½ | âœ… ShardBytes::Mmap |
| P1-LOAD-3 | è¿›åº¦å›è°ƒæ¥å£ | **[CPU]** | LoadProgress trait | âœ… LoadProgress |

---

#### å…­ã€P2-QUANT: åŸç”Ÿé‡åŒ–æ¨ç†

> **æ¥æº**ï¼šARCH-ADR-011 åŸç”Ÿé‡åŒ–æ¨ç† Kernel
> **çŠ¶æ€**: âœ… CPU å‚è€ƒå®ç°å·²å®Œæˆ (2026-01-26) [commit: 66af66e, e94a31ee] REQ-QUANT-001

| ID | ä»»åŠ¡ | åç«¯ | è¯´æ˜ | çŠ¶æ€ |
|----|------|------|------|------|
| P2-QUANT-1 | Q4_0 æ•°æ®ç»“æ„ | **[CPU]** | Block å®šä¹‰ | âœ… gllm-kernels |
| P2-QUANT-2 | Q4_0 WGSL kernel | **[WGPU]** | in-kernel dequant | ğŸ”² |
| P2-QUANT-3 | Q4_0 CUDA kernel | **[CUDA]** | shared memory ä¼˜åŒ– | ğŸ”² |
| P2-QUANT-4 | AWQ æ•°æ®ç»“æ„ | **[CPU]** | AwqPackedWeight | âœ… gllm-kernels |
| P2-QUANT-5 | AWQ WGSL kernel | **[WGPU]** | åˆ†ç»„åé‡åŒ– | ğŸ”² |
| P2-QUANT-6 | AWQ CUDA kernel | **[CUDA]** | tensor core åˆ©ç”¨ | ğŸ”² |

---

#### ä¸ƒã€P1-MISC: å…¶ä»–ä¼˜åŒ–

> **æ¥æº**ï¼šARCH-AUDIT-001 æ•°æ®ç»“æ„å®¡è®¡ã€ARCH-OPT-001 æ— çŠ¶æ€ç®—æ³•

| ID | ä»»åŠ¡ | åç«¯ | è¯´æ˜ |
|----|------|------|------|
| P1-MISC-1 | TokenizerWrapper ç®€åŒ– | **[CPU]** | ç§»é™¤å†—ä½™å­—æ®µ |
| P1-MISC-2 | ScratchBuffer å®ç° | **[CPU]** | çƒ­è·¯å¾„é¢„åˆ†é…å·¥ä½œåŒº |
| P1-MISC-3 | PromptCache å®ç° | **[CPU]** | ç›¸åŒ prompt å¤ç”¨ |
| P1-MISC-4 | ç®—å­çº¯å‡½æ•°åŒ– | **[CPU]** | æ— çŠ¶æ€ç®—æ³•é›†åˆ |
| P1-MISC-5 | KV å‹ç¼©ç­–ç•¥ | **[CPU]** | é•¿åºåˆ—å†…å­˜ä¼˜åŒ– |

---

#### ä»»åŠ¡ç»Ÿè®¡

| åç«¯ç±»å‹ | ä»»åŠ¡æ•° | å æ¯” |
|----------|--------|------|
| **[CPU]** | 20 | 60.6% |
| **[WGPU]** | 6 | 18.2% |
| **[CUDA]** | 3 | 9.1% |
| **[Metal]** | 3 | 9.1% |
| **[ROCm]** | 3 | 9.1% |
| **[å…¨åç«¯]** | 2 | 6.1% |
| **æ€»è®¡** | 33 | - |

> æ³¨ï¼šP0-GPU ä»»åŠ¡éœ€è¦åœ¨ WGPU/CUDA/Metal/ROCm å››ä¸ªåç«¯åˆ†åˆ«å®ç°ï¼Œå› æ­¤æ¯ä¸ªä»»åŠ¡è®¡ä¸º 4 ä¸ªåç«¯

---

#### æ‰§è¡Œè·¯çº¿å›¾

```
Phase 1: P0 æ ¸å¿ƒä¼˜åŒ–ï¼ˆå»ºè®®é¡ºåºï¼‰
â”œâ”€ P0-GPU-1 linear_forward â†’ æœ€å¤§æ”¶ç›Šï¼ˆ70%è®¡ç®—é‡ï¼‰
â”œâ”€ P0-MEM-1/2 BufferPool â†’ å‡å°‘ GPU åˆ†é…å¼€é”€
â”œâ”€ P0-ARCH-1/2 OnceLock â†’ æ¶æ„åŸºç¡€
â””â”€ P0-OPS-1~6 ç®—å­æ›¿æ¢ â†’ CPU æ€§èƒ½æå‡

Phase 2: P1 å¢å¼ºä¼˜åŒ–
â”œâ”€ P1-LOAD-1~3 â†’ å¤§æ¨¡å‹åŠ è½½ä½“éªŒ
â”œâ”€ P1-MISC-1~5 â†’ å†…å­˜å’Œç¼“å­˜ä¼˜åŒ–
â””â”€ P0-GPU-2/3 â†’ è¡¥å…… GPU ç®—å­

Phase 3: P2 é«˜çº§ç‰¹æ€§
â””â”€ P2-QUANT-1~6 â†’ é‡åŒ–æ¨ç†æ”¯æŒ
```

---

#### éªŒæ”¶æ ‡å‡†æ±‡æ€»

| æŒ‡æ ‡ | åŸºå‡† | ç›®æ ‡ | æ¥æº |
|------|------|------|------|
| WGPU forward GPU åˆ†é… | ~10 æ¬¡ | â‰¤ 2 æ¬¡ | P0-MEM |
| åç«¯æ£€æµ‹è°ƒç”¨ | ~4 æ¬¡/Engine | 1 æ¬¡/è¿›ç¨‹ | P0-ARCH |
| æ¨¡å‹åŠ è½½å³°å€¼å†…å­˜ | 100% | â‰¤ 50% | P1-LOAD |
| CPU ç®—å­è€—æ—¶ | åŸºå‡† | â‰¤ 50% | P0-OPS |
| linear_forward å»¶è¿Ÿ | CPU åŸºå‡† | â‰¥ 8x åŠ é€Ÿ | P0-GPU-1 |
| é‡åŒ–æ¨¡å‹å†…å­˜ | FP16 åŸºå‡† | Q4: 25%, Q8: 50% | P2-QUANT |

**çŠ¶æ€**: âš ï¸ è·¯çº¿å›¾éœ€æ›´æ–°ï¼ˆå‘ç° P0-CRITICAL çº§åˆ«é—®é¢˜ï¼‰

---

### ARCH-AUDIT-003: CPU/GPU å†…å­˜æ¶æ„å®¡è®¡ï¼ˆå…³é”®ç¼ºé™·ï¼‰

> **ä¸¥é‡ç¨‹åº¦**ï¼šğŸ”´ CRITICAL - å½“å‰æ¶æ„å¯¼è‡´ 50-100x æ€§èƒ½æŸå¤±
>
> **æ ¹æœ¬é—®é¢˜**ï¼šæ•°æ®å§‹ç»ˆåœ¨ CPU å†…å­˜ï¼ŒGPU æ²¦ä¸º"è¿œç¨‹åå¤„ç†å™¨"

#### é—®é¢˜æ¦‚è¿°

| æŒ‡æ ‡ | å½“å‰çŠ¶æ€ | ç†æƒ³çŠ¶æ€ | å·®è· |
|------|----------|----------|------|
| å• token å»¶è¿Ÿ | 5-10 ç§’ | 50-100ms | **50-100x** |
| GPU æ˜¾å­˜åˆ©ç”¨ç‡ | <10% | >80% | **8x** |
| PCIe ä¼ è¾“/token | ~256MB | ~0 | âˆ |
| ç”Ÿæˆ 100 tokens | 8-12 åˆ†é’Ÿ | 5-10 ç§’ | **60-100x** |

#### æ•°æ®æµå¯¹æ¯”

**å½“å‰æµç¨‹ï¼ˆé”™è¯¯ï¼‰**ï¼š
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CPU å†…å­˜ (RAM)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ æƒé‡      â”‚  â”‚ KV Cache â”‚  â”‚ æ¿€æ´»å€¼    â”‚  â”‚ ä¸­é—´ç»“æœ  â”‚    â”‚
â”‚  â”‚ Vec<f32> â”‚  â”‚ Vec<f32> â”‚  â”‚ Vec<f32> â”‚  â”‚ Vec<f32> â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â”‚
â”‚       â”‚             â”‚             â”‚             â”‚           â”‚
â”‚       â–¼             â–¼             â–¼             â–¼           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              PCIe æ€»çº¿ (ç“¶é¢ˆ: 32GB/s)                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚             â”‚             â”‚             â”‚           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–¼             â–¼             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      GPU æ˜¾å­˜ (VRAM)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ ä¸´æ—¶ Bufferï¼ˆæ¯æ¬¡ç®—å­è°ƒç”¨é‡æ–°åˆ†é…ï¼Œç”¨å®Œé‡Šæ”¾ï¼‰          â”‚ â”‚
â”‚  â”‚ ä»…ç”¨äº Flash Attention ç­‰å•ä¸ªç®—å­                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ¯ç”Ÿæˆ 1 ä¸ª tokenï¼š
  - ä¸Šä¼ æƒé‡: ~14GB (7B æ¨¡å‹)
  - ä¸Šä¼  KV Cache: ~256MB (ctx=2048)
  - ä¸Šä¼ æ¿€æ´»: ~æ•° MB
  - ä¸‹è½½ç»“æœ: ~æ•° MB
  - PCIe å¾€è¿”: ~14GB Ã— 2 = 28GB
  - ä¼ è¾“æ—¶é—´: 28GB / 32GB/s = ~0.9 ç§’ (ä»…ä¼ è¾“!)
```

**ç†æƒ³æµç¨‹ï¼ˆGPU é©»ç•™ï¼‰**ï¼š
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CPU å†…å­˜ (RAM)                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ä»…å­˜æ”¾ï¼šè¾“å…¥ token IDsã€è¾“å‡º logitsã€é…ç½®             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â”‚ (ä¸€æ¬¡æ€§åŠ è½½)      â–² (ä»…ä¸‹è½½ logits, KB çº§)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–¼                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      GPU æ˜¾å­˜ (VRAM)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ æƒé‡      â”‚  â”‚ KV Cache â”‚  â”‚ æ¿€æ´»å€¼    â”‚  â”‚ ä¸­é—´ç»“æœ  â”‚  â”‚
â”‚  â”‚ GpuBufferâ”‚  â”‚ GpuBufferâ”‚  â”‚ GpuBufferâ”‚  â”‚ GpuBufferâ”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                           â”‚
â”‚  æ‰€æœ‰è®¡ç®—åœ¨ GPU ä¸Šå®Œæˆï¼Œé›¶ PCIe å¾€è¿”                       â”‚
â”‚  - Embedding lookup (GPU)                                 â”‚
â”‚  - Linear Q/K/V (GPU)                                     â”‚
â”‚  - RoPE (GPU)                                             â”‚
â”‚  - Flash Attention (GPU)                                  â”‚
â”‚  - FFN (GPU)                                              â”‚
â”‚  - LayerNorm (GPU)                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ¯ç”Ÿæˆ 1 ä¸ª tokenï¼š
  - PCIe ä¼ è¾“: ~0 (æƒé‡å·²åœ¨ GPU)
  - GPU è®¡ç®—: ~50-100ms
  - ä¸‹è½½ logits: ~128KB (vocab_size Ã— 4 bytes)
```

---

#### é—®é¢˜æ¸…å•ï¼ˆæŒ‰ä¸¥é‡ç¨‹åº¦æ’åºï¼‰

| ID | é—®é¢˜ | ä½ç½® | å½“å‰å®ç° | å½±å“ |
|----|------|------|----------|------|
| **CRIT-1** | **KV Cache åœ¨ CPU** | kv_cache.rs:8-19 | `Vec<Vec<f32>>` | **50x æ€§èƒ½æŸå¤±** |
| **CRIT-2** | **æƒé‡åœ¨ CPU** | weights.rs:17-28 | `Vec<f32>` | **10x æ€§èƒ½æŸå¤±** |
| **CRIT-3** | **çº¿æ€§æŠ•å½±åœ¨ CPU** | causal_attention.rs:219-228 | CPU matmul | **5x æ€§èƒ½æŸå¤±** |
| HIGH-1 | ä¸­é—´æ¿€æ´»åœ¨ CPU | causal_attention.rs:167-174 | `Vec<f32>` | 3x æ€§èƒ½æŸå¤± |
| HIGH-2 | RoPE åœ¨ CPU | causal_attention.rs:177-178 | CPU è®¡ç®— | 2x æ€§èƒ½æŸå¤± |
| HIGH-3 | repeat_kv åœ¨ CPU | causal_attention.rs:232-255 | CPU å¾ªç¯ | 2x æ€§èƒ½æŸå¤± |
| MED-1 | æ—  Buffer æŠ½è±¡ | backend_trait.rs | æ— ç»Ÿä¸€æ¥å£ | æ¶æ„é—®é¢˜ |
| MED-2 | æ— è®¡ç®—å›¾èåˆ | å…¨é“¾è·¯ | é€ç®—å­æ‰§è¡Œ | 5x æ€§èƒ½æŸå¤± |

---

#### ARCH-BUFFER-001: GPU Buffer æŠ½è±¡å±‚è®¾è®¡

##### æ ¸å¿ƒæ¦‚å¿µ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TensorBuffer Trait                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ + location() -> MemoryLocation                          â”‚
â”‚ + shape() -> &[usize]                                   â”‚
â”‚ + dtype() -> DType                                      â”‚
â”‚ + as_cpu_slice() -> Option<&[T]>                        â”‚
â”‚ + as_gpu_ptr() -> Option<GpuPtr>                        â”‚
â”‚ + to_device(device: Device) -> Result<Self>             â”‚
â”‚ + download() -> Result<CpuBuffer>                       â”‚
â”‚ + upload_from(data: &[T]) -> Result<()>                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–²                              â–²
           â”‚                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     CpuBuffer       â”‚      â”‚      GpuBuffer        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ data: Vec<T>        â”‚      â”‚ ptr: *mut c_void        â”‚
â”‚ shape: Vec<usize>   â”‚      â”‚ size: usize             â”‚
â”‚                     â”‚      â”‚ device: Device          â”‚
â”‚                     â”‚      â”‚ backend: BackendType    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â–²
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                  â”‚                  â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  WgpuBuffer    â”‚ â”‚  CudaBuffer    â”‚ â”‚  MetalBuffer   â”‚
           â”‚  (wgpu::Buffer)â”‚ â”‚  (CUdeviceptr) â”‚ â”‚  (MTLBuffer)   â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### å†…å­˜ä½ç½®æšä¸¾

| ä½ç½® | è¯´æ˜ | ä¼ è¾“æˆæœ¬ |
|------|------|----------|
| `MemoryLocation::Cpu` | CPU ä¸»å†…å­˜ | - |
| `MemoryLocation::Gpu(device_id)` | GPU æ˜¾å­˜ | CPUâ†”GPU éœ€è¦ PCIe |
| `MemoryLocation::Unified` | ç»Ÿä¸€å†…å­˜ï¼ˆæ”¯æŒçš„å¹³å°ï¼‰ | è‡ªåŠ¨è¿ç§» |
| `MemoryLocation::Pinned` | é”é¡µå†…å­˜ | å¿«é€Ÿ DMA |

##### æƒé‡å®¹å™¨å‡çº§

| å­—æ®µ | å½“å‰ | å‡çº§å |
|------|------|--------|
| WeightMatrix.data | `Vec<f32>` | `Arc<dyn TensorBuffer>` |
| WeightVector.data | `Vec<f32>` | `Arc<dyn TensorBuffer>` |
| KVCache.k_cache | `Vec<Vec<f32>>` | `Vec<GpuBuffer>` |
| KVCache.v_cache | `Vec<Vec<f32>>` | `Vec<GpuBuffer>` |

##### å…³é”®æ¥å£

| æ¥å£ | è¯´æ˜ |
|------|------|
| `GpuBuffer::alloc(size, device)` | åœ¨ GPU ä¸Šåˆ†é…æ˜¾å­˜ |
| `GpuBuffer::from_cpu(data)` | CPUâ†’GPU ä¸Šä¼  |
| `GpuBuffer::to_cpu()` | GPUâ†’CPU ä¸‹è½½ |
| `GpuBuffer::copy_from(other)` | GPU å†…æ‹·è´ï¼ˆé›¶ PCIeï¼‰ |
| `BufferPool::get(size)` | ä»æ± ä¸­è·å–é¢„åˆ†é… buffer |
| `BufferPool::release(buffer)` | å½’è¿˜åˆ°æ± ä¸­ |

---

#### ARCH-RESIDENT-001: GPU é©»ç•™æ¨ç†è®¾è®¡

##### åˆå§‹åŒ–é˜¶æ®µï¼ˆä¸€æ¬¡æ€§ï¼‰

| æ­¥éª¤ | æ“ä½œ | æ•°æ®ä½ç½® |
|------|------|----------|
| 1 | åŠ è½½ SafeTensors åˆ° CPU | CPU |
| 2 | ä¸Šä¼ æƒé‡åˆ° GPU | CPU â†’ **GPU** |
| 3 | é‡Šæ”¾ CPU æƒé‡å‰¯æœ¬ | CPU é‡Šæ”¾ |
| 4 | é¢„åˆ†é… KV Cache buffers | **GPU** |
| 5 | é¢„åˆ†é…æ¿€æ´» buffers | **GPU** |

##### æ¨ç†é˜¶æ®µï¼ˆæ¯ tokenï¼‰

| æ­¥éª¤ | æ“ä½œ | æ•°æ®ä½ç½® | PCIe ä¼ è¾“ |
|------|------|----------|-----------|
| 1 | ä¸Šä¼  token IDs | CPU â†’ GPU | ~4 bytes |
| 2 | Embedding lookup | GPU | 0 |
| 3 | 32Ã—Decoder Layer | GPU | 0 |
| 4 | LM Head | GPU | 0 |
| 5 | ä¸‹è½½ logits | GPU â†’ CPU | ~128KB |

**æ€» PCIe ä¼ è¾“**: ~128KB/token (å¯¹æ¯”å½“å‰ ~28GB/token)

##### Decoder Layer å†…éƒ¨ï¼ˆå…¨ GPUï¼‰

```
è¾“å…¥ (GpuBuffer)
    â”‚
    â”œâ”€â†’ RMSNorm (GPU) â”€â†’ Q/K/V Linear (GPU) â”€â†’ RoPE (GPU)
    â”‚                                              â”‚
    â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚   â–¼
    â”‚   Flash Attention (GPU) â† KV Cache (GpuBuffer, åŸåœ°æ›´æ–°)
    â”‚   â”‚
    â”‚   â–¼
    â”‚   Output Linear (GPU)
    â”‚   â”‚
    â”œâ”€â”€â”€â”´â”€â†’ Residual Add (GPU)
    â”‚
    â”œâ”€â†’ RMSNorm (GPU) â”€â†’ Gate/Up Linear (GPU) â”€â†’ SiLU (GPU)
    â”‚                                              â”‚
    â”‚                                              â–¼
    â”‚                                         Down Linear (GPU)
    â”‚                                              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â†’ Residual Add (GPU)
                                                              â”‚
                                                              â–¼
                                                         è¾“å‡º (GpuBuffer)
```

---

#### æ›´æ–°åçš„ä¼˜åŒ–è·¯çº¿å›¾

##### P0-CRITICAL: GPU é©»ç•™ä¼˜åŒ–ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰

| ID | ä»»åŠ¡ | åç«¯ | é¢„æœŸæ”¶ç›Š | å¤æ‚åº¦ |
|----|------|------|----------|--------|
| **P0-CRIT-1** | **TensorBuffer æŠ½è±¡å±‚** | **[å…¨åç«¯]** | æ¶æ„åŸºç¡€ | é«˜ |
| **P0-CRIT-2** | **æƒé‡ GPU é©»ç•™** | **[WGPU][CUDA][Metal][ROCm]** | **10x** | ä¸­ |
| **P0-CRIT-3** | **KV Cache GPU é©»ç•™** | **[WGPU][CUDA][Metal][ROCm]** | **50x** | ä¸­ |
| **P0-CRIT-4** | **æ¿€æ´»å€¼ GPU é©»ç•™** | **[WGPU][CUDA][Metal][ROCm]** | **5x** | ä¸­ |
| **P0-CRIT-5** | **å…¨é“¾è·¯ GPU æ¨ç†** | **[WGPU][CUDA][Metal][ROCm]** | **3x** | é«˜ |

##### P0-CRITICAL ä¾èµ–å…³ç³»

```
P0-CRIT-1 (Buffer æŠ½è±¡)
    â”‚
    â”œâ”€â†’ P0-CRIT-2 (æƒé‡ GPU)
    â”‚       â”‚
    â”‚       â””â”€â†’ P0-CRIT-5 (å…¨é“¾è·¯)
    â”‚               â–²
    â”œâ”€â†’ P0-CRIT-3 (KV Cache GPU) â”€â”˜
    â”‚
    â””â”€â†’ P0-CRIT-4 (æ¿€æ´» GPU) â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### å®Œæ•´ä¼˜å…ˆçº§çŸ©é˜µï¼ˆæ›´æ–°ï¼‰

| ä¼˜å…ˆçº§ | ç±»åˆ« | ä»»åŠ¡æ•° | é¢„æœŸæ€»æ”¶ç›Š |
|--------|------|--------|------------|
| **P0-CRITICAL** | GPU é©»ç•™ | 5 | **50-100x** |
| P0-GPU | GPU ç®—å­ | 3 | 8-16x |
| P0-MEM | å†…å­˜ç®¡ç† | 8 | 2-5x |
| P0-ARCH | æ¶æ„ä¼˜åŒ– | 2 | 2x |
| P0-OPS | ç®—å­æ›¿æ¢ | 6 | 2x |
| P1-LOAD | æ¨¡å‹åŠ è½½ | 3 | 2x |
| P2-QUANT | é‡åŒ–æ¨ç† | 6 | 4x |
| P1-MISC | å…¶ä»– | 5 | 1.5x |

---

#### éªŒæ”¶æ ‡å‡†ï¼ˆP0-CRITICALï¼‰

| æŒ‡æ ‡ | å½“å‰ | ç›®æ ‡ | éªŒè¯æ–¹æ³• |
|------|------|------|----------|
| å• token å»¶è¿Ÿ | 5-10s | â‰¤100ms | benchmark |
| GPU æ˜¾å­˜åˆ©ç”¨ç‡ | <10% | >80% | nvidia-smi |
| PCIe ä¼ è¾“/token | ~28GB | <1MB | profiler |
| æƒé‡ä½ç½® | CPU | GPU | ä»£ç æ£€æŸ¥ |
| KV Cache ä½ç½® | CPU | GPU | ä»£ç æ£€æŸ¥ |
| æ¨ç†å…¨é“¾è·¯ | CPU+GPU | å…¨ GPU | profiler |

**çŠ¶æ€**: ğŸ”´ å¾…å®æ–½ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰

---

### ARCH-EXECUTION-PLAN-001: é›¶å¦¥åæè‡´æ€§èƒ½æ‰§è¡Œè®¡åˆ’

> **é“å¾‹**ï¼š
> - 42 ä¸ªä»»åŠ¡å…¨éƒ¨å¿…é¡»å®Œæˆï¼Œæ— ä¸€ä¾‹å¤–
> - æ‰€æœ‰åç«¯ï¼ˆCPU/WGPU/CUDA/Metal/ROCmï¼‰åŒæ­¥å®ç°ï¼Œä¸å­˜åœ¨"ä¼˜å…ˆæŸåç«¯"
> - æ‰€æœ‰éªŒæ”¶æ ‡å‡†å¿…é¡» 100% è¾¾æˆï¼Œä¸æ¥å—"åŸºæœ¬è¾¾åˆ°"
> - ä¾èµ–å…³ç³»æ˜¯å”¯ä¸€çš„ä¸²è¡Œçº¦æŸï¼Œæ— ä¾èµ–å³å¹¶è¡Œ

---

#### ä»»åŠ¡æ¸…å•ï¼ˆå…¨éƒ¨å¼ºåˆ¶ï¼‰

| ç±»åˆ« | ä»»åŠ¡æ•° | éªŒæ”¶æ ‡å‡† |
|------|--------|----------|
| CRITICAL | 5 | å• token â‰¤50msï¼ŒGPU æ˜¾å­˜åˆ©ç”¨ç‡ >90% |
| GPU-OPS | 3Ã—5åç«¯=15 | å…¨åç«¯å®ç°ï¼Œæ€§èƒ½è¾¾åˆ°ç†è®ºå³°å€¼ 80% |
| MEM | 8 | forward å†…å­˜åˆ†é… = 0ï¼ˆå…¨é¢„åˆ†é…ï¼‰ |
| ARCH | 2 | åç«¯æ£€æµ‹ = 1æ¬¡/è¿›ç¨‹ï¼Œé›¶è¿è¡Œæ—¶å¼€é”€ |
| OPS | 6 | å…¨éƒ¨æ›¿æ¢ä¸º gllm-kernelsï¼Œé›¶æ‰‹å†™å¾ªç¯ |
| REFACTOR | 4 | é›¶ `dyn Trait`ï¼Œå…¨ enum é™æ€æ´¾å‘ |
| LOAD | 3 | 70B æ¨¡å‹åŠ è½½ â‰¤30sï¼ˆNVMe SSDï¼‰ |
| QUANT | 6 | Q4 ç²¾åº¦æŸå¤± <0.5%ï¼Œé€Ÿåº¦ â‰¥FP16 |
| MISC | 5 | å…¨éƒ¨å®ç°ï¼Œæ— é—ç•™ |
| **æ€»è®¡** | **42** | **å…¨éƒ¨ PASS** |

---

#### ä¾èµ–å›¾ï¼ˆå”¯ä¸€çº¦æŸï¼‰

```
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚    STAGE 0       â”‚
                                    â”‚   åŸºç¡€æ¶æ„å±‚      â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                                   â”‚                                   â”‚
         â–¼                                   â–¼                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   REFACTOR-4    â”‚               â”‚   ARCH-1 + 2    â”‚               â”‚   æ— ä¾èµ–ä»»åŠ¡     â”‚
â”‚ BackendImpl     â”‚               â”‚   OnceLock      â”‚               â”‚ REFACTOR-1,2,3  â”‚
â”‚ [gllm-kernels]  â”‚               â”‚   BackendType   â”‚               â”‚ MISC-1,2,3,4    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ å®Œæˆåè§£é”
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                    STAGE 1                                          â”‚
â”‚                              GPU åŸºç¡€è®¾æ–½å±‚                                          â”‚
â”‚                                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ CRIT-1      â”‚  â”‚ GPU-1       â”‚  â”‚ GPU-2       â”‚  â”‚ GPU-3       â”‚               â”‚
â”‚  â”‚ TensorBufferâ”‚  â”‚ linear      â”‚  â”‚ rms_norm    â”‚  â”‚ silu        â”‚               â”‚
â”‚  â”‚             â”‚  â”‚ Ã—5åç«¯      â”‚  â”‚ Ã—5åç«¯      â”‚  â”‚ Ã—5åç«¯      â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚         â”‚                                                                          â”‚
â”‚         â”‚ å®Œæˆåè§£é”                                                               â”‚
â”‚         â–¼                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚  â”‚ CRIT-2      â”‚  â”‚ CRIT-3      â”‚  â”‚ CRIT-4      â”‚  â† å…¨éƒ¨å¹¶è¡Œ                     â”‚
â”‚  â”‚ æƒé‡GPUé©»ç•™ â”‚  â”‚ KV Cache    â”‚  â”‚ æ¿€æ´»å€¼      â”‚                                â”‚
â”‚  â”‚ Ã—5åç«¯      â”‚  â”‚ GPUé©»ç•™     â”‚  â”‚ GPUé©»ç•™     â”‚                                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚         â”‚                â”‚                â”‚                                        â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                          â–¼                                                          â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚                   â”‚ CRIT-5      â”‚                                                   â”‚
â”‚                   â”‚ å…¨é“¾è·¯GPU   â”‚                                                   â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                             â”‚
                                             â”‚ å®Œæˆåè§£é”
                                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                    STAGE 2                                          â”‚
â”‚                              æè‡´ä¼˜åŒ–å±‚ï¼ˆå…¨å¹¶è¡Œï¼‰                                     â”‚
â”‚                                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ MEM-1~8      â”‚ OPS-1~6      â”‚ LOAD-1â†’2â†’3   â”‚ QUANT-1,4â†’2,3,5,6 â”‚ MISC-5   â”‚   â”‚
â”‚  â”‚ å†…å­˜ä¼˜åŒ–     â”‚ ç®—å­æ›¿æ¢     â”‚ åŠ è½½ä¼˜åŒ–     â”‚ é‡åŒ–æ¨ç†           â”‚ KVå‹ç¼©   â”‚   â”‚
â”‚  â”‚ (MEM-3ä¾èµ–   â”‚ å…¨å¹¶è¡Œ       â”‚ ä¸²è¡Œä¾èµ–     â”‚ æ•°æ®ç»“æ„â†’kernel   â”‚          â”‚   â”‚
â”‚  â”‚  MEM-1,2)    â”‚              â”‚              â”‚                   â”‚          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### è¯¦ç»†ä»»åŠ¡è¡¨ï¼ˆæŒ‰ä¾èµ–æ’åºï¼‰

##### STAGE 0: åŸºç¡€æ¶æ„ï¼ˆè§£é”å…¨éƒ¨åç»­ï¼‰

| ID | ä»»åŠ¡ | é¡¹ç›® | ä¾èµ– | åç«¯ | éªŒæ”¶æ ‡å‡† |
|----|------|------|------|------|----------|
| **REFACTOR-4** | BackendImpl enum | gllm-kernels | æ—  | å…¨åç«¯ | `Arc<dyn Backend>` å…¨éƒ¨æ›¿æ¢ä¸º `BackendImpl` |
| **ARCH-1** | OnceLock åç«¯æ£€æµ‹ | gllm | æ—  | å…¨åç«¯ | `auto_select_backend()` è°ƒç”¨ = 1æ¬¡/è¿›ç¨‹ |
| **ARCH-2** | BackendType é™æ€ | gllm | ARCH-1 | å…¨åç«¯ | é›¶è¿è¡Œæ—¶ç±»å‹åˆ¤æ–­ |
| REFACTOR-1 | Trait é‡å‘½å | gllm | æ—  | - | æ¶ˆé™¤ GeneratorModelTrait æ­§ä¹‰ |
| REFACTOR-2 | EmbeddingModel enum | gllm | æ—  | - | é›¶ `Box<dyn>` |
| REFACTOR-3 | GeneratorModel enum | gllm | æ—  | - | é›¶ `Box<dyn>` |
| MISC-1 | TokenizerWrapper | gllm | æ—  | - | ç§»é™¤å†—ä½™å­—æ®µ |
| MISC-2 | ScratchBuffer | gllm | æ—  | - | çƒ­è·¯å¾„é›¶åˆ†é… |
| MISC-3 | PromptCache | gllm | æ—  | - | ç›¸åŒ prompt å¤ç”¨ |
| MISC-4 | ç®—å­çº¯å‡½æ•°åŒ– | gllm | æ—  | - | æ— çŠ¶æ€ç®—æ³•é›†åˆ |

**å¹¶è¡Œç»„**ï¼šREFACTOR-4, ARCH-1, REFACTOR-1/2/3, MISC-1/2/3/4 å…¨éƒ¨å¹¶è¡Œå¯åŠ¨

---

##### STAGE 1: GPU åŸºç¡€è®¾æ–½

| ID | ä»»åŠ¡ | é¡¹ç›® | ä¾èµ– | åç«¯ | éªŒæ”¶æ ‡å‡† |
|----|------|------|------|------|----------|
| **CRIT-1** | TensorBuffer æŠ½è±¡ | gllm | REFACTOR-4 | å…¨åç«¯ | ç»Ÿä¸€ CPU/GPU buffer æ¥å£ |
| **GPU-1** | linear_forward | gllm-kernels | REFACTOR-4 | CPU/WGPU/CUDA/Metal/ROCm | ç†è®ºå³°å€¼ 80%ï¼Œå…¨åç«¯ |
| **GPU-2** | rms_norm | gllm-kernels | REFACTOR-4 | CPU/WGPU/CUDA/Metal/ROCm | reduce kernelï¼Œå…¨åç«¯ |
| **GPU-3** | silu_inplace | gllm-kernels | REFACTOR-4 | CPU/WGPU/CUDA/Metal/ROCm | element-wiseï¼Œå…¨åç«¯ |
| **CRIT-2** | æƒé‡ GPU é©»ç•™ | gllm | CRIT-1 | WGPU/CUDA/Metal/ROCm | æƒé‡å¸¸é©» GPUï¼Œé›¶ PCIe ä¼ è¾“ |
| **CRIT-3** | KV Cache GPU | gllm | CRIT-1 | WGPU/CUDA/Metal/ROCm | KV å¸¸é©» GPUï¼ŒåŸåœ°æ›´æ–° |
| **CRIT-4** | æ¿€æ´»å€¼ GPU | gllm | CRIT-1 | WGPU/CUDA/Metal/ROCm | ä¸­é—´æ¿€æ´»å…¨ GPU |
| **CRIT-5** | å…¨é“¾è·¯ GPU | gllm | CRIT-2,3,4 + GPU-1,2,3 | å…¨åç«¯ | embedâ†’decodeâ†’sample å…¨ GPU |

**å¹¶è¡Œç»„**ï¼š
- CRIT-1, GPU-1, GPU-2, GPU-3 å¹¶è¡Œ
- CRIT-2, CRIT-3, CRIT-4 å¹¶è¡Œï¼ˆç­‰å¾… CRIT-1ï¼‰
- CRIT-5 ç­‰å¾…å…¨éƒ¨å®Œæˆ

---

##### STAGE 2: æè‡´ä¼˜åŒ–ï¼ˆå…¨å¹¶è¡Œï¼‰

| ID | ä»»åŠ¡ | é¡¹ç›® | ä¾èµ– | éªŒæ”¶æ ‡å‡† |
|----|------|------|------|----------|
| MEM-1 | BufferPool | gllm-kernels | STAGE 1 | WGPU buffer å¤ç”¨ |
| MEM-2 | StagingBuffer | gllm-kernels | STAGE 1 | map_read é›¶åˆ†é… |
| MEM-3 | PagedAttn dispatch | gllm-kernels | MEM-1,2 | çƒ­è·¯å¾„é›¶ Vec |
| MEM-4 | KV Cache é¢„åˆ†é… | gllm | STAGE 1 | å›ºå®šå®¹é‡ï¼Œé›¶æ‰©å±• |
| MEM-5 | attention è¾“å‡ºå¤ç”¨ | gllm | STAGE 1 | æ¶ˆé™¤ .to_vec() |
| MEM-6 | WeightLoader é›¶æ‹·è´ | gllm | STAGE 1 | mmap ç›´æ¥ä½¿ç”¨ |
| MEM-7 | PagedAttn ç´¢å¼• | gllm | STAGE 1 | é¢„åˆ†é…ç´¢å¼•æ•°ç»„ |
| MEM-8 | MoE routing æ‰¹é‡ | gllm | STAGE 1 | æ‰¹é‡å¤„ç† tokens |
| OPS-1 | softmax æ›¿æ¢ | gllm | STAGE 1 | gllm_kernels::softmax |
| OPS-2 | layer_norm æ›¿æ¢ | gllm | STAGE 1 | gllm_kernels::layer_norm |
| OPS-3 | gelu æ›¿æ¢ | gllm | STAGE 1 | gllm_kernels::gelu |
| OPS-4 | rope æ›¿æ¢ | gllm | STAGE 1 | gllm_kernels::rope |
| OPS-5 | attention æ›¿æ¢ | gllm | STAGE 1 | gllm_kernels::attention |
| OPS-6 | cross_entropy æ›¿æ¢ | gllm | STAGE 1 | gllm_kernels::cross_entropy |
| LOAD-1 | AsyncShardLoader | gllm | STAGE 1 | å¹¶è¡Œä¸‹è½½ |
| LOAD-2 | MmapWeightLoader | gllm | LOAD-1 | å¹¶è¡Œ mmap |
| LOAD-3 | è¿›åº¦å›è°ƒ | gllm | LOAD-2 | LoadProgress trait |
| QUANT-1 | Q4_0 ç»“æ„ | gllm-kernels | STAGE 1 | Block å®šä¹‰ |
| QUANT-4 | AWQ ç»“æ„ | gllm-kernels | STAGE 1 | AwqPackedWeight |
| QUANT-2 | Q4_0 WGSL | gllm-kernels | QUANT-1 | in-kernel dequant |
| QUANT-3 | Q4_0 CUDA | gllm-kernels | QUANT-1 | shared memory |
| QUANT-5 | AWQ WGSL | gllm-kernels | QUANT-4 | åˆ†ç»„åé‡åŒ– |
| QUANT-6 | AWQ CUDA | gllm-kernels | QUANT-4 | tensor core |
| MISC-5 | KV å‹ç¼© | gllm | CRIT-3 | é•¿åºåˆ—ä¼˜åŒ– |

---

#### éªŒæ”¶æ ‡å‡†ï¼ˆå…¨éƒ¨å¼ºåˆ¶ï¼‰

| æŒ‡æ ‡ | è¦æ±‚ | éªŒè¯æ–¹æ³• | é€šè¿‡æ¡ä»¶ |
|------|------|----------|----------|
| **å• token å»¶è¿Ÿ** | â‰¤50ms | benchmark | 100æ¬¡å¹³å‡ |
| **GPU æ˜¾å­˜åˆ©ç”¨ç‡** | >90% | nvidia-smi | æŒç»­ç›‘æ§ |
| **PCIe ä¼ è¾“/token** | <1KB | profiler | æ¨ç†æœŸé—´ |
| **forward å†…å­˜åˆ†é…** | =0 | valgrind | æ—  malloc |
| **åç«¯æ£€æµ‹** | =1æ¬¡/è¿›ç¨‹ | æ—¥å¿— | å¯åŠ¨åé›¶è°ƒç”¨ |
| **dyn Trait** | =0 | grep | æ— åŒ¹é… |
| **æ‰‹å†™ç®—å­** | =0 | grep | å…¨æ›¿æ¢ |
| **70B åŠ è½½æ—¶é—´** | â‰¤30s | benchmark | NVMe SSD |
| **Q4 ç²¾åº¦æŸå¤±** | <0.5% | eval | å¯¹æ¯” FP16 |
| **å…¨åç«¯æ”¯æŒ** | 5/5 | CI | å…¨ç»¿ |

---

#### æ‰§è¡Œç­–ç•¥

```
å¯åŠ¨æ—¶åˆ» T=0:
â”œâ”€ å¹¶è¡Œå¯åŠ¨ STAGE 0 å…¨éƒ¨ 10 ä¸ªæ— ä¾èµ–ä»»åŠ¡
â”‚   â”œâ”€ gllm-kernels: REFACTOR-4
â”‚   â””â”€ gllm: ARCH-1, REFACTOR-1/2/3, MISC-1/2/3/4
â”‚
â”œâ”€ REFACTOR-4 å®Œæˆ â†’ è§£é”:
â”‚   â”œâ”€ CRIT-1 (TensorBuffer)
â”‚   â””â”€ GPU-1/2/3 (å…¨åç«¯å¹¶è¡Œ)
â”‚
â”œâ”€ CRIT-1 å®Œæˆ â†’ è§£é”:
â”‚   â””â”€ CRIT-2/3/4 (å¹¶è¡Œ)
â”‚
â”œâ”€ CRIT-2/3/4 + GPU-1/2/3 å®Œæˆ â†’ è§£é”:
â”‚   â””â”€ CRIT-5 (å…¨é“¾è·¯)
â”‚
â””â”€ STAGE 1 å®Œæˆ â†’ è§£é” STAGE 2 å…¨éƒ¨ä»»åŠ¡å¹¶è¡Œæ‰§è¡Œ
```

**çŠ¶æ€**: ğŸ”´ å¾…æ‰§è¡Œï¼ˆé›¶å¦¥åæ–¹æ¡ˆï¼‰

---

### ARCH-REFACTOR-003: å†—ä½™æŠ½è±¡å±‚å®¡è®¡ä¸ç²¾ç®€

> **ç›®æ ‡**ï¼šä¼˜é›…çš„ã€ä¸å¤šä¸€åˆ†ä¸å°‘ä¸€ç‚¹çš„æ¶æ„å’Œå®Œç¾çš„åˆ†å±‚è®¾è®¡
>
> **åŸåˆ™**ï¼šæ¯ä¸€å±‚æŠ½è±¡å¿…é¡»æœ‰æ˜ç¡®çš„ä»·å€¼ï¼Œçº¯é€ä¼ æˆ–ä»…2ä¸ªå®ç°çš„ trait åº”è€ƒè™‘æ¶ˆé™¤

---

#### é—®é¢˜1ï¼š`GeneratorModelTrait` é‡å¤å®šä¹‰ï¼ˆğŸ”´ ä¸¥é‡ï¼‰

**ç°çŠ¶**ï¼šåŒå trait åœ¨ä¸¤ä¸ªæ–‡ä»¶ä¸­å®šä¹‰ï¼Œæ–¹æ³•ç­¾åå®Œå…¨ä¸åŒï¼Œé€ æˆæ··ä¹±

| æ–‡ä»¶ | æ–¹æ³• | ç”¨é€” |
|------|------|------|
| `generator_engine.rs:11-22` | generate, load_safetensors, load_awq, load_gguf, max_position_embeddings | å¼•æ“å±‚åŠ è½½/ç”Ÿæˆæ¥å£ |
| `generator_model.rs:165-172` | forward, sample, hidden_size, num_layers, vocab_size, max_position_embeddings | æ¨¡å‹å±‚æ¨ç†æ¥å£ |

**é—®é¢˜**ï¼š
- åŒåä¸åŒä¹‰ï¼Œè¿åæœ€å°æƒŠè®¶åŸåˆ™
- ä»£ç é˜…è¯»è€…éš¾ä»¥åŒºåˆ†
- å­˜åœ¨å‘½åç©ºé—´æ±¡æŸ“é£é™©

**å»ºè®®**ï¼š
- æ–¹æ¡ˆAï¼šé‡å‘½åä¸º `GeneratorEngineModel` + `GeneratorInferModel`
- æ–¹æ¡ˆBï¼šåˆå¹¶ä¸ºå•ä¸€ traitï¼Œengine å±‚ç›´æ¥ä½¿ç”¨å…·ä½“ç±»å‹

---

#### é—®é¢˜2ï¼š`EmbeddingModelTrait` çº¯é€ä¼ ï¼ˆğŸŸ¡ ä¸­ç­‰ï¼‰

**ç°çŠ¶**ï¼š`engine.rs:316-329`

```rust
pub(crate) trait EmbeddingModelTrait {
    fn forward(&self, tokens: &[Vec<i64>]) -> Result<Vec<Vec<f32>>>;
    fn hidden_size(&self) -> usize;
}

// å®ç°1ï¼šçº¯é€ä¼ 
impl EmbeddingModelTrait for DynamicBertModel {
    fn forward(&self, tokens: &[Vec<i64>]) -> Result<Vec<Vec<f32>>> {
        let hidden_states = DynamicBertModel::forward(self, tokens)?;  // ç›´æ¥è°ƒç”¨åŒåæ–¹æ³•
        let pooled = DynamicBertModel::pool_hidden_states(self, &hidden_states, tokens);
        Ok(pooled_rows(pooled))
    }
    fn hidden_size(&self) -> usize { DynamicBertModel::hidden_size(self) }  // çº¯é€ä¼ 
}

// å®ç°2ï¼šçº¯é€ä¼ 
impl EmbeddingModelTrait for DecoderModel {
    fn forward(&self, tokens: &[Vec<i64>]) -> Result<Vec<Vec<f32>>> {
        DecoderModel::forward(self, tokens)  // çº¯é€ä¼ 
    }
    fn hidden_size(&self) -> usize { DecoderModel::hidden_size(self) }  // çº¯é€ä¼ 
}
```

**é—®é¢˜**ï¼š
- ä»… 2 ä¸ªå®ç°ï¼Œä½¿ç”¨ `Box<dyn EmbeddingModelTrait>` å¼•å…¥åŠ¨æ€æ´¾å‘å¼€é”€
- å®ç°ä»£ç å‡ ä¹æ˜¯çº¯é€ä¼ ï¼Œæ— é™„åŠ é€»è¾‘ï¼ˆé™¤ pool_hidden_statesï¼‰
- trait æœ¬èº«æ²¡æœ‰æä¾›æŠ½è±¡ä»·å€¼

**å»ºè®®**ï¼šä½¿ç”¨ enum æ›¿ä»£ trait object

```rust
// ä¼˜åŒ–åï¼ˆé›¶æˆæœ¬æŠ½è±¡ï¼‰
pub(crate) enum EmbeddingModel {
    Bert(DynamicBertModel),
    Decoder(DecoderModel),
}

impl EmbeddingModel {
    #[inline]
    fn forward(&self, tokens: &[Vec<i64>]) -> Result<Vec<Vec<f32>>> {
        match self {
            Self::Bert(m) => { /* ... */ }
            Self::Decoder(m) => m.forward(tokens),
        }
    }
}
```

**æ”¶ç›Š**ï¼šæ¶ˆé™¤åŠ¨æ€æ´¾å‘ï¼Œå¯ç”¨å†…è”ä¼˜åŒ–

---

#### é—®é¢˜3ï¼š`Box<dyn GeneratorModelTrait>` ä»…2ä¸ªå®ç°ï¼ˆğŸŸ¡ ä¸­ç­‰ï¼‰

**ç°çŠ¶**ï¼š`generator_engine.rs:79`

```rust
pub(crate) struct GeneratorEngine {
    model: Box<dyn GeneratorModelTrait>,  // åŠ¨æ€æ´¾å‘
}

// åˆ›å»ºæ—¶ï¼ˆline 102-109ï¼‰
let mut model: Box<dyn GeneratorModelTrait> = match info.architecture {
    Architecture::GLM4MoE | Architecture::Qwen3MoE | ...
        => Box::new(MoEGeneratorModel::new(config, backend)?),
    _ => Box::new(GeneratorModel::new(config, backend)?),
};
```

**é—®é¢˜**ï¼š
- ä»… 2 ä¸ªå®ç°ï¼š`GeneratorModel`ã€`MoEGeneratorModel`
- ç¼–è¯‘æœŸå·²çŸ¥æ‰€æœ‰å¯èƒ½ç±»å‹
- åŠ¨æ€æ´¾å‘æ— å¿…è¦

**å»ºè®®**ï¼šä½¿ç”¨ enum æ›¿ä»£

```rust
pub(crate) enum GeneratorModelImpl {
    Standard(GeneratorModel),
    MoE(MoEGeneratorModel),
}
```

---

#### é—®é¢˜4ï¼š`Arc<dyn Backend>` åŠ¨æ€æ´¾å‘ï¼ˆğŸ”´ éœ€è¦é™æ€åŒ–ï¼‰

**ç°çŠ¶**ï¼š12 å¤„ä½¿ç”¨ `Arc<dyn Backend>`

| æ–‡ä»¶ | è¡Œå· | ç”¨é€” |
|------|------|------|
| generator_model.rs | 21, 25 | æ¨¡å‹æŒæœ‰ backend |
| moe_generator_model.rs | 68, 72 | MoE æ¨¡å‹æŒæœ‰ |
| decoder_model.rs | 34 | Decoder æ¨¡å‹æŒæœ‰ |
| dynamic_bert.rs | 42, 123, 158, 379 | BERT æ¨¡å‹æŒæœ‰ |
| moe_layer.rs | 33, 42 | MoE å±‚æŒæœ‰ |
| causal_attention.rs | 112, 120 | æ³¨æ„åŠ›å±‚æŒæœ‰ |

**å½“å‰è¡Œä¸º**ï¼š
- åç«¯å®ä¾‹åœ¨ `auto_select_backend()` æ—¶åˆ›å»ºä¸€æ¬¡
- ä¹‹åæ‰€æœ‰ç®—å­è°ƒç”¨å¤ç”¨åŒä¸€ä¸ª `Arc<dyn Backend>` å®ä¾‹
- ä¸å­˜åœ¨é‡å¤åˆå§‹åŒ–é—®é¢˜

**æ¶æ„é—®é¢˜**ï¼š
- `dyn Trait` å¼•å…¥ vtable é—´æ¥è·³è½¬ï¼ˆéæ€§èƒ½ç“¶é¢ˆï¼Œä½†ä¸å¤Ÿä¼˜é›…ï¼‰
- é˜»æ­¢ç¼–è¯‘å™¨è·¨å‡½æ•°å†…è”ä¼˜åŒ–
- ä¸ Rust "é›¶æˆæœ¬æŠ½è±¡"å“²å­¦ä¸ç¬¦

**ç›®æ ‡**ï¼šæ”¹ç”¨ enum å®ç°é›¶æˆæœ¬é™æ€æ´¾å‘ï¼Œä¿æŒè¿è¡Œæ—¶åç«¯é€‰æ‹©èƒ½åŠ›

---

##### ARCH-STATIC-BACKEND-001: Backend é™æ€åŒ–è®¾è®¡

**æ–¹æ¡ˆ**ï¼šä½¿ç”¨ enum æ›¿ä»£ trait object

```rust
// gllm-kernels/src/backend.rs

/// é›¶æˆæœ¬ Backend æŠ½è±¡ï¼ˆç¼–è¯‘æœŸé™æ€æ´¾å‘ï¼‰
#[derive(Clone)]
pub enum BackendImpl {
    Cpu(CpuBackend),
    #[cfg(feature = "wgpu")]
    Wgpu(Arc<WgpuBackend>),  // Arc ç”¨äºå…±äº« GPU èµ„æº
    #[cfg(feature = "cuda")]
    Cuda(Arc<CudaBackend>),
    #[cfg(feature = "metal")]
    Metal(Arc<MetalBackend>),
    #[cfg(feature = "rocm")]
    Rocm(Arc<RocmBackend>),
}

impl BackendImpl {
    /// è¿è¡Œæ—¶è‡ªåŠ¨é€‰æ‹©æœ€ä½³åç«¯
    pub fn auto_select() -> Self {
        #[cfg(feature = "cuda")]
        if cuda_available() { return Self::Cuda(Arc::new(CudaBackend::new())); }

        #[cfg(feature = "rocm")]
        if rocm_available() { return Self::Rocm(Arc::new(RocmBackend::new())); }

        #[cfg(feature = "metal")]
        if metal_available() { return Self::Metal(Arc::new(MetalBackend::new())); }

        #[cfg(feature = "wgpu")]
        if wgpu_available() { return Self::Wgpu(Arc::new(WgpuBackend::new())); }

        Self::Cpu(CpuBackend::new())
    }

    /// å†…è”æ´¾å‘ï¼ˆé›¶æˆæœ¬ï¼‰
    #[inline]
    pub fn matmul(&self, a: TensorSlice, b: TensorSlice, c: TensorSliceMut, config: MatmulConfig) -> Result<(), String> {
        match self {
            Self::Cpu(b) => b.matmul(a, b, c, config),
            #[cfg(feature = "wgpu")]
            Self::Wgpu(b) => b.matmul(a, b, c, config),
            #[cfg(feature = "cuda")]
            Self::Cuda(b) => b.matmul(a, b, c, config),
            #[cfg(feature = "metal")]
            Self::Metal(b) => b.matmul(a, b, c, config),
            #[cfg(feature = "rocm")]
            Self::Rocm(b) => b.matmul(a, b, c, config),
        }
    }

    // ... å…¶ä»– 17 ä¸ªæ–¹æ³•åŒç†ï¼ˆå¯ç”¨å®ç”Ÿæˆï¼‰
}
```

**å®ç®€åŒ–å®ç°**ï¼š

```rust
macro_rules! dispatch_backend {
    ($self:expr, $method:ident, $($arg:expr),*) => {
        match $self {
            BackendImpl::Cpu(b) => b.$method($($arg),*),
            #[cfg(feature = "wgpu")]
            BackendImpl::Wgpu(b) => b.$method($($arg),*),
            #[cfg(feature = "cuda")]
            BackendImpl::Cuda(b) => b.$method($($arg),*),
            #[cfg(feature = "metal")]
            BackendImpl::Metal(b) => b.$method($($arg),*),
            #[cfg(feature = "rocm")]
            BackendImpl::Rocm(b) => b.$method($($arg),*),
        }
    };
}

impl BackendImpl {
    #[inline]
    pub fn matmul(&self, a: TensorSlice, b: TensorSlice, c: TensorSliceMut, config: MatmulConfig) -> Result<(), String> {
        dispatch_backend!(self, matmul, a, b, c, config)
    }

    #[inline]
    pub fn flash_attention(&self, q: TensorSlice, k: TensorSlice, v: TensorSlice,
                           output: TensorSliceMut, config: FlashAttentionConfig) -> Result<(), String> {
        dispatch_backend!(self, flash_attention, q, k, v, output, config)
    }

    // ... å®è‡ªåŠ¨ç”Ÿæˆå…¶ä½™æ–¹æ³•
}
```

**gllm ä¾§å˜æ›´**ï¼š

```rust
// ä¹‹å‰
pub struct GeneratorModel {
    backend: Arc<dyn Backend>,  // åŠ¨æ€æ´¾å‘
}

// ä¹‹å
pub struct GeneratorModel {
    backend: BackendImpl,  // é™æ€æ´¾å‘ï¼Œå¯å†…è”
}
```

**æ”¶ç›Š**ï¼š

| æ–¹é¢ | è¯´æ˜ |
|------|------|
| é›¶æˆæœ¬æŠ½è±¡ | ç¬¦åˆ Rust è®¾è®¡å“²å­¦ï¼Œenum match ç¼–è¯‘ä¸ºè·³è½¬è¡¨ |
| å†…è”ä¼˜åŒ– | ç¼–è¯‘å™¨å¯è·¨å‡½æ•°å†…è”ï¼ŒLTO æ—¶ä¼˜åŒ–æ›´å……åˆ† |
| ä»£ç ä¸€è‡´æ€§ | å…¨é¡¹ç›®ç»Ÿä¸€ä½¿ç”¨ enum æŠ½è±¡ï¼Œæ—  `dyn Trait` |
| è¿è¡Œæ—¶é€‰æ‹© | ä¿æŒ `auto_select()` è¿è¡Œæ—¶æ£€æµ‹åç«¯èƒ½åŠ› |

> æ³¨ï¼švtable å¼€é”€æœ¬èº«å¯å¿½ç•¥ï¼ˆ~ns çº§ï¼‰ï¼Œé™æ€åŒ–ä¸»è¦ç›®çš„æ˜¯æ¶æ„çº¯å‡€æ€§

**Backend trait ä¿ç•™**ï¼š
- trait å®šä¹‰ä¿ç•™ï¼Œç”¨äºçº¦æŸå„åç«¯å®ç°ï¼ˆ`impl Backend for CpuBackend`ï¼‰
- æ¶ˆè´¹ç«¯ä» `Arc<dyn Backend>` æ”¹ä¸º `BackendImpl` enum
- trait çš„ 18 ä¸ªæ–¹æ³•ä¿æŒä¸å˜ï¼Œä¸æ‹†åˆ†

---

#### é—®é¢˜5ï¼šFallbackEmbedder åŒ…è£…å™¨ï¼ˆğŸŸ¢ åˆç†ï¼‰

**ç°çŠ¶**ï¼š`fallback.rs` - 436 è¡Œ

**åˆ†æ**ï¼š
- æä¾› GPUâ†’CPU è‡ªåŠ¨å›é€€åŠŸèƒ½
- åŒ…å«å¤±è´¥è®¡æ•°ã€åç«¯åˆ‡æ¢é€»è¾‘
- æ˜¯åˆç†çš„åŠŸèƒ½å°è£…ï¼Œéå†—ä½™æŠ½è±¡

**ç»“è®º**ï¼šä¿ç•™ï¼Œè¿™æ˜¯æœ‰ä»·å€¼çš„åŠŸèƒ½å±‚

---

#### é‡æ„ä¼˜å…ˆçº§çŸ©é˜µ

| ID | é—®é¢˜ | ä¼˜å…ˆçº§ | é£é™© | æ”¶ç›Š | å»ºè®® |
|----|------|--------|------|------|------|
| REFACTOR-1 | GeneratorModelTrait é‡å¤ | **P0** | ä½ | é«˜ï¼ˆæ¸…æ™°åº¦ï¼‰ | é‡å‘½åæˆ–åˆå¹¶ |
| REFACTOR-2 | EmbeddingModelTrait é€ä¼  | **P0** | ä½ | ä¸­ï¼ˆæ€§èƒ½ï¼‰ | æ”¹ç”¨ enum |
| REFACTOR-3 | GeneratorEngine Box<dyn> | **P0** | ä½ | ä¸­ï¼ˆæ€§èƒ½ï¼‰ | æ”¹ç”¨ enum |
| **REFACTOR-4** | **Arc<dyn Backend> é™æ€åŒ–** | **P0** | ä¸­ | **é«˜ï¼ˆé›¶æˆæœ¬ï¼‰** | **æ”¹ç”¨ BackendImpl enum** |

> æ³¨ï¼šBackend trait ä¿æŒ 18 ä¸ªæ–¹æ³•ä¸å˜ï¼Œä¸éœ€è¦æ‹†åˆ†

---

#### ç†æƒ³æ¶æ„åˆ†å±‚ï¼ˆç›®æ ‡çŠ¶æ€ï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ç”¨æˆ· API å±‚                               â”‚
â”‚  Client, AsyncClient, EmbedderHandle, RerankerHandle            â”‚
â”‚  èŒè´£ï¼šå…¬å¼€æ¥å£ã€å¼‚æ­¥åŒ…è£…ã€è¿æ¥æ±                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        å¼•æ“å±‚ (Engine)                           â”‚
â”‚  EmbeddingEngine, GeneratorEngine                                â”‚
â”‚  èŒè´£ï¼šæ¨¡å‹ç”Ÿå‘½å‘¨æœŸã€æ‰¹å¤„ç†è°ƒåº¦ã€å†…å­˜ç®¡ç†                          â”‚
â”‚                                                                 â”‚
â”‚  âŒ ç§»é™¤ï¼šEmbeddingModelTraitï¼ˆæ”¹ç”¨ enum EmbeddingModelï¼‰         â”‚
â”‚  âŒ ç§»é™¤ï¼šGeneratorModelTrait@engineï¼ˆæ”¹ç”¨ enum GeneratorModelï¼‰  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        æ¨¡å‹å±‚ (Model)                            â”‚
â”‚  DynamicBertModel, GeneratorModel, MoEGeneratorModel             â”‚
â”‚  èŒè´£ï¼šæ¨¡å‹ç»“æ„å®šä¹‰ã€å‰å‘ä¼ æ’­ã€æƒé‡ç®¡ç†                            â”‚
â”‚                                                                 â”‚
â”‚  âœ… ä¿ç•™ï¼šGeneratorInferTraitï¼ˆç»Ÿä¸€æ¨ç†æ¥å£ï¼Œé‡å‘½ååï¼‰            â”‚
â”‚  âœ… ä½¿ç”¨ï¼šBackendImplï¼ˆé›¶æˆæœ¬é™æ€æ´¾å‘ï¼‰                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ç®—å­å±‚ (Operators) - gllm-kernels         â”‚
â”‚                                                                 â”‚
â”‚  âœ… ä¿ç•™ï¼šBackend traitï¼ˆ18ä¸ªæ–¹æ³•ï¼Œçº¦æŸå„åç«¯å®ç°ï¼‰               â”‚
â”‚  âœ… æ–°å¢ï¼šBackendImpl enumï¼ˆé›¶æˆæœ¬é™æ€æ´¾å‘ï¼‰                      â”‚
â”‚                                                                 â”‚
â”‚  pub enum BackendImpl {                                         â”‚
â”‚      Cpu(CpuBackend),                                           â”‚
â”‚      Wgpu(Arc<WgpuBackend>),                                    â”‚
â”‚      Cuda(Arc<CudaBackend>),                                    â”‚
â”‚      Metal(Arc<MetalBackend>),                                  â”‚
â”‚      Rocm(Arc<RocmBackend>),                                    â”‚
â”‚  }                                                              â”‚
â”‚                                                                 â”‚
â”‚  èŒè´£ï¼šç®—å­å®šä¹‰ã€é™æ€æ´¾å‘ã€ç¼–è¯‘æœŸä¼˜åŒ–                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        åç«¯å±‚ (Backend Impl)                     â”‚
â”‚  CpuBackend, WgpuBackend, CudaBackend, MetalBackend, RocmBackend â”‚
â”‚  èŒè´£ï¼šç¡¬ä»¶æŠ½è±¡ã€Kernel å®ç°ã€impl Backend for XxxBackend        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**é›¶æˆæœ¬æŠ½è±¡åŸåˆ™**ï¼š

| å±‚çº§ | æŠ½è±¡æ–¹å¼ | æ´¾å‘å¼€é”€ |
|------|----------|----------|
| å¼•æ“å±‚ | enumï¼ˆEmbeddingModel, GeneratorModelï¼‰ | 0ï¼ˆmatch ç¼–è¯‘ä¸ºè·³è½¬è¡¨ï¼‰ |
| æ¨¡å‹å±‚ | å…·ä½“ç±»å‹ï¼ˆDynamicBertModel ç­‰ï¼‰ | 0ï¼ˆæ— æŠ½è±¡ï¼‰ |
| ç®—å­å±‚ | enum BackendImpl | 0ï¼ˆmatch + å†…è”ï¼‰ |
| åç«¯å±‚ | å…·ä½“ç±»å‹ï¼ˆCpuBackend ç­‰ï¼‰ | 0ï¼ˆæ— æŠ½è±¡ï¼‰ |

**å…¨æ ˆé›¶åŠ¨æ€æ´¾å‘**ï¼šä»ç”¨æˆ· API åˆ° Kernel è°ƒç”¨ï¼Œæ— ä»»ä½• `dyn Trait`

---

#### ä»£ç å˜æ›´é¢„ä¼°

| ç»„ä»¶ | å½“å‰ | å˜æ›´å | è¯´æ˜ |
|------|------|--------|------|
| **gllm-kernels** | | | |
| backend.rs | 21 | ~150 | æ–°å¢ BackendImpl enum + dispatch å® |
| **gllm** | | | |
| generator_engine.rs | 137 | ~80 | ç§»é™¤ traitï¼Œæ”¹ç”¨ enum |
| engine.rs | ~100 | ~60 | ç§»é™¤ EmbeddingModelTrait |
| å„æ¨¡å‹æ–‡ä»¶ | Arc<dyn Backend> | BackendImpl | ç±»å‹æ›¿æ¢ï¼ˆ12å¤„ï¼‰ |

**å‡€æ•ˆæœ**ï¼š
- gllm-kernels å¢åŠ  ~130 è¡Œï¼ˆBackendImpl å®ç°ï¼‰
- gllm å‡å°‘ ~100 è¡Œï¼ˆç§»é™¤å†—ä½™ traitï¼‰
- **è¿è¡Œæ—¶é›¶æˆæœ¬**ï¼šæ¶ˆé™¤æ‰€æœ‰åŠ¨æ€æ´¾å‘

---

#### å®æ–½æ£€æŸ¥æ¸…å•

| æ­¥éª¤ | å†…å®¹ | å½±å“èŒƒå›´ | çŠ¶æ€ |
|------|------|----------|------|
| **1** | **gllm-kernels: æ–°å¢ BackendImpl enum** | backend.rs | ğŸ”² |
| **2** | **gllm-kernels: å®ç° dispatch_backend! å®** | backend.rs | ğŸ”² |
| **3** | **gllm-kernels: BackendImpl å®ç°æ‰€æœ‰ 18 ä¸ªæ–¹æ³•** | backend.rs | ğŸ”² |
| **4** | **gllm-kernels: auto_select() è¿”å› BackendImpl** | backend.rs | ğŸ”² |
| 5 | gllm: `Arc<dyn Backend>` â†’ `BackendImpl`ï¼ˆ12å¤„ï¼‰ | å…¨æ¨¡å‹å±‚ | ğŸ”² |
| 6 | é‡å‘½å `generator_model.rs::GeneratorModelTrait` â†’ `GeneratorInferTrait` | generator_model.rs | ğŸ”² |
| 7 | åˆ é™¤ `generator_engine.rs::GeneratorModelTrait`ï¼Œæ”¹ç”¨ enum | generator_engine.rs | ğŸ”² |
| 8 | `EmbeddingModelTrait` â†’ `enum EmbeddingModel` | engine.rs | ğŸ”² |
| 9 | `Box<dyn GeneratorModelTrait>` â†’ `enum GeneratorModelImpl` | generator_engine.rs | ğŸ”² |
| 10 | æ›´æ–°æ‰€æœ‰è°ƒç”¨ç‚¹ | å…¨é¡¹ç›® | ğŸ”² |
| 11 | åŸºå‡†æµ‹è¯•éªŒè¯æ€§èƒ½æå‡ | benchmark | ğŸ”² |

**ä¾èµ–å…³ç³»**ï¼š
```
æ­¥éª¤ 1-4ï¼ˆgllm-kernelsï¼‰
    â†“
æ­¥éª¤ 5ï¼ˆgllm é€‚é… BackendImplï¼‰
    â†“
æ­¥éª¤ 6-9ï¼ˆgllm å†…éƒ¨é‡æ„ï¼‰
    â†“
æ­¥éª¤ 10-11ï¼ˆæ”¶å°¾éªŒè¯ï¼‰
```

**çŠ¶æ€**: âœ… å®¡è®¡å®Œæˆï¼Œå¾…å®æ–½
